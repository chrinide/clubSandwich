\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.7in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage[textwidth=1in, textsize=tiny]{todonotes}
%\usepackage[disable]{todonotes}

\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Small sample correction methods for cluster-robust variance estimators and hypothesis tests}
  \author{\\James E. Pustejovsky\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Educational Psychology \\ 
    University of Texas at Austin\\ \\
    and \\ \\
    Elizabeth Tipton \\
    Department of Human Development \\ 
    Teachers College, Columbia University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Small sample correction methods for cluster-robust variance estimators and hypothesis tests}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{INTRODUCTION}
\label{sec:intro}



Cluster-robust variance estimators (CRVE) and hypothesis tests based upon such estimators are ubiquitous in applied econometric work. Nearly every respectable paper in the past 15 years uses cluster-robust variance estimators because to do otherwise would be to risk being seen as insufficiently rigorous (or anti-conservative....ughh....how gauche!).

There's been a lot of fretting recently that even CRVE may actually not be rigorous enough. Cite the following people so as not to get their ire up:
\begin{itemize}
\item \citet{Brewer2013inference}
\item \citet{Cameron2008bootstrap}
\item \citet{Cameron2015practitioners}
\item \citet{Carter2013asymptotic}
\item \citet{Ibragimov2010tstatistic}
\item \citet{Imbens2012robust}
\item \citet{Kezdi2004robust}
\item \citet{McCaffrey2001generalizations, Bell2002bias}
\item \citet{McCaffrey2006improved}
\item \citet{Webb2013wild}
\item \citet{Kline2012score}
\end{itemize}

\subsection{Econometric framework}

We will consider linear regression models in which the errors within a cluster have an unknown variance structure. 
The model is
\begin{equation}
\label{eq:model_vector}
\bm{Y}_j = \bm{X}_j \bs\beta + \bs\epsilon_j,
\end{equation}
for $j=1,...,m$, where $\bm{Y}_j$ is $n_j \times 1$, $\bm{X}_j$ is an $n_j \times p$ matrix of regressors for cluster $j$, $\bs\beta$ is a $p \times 1$ vector, and $\bs\epsilon_j$ is an $n_j \times 1$ vector of errors. 
Assume that $\E\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bm{0}$ and $\Var\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bs\Sigma_j$, for $j = 1,...,m$, where $\bs\Sigma_1,...,\bs\Sigma_m$ may be unknown, and the errors are independent across clusters. 
Let $\bm{X} = \left(\bm{X}_1',\bm{X}_2',...,\bm{X}_m'\right)'$ and $\bs\Sigma = \bigoplus_{j=1}^m \bs\Sigma_j$. Additionally, let $N = \sum_{j=1}^m n_j$, let $\bm{I}$ denote an $N \times N$ identity matrix, and let $\bm{I}_j$ denote an $n_j \times n_j$ identity matrix.

The vector of regression coefficients is estimated by weighted least squares (WLS). 
Given a set of $m$ symmetric weighting matrices $\bm{W}_1,...,\bm{W}_m$, the WLS estimator is 
\begin{equation}
\label{eq:WLS}
\bs{\hat\beta} = \bm{M} \sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{Y}_j, 
\end{equation}
where $\bm{M} = \left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{X}_j\right)^{-1}$. Let $\bm{W} = \bigoplus_{j=1}^m \bm{W}_j$. 

Common choices for weighting include the unweighted case, in which $\bm{W}_j = \bm{I}_j$ for $j = 1,...,m$, and inverse-variance weighting under a working model. 
In the latter case, the errors are assumed to follow some known structure, $\Var\left(\bm{e}_j\left|\bm{X}_j\right.\right) = \bs\Phi_j$, where $\bs\Phi_j$ is a known function of a low-dimensional parameter and $\bs\Phi = \bigoplus_{j=1}^m \bs\Phi_j$. 
The weighting matrices are then taken to be $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$, where the $\hat{\bs\Phi}_j$ are constructed from estimates of the variance parameter.

The WLS estimator also encompasses the estimator proposed by \citet{Ibragimov2010tstatistic} for clustered data. 
Assuming that $\bm{X}_j$ has rank $p$ for $j = 1,...,m$, their proposed approach involves estimating $\bs\beta$ separately within each cluster and taking the simple average of these estimates. 
The resulting average is equivalent to the WLS estimator with weights $\bm{W}_j = \bm{X}_j \left(\bm{X}_j'\bm{X}_j\right)^{-2} \bm{X}_j$.

\section{CLUSTER-ROBUST VARIANCE ESTIMATORS}
\label{sec:CRVE}

The variance of the WLS estimator is 
\begin{equation}
\label{eq:var_WLS}
\Var\left(\bs{\hat\beta}\right) = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bs\Sigma_j \bm{W}_j\bm{X}_j\right) \bm{M},
\end{equation}
which depends upon the unknown variance matrices. 
One approach to estimating this variance would be to posit a working model---typically the same working model used to construct weights---and substitute estimates of the working variance structure in place of $\bs\Sigma$. 
Under working model $\bs\Phi$, denote this "model-based" variance estimator as
\begin{equation}
\label{eq:V_model}
\bm{V}^M = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j\bm{X}_j\right) \bm{M}.
\end{equation}
If $\bs\beta$ is estimated using inverse-variance weights defined under the same working model, then $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$ and the model-based variance estimator simplifies to $\bm{V}^M = \bm{M}$. 

Cluster-robust variance estimators provide a means of estimating $\Var\left(\bs{\hat\beta}\right)$ and testing hypotheses regarding $\hat{\bs\beta}$ in the absence of a valid working model for the error structure, or when the working variance model used to develop weights is mis-specified. 
They are thus a generalization of heteroskedasticity-consistent (HC) variance estimators \citep{MacKinnon1985some}. 
Like the HC estimators, several different variants have been proposed, with different rationales and different finite-sample properties. 

The most widely used estimator is 
\begin{equation}
\label{eq:V_CR0}
\bm{V}^{CR0} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{e}_j \bm{e}_j'\bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
where $\bm{e}_j = \bm{Y}_j - \bm{X}_j \bs{\hat\beta}$. Following the naming conventions used by \citet{Cameron2015practitioners}, we will refer to this estimator as CR0. 
Note that CR0 is constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$ in (\ref{eq:var_WLS}). 
Although the individual squared residuals provide only very crude estimates of the unknown variance matrices, the resulting estimator is asymptotically consistent for the variance of $\bs{\hat\beta}$ as $m$ increases (CITE). 
However, CR0 is known to have a downward bias when the number of independent clusters is small (CITE).

The small-sample bias in CR0 can be seen as analogous to that arising by estimating the variance of a sample of $m$ observations using a denominator of $m$ rather than $m - 1$. 
One approach to correcting this bias is to scale CR0 by a factor of $m / (m - 1)$. Thus, define the CR1 estimator as $\bm{V}^{CR1} = [m / (m - 1)] \bm{V}^{CR0}$.
Some software implementations use a slightly different correction factor. 
For example, the Stata command \texttt{regress} scales CR0 by the factor $m (N - 1) / [(m - 1) (N - p)]$. 

\subsection{Correction based on a working-model}

\citet[see also \citealp{Bell2002bias}]{McCaffrey2001generalizations} proposed to correct the small-sample bias of CR0 so that it is exactly unbiased under a specified working model. 
In their implementation, the residuals from each cluster are multiplied by adjustment matrices $\bm{A}_1,...,\bm{A}_m$ that are chosen to lead to the unbiasedness property. 
The variance estimator, which we will call CR2a, is then 
\begin{equation}
\label{eq:V_CR2a}
\bm{V}^{CR2a} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{A}_j \bm{e}_j \bm{e}_j' \bm{A}_j' \bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
The adjustment matrix $\bm{A}_j$ is of dimension $n_j \times n_j$ and satisfies
\begin{equation}
\label{eq:CR2a_criterion}
\bm{X}_j' \bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j \bm{X}_j = \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j \bm{X}_j,
\end{equation}
where $\bm{H} = \bm{X}\bm{M}\bm{X}'\bm{W}$, and $\left(\bm{I} - \bm{H}\right)_j$ denotes the rows of $\bm{I} - \bm{H}$ corresponding to cluster $j$. 
The criterion (\ref{eq:CR2a_criterion}) does not uniquely define $\bm{A}_j$. 
Based on extensive simulations, \citet{McCaffrey2001generalizations} found that a symmetric solution worked well, with 
\begin{equation}
\label{eq:CR2a_adjustment}
\bm{A}_j = {\bs{\hat\Phi}_j^C}' \bm{B}_j^{-1/2}\hat{\bs\Phi}_j^C,
\end{equation}
where $\hat{\bs\Phi}_j^C$ is the upper triangular Cholesky factorization of $\hat{\bs\Phi}_j$, 
\begin{equation}
\label{eq:CR2a_Bmatrix}
\bm{B}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' {\bs{\hat\Phi}_j^C}',
\end{equation}
and $\bm{B}_j^{-1/2}$ is the inverse of the symmetric square root of $\bm{B}_j $. 
If ordinary (unweighted) least squares is used to estimate $\bs\beta$ and the working variance model posits that the errors are all independent and homoskedastic, then $\bm{W} = \bs\Phi = \bm{I}$ and $\bm{A}_j = \left(\bm{I}_j - \bm{X}_j\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}_j'\right)^{-1/2}$.

Two difficulties arise in the implementation of CR2a.
First, the matrices $\bm{B}_1,...,\bm{B}_m$ may not be positive definite, so that $\bm{B}_j^{-1/2}$ cannot be calculated for every cluster. 
This occurs, for instance, in balanced panel models when the specification includes fixed effects for each unit and each timepoint and clustering is over the units \citep[p. 320]{Angrist2009mostly}. 
However, this problem can be overcome by using a generalized inverse of $\bm{B}_j$. 
A second, computational difficulty with CR2a is that it requires the inversion (or pseudo-inversion) of $m$ matrices, each of dimension $n_j \times n_j$. 
Consequently, computation of CR2a will be slow if some clusters contain a large number of of individual units. 

\subsection{Another working-model correction}

The criterion (\ref{eq:CR2a_criterion}) is not the only way to obtain a variance estimator that is precisely unbiased under a working model. An alternative approach, which to our knowledge is novel, is to use
\begin{equation}
\label{eq:V_CR2b}
\bm{V}^{CR2b} = \bm{M} \left(\sum_{j=1}^m \bm{D}_j \bm{X}_j' \bm{W}_j \bm{e}_j \bm{e}_j' \bm{W}_j \bm{X}_j \bm{D}_j'\right) \bm{M},
\end{equation}
where the adjustment matrices $\bm{D}_1,...,\bm{D}_m$ are chosen so that
\begin{equation}
\label{eq:CR2b_criterion}
\bm{D}_j \bm{X}_j' \bm{W}_j \left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \bm{W}_j \bm{X}_j \bm{D}_j' = \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j \bm{X}_j.
\end{equation}
Just as with CR2a, there are several different forms of adjustment matrices that satisfy (\ref{eq:CR2b_criterion}). 
A symmetric solution is to take
\begin{equation}
\bm{D}_j = {\bm{F}_j^C}'\left[\bm{F}_j^C \bm{G}_j {\bm{F}_j^C}'\right]^{-1/2}\bm{F}_j^C
\end{equation}
where $\bm{F}_j = \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j \bm{X}_j$ and $\bm{G}_j = \bm{X}_j' \bm{W}_j \left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \bm{W}_j \bm{X}_j$. Note that $\bm{F}_j$ and $\bm{G}_j$ might not be positive definite, and so generalized forms of the Cholesky decomposition and symmetric inverse-square root must be used. In datasets where clusters have a large number of individual units, CR2b will be less computationally intensive than CR2a because the adjustment matrices are all of dimension $p \times p$.

\subsection{Jackknife correction}

The Jackknife is an alternative, general-purpose approach to estimating $\Var\left(\bs{\hat\beta}\right)$ under unknown variance structures (CITE). 
The jackknife variance estimator involves re-estimating the vector of regression coefficients $m$ times, each time omitting a single cluster of data. 
Let $\bs{\hat\beta}_{(j)}$ denote the WLS estimator of $\bs\beta$ based on omitting cluster $j$. 
One form of the jackknife estimator is then 
\begin{equation}
\label{eq:V_JK}
\bm{V}^{JK} = \frac{m - 1}{m} \sum_{j=1}^m \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right) \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right)'.
\end{equation}
This estimator can be expressed in the form of a sandwich estimator with adjusted residuals, and therefore generalizes the HC3 estimator in the heteroskedastic case. 
For simplicity, we omit the correction factor $(m - 1) / m$ and define the CR3 estimator as \[
\bm{V}^{CR3} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \left(\bm{I}_j - \bm{H}_{jj}\right)^{-1}\bm{e}_j \bm{e}_j' \left(\bm{I}_j - \bm{H}_{jj}'\right)^{-1} \bm{W}_j \bm{X}_j\right) \bm{M}. \]
\citet{Bell2002bias} show that when $\left(\bm{I}_j - \bm{H}_{jj}\right)^{-1}$ exists for each $j = 1,...,m$, then $\bm{V}^{JK} = [(m - 1) / m] \bm{V}^{CR3}$.  

\subsection{Considerations with panel models}

CRVEs are often used in connection with fixed effects panel data models. 
In such models, clusters correspond to repeated measures on individual units (e.g., yearly data describing each of the states in the U.S.), and the regression specification includes separate intercepts for each unit.
One common model is 
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \epsilon_{jt} \]
for $j=1,...m$ and $t = 1,...,n_j$, where $\bm{r}_{ij}$ is an $r \times 1$ row vector of covariates. If the number and timing of the measurements is identical across cases, then the panel is balanced. Another common specification for balanced panels includes additional effects for each unique measurement occassion:
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \nu_t + \epsilon_{jt} \]
for $j=1,...,m$ and $t = 1,...,n$. 
In what follows, we consider a generic fixed effects model in which
\begin{equation}
\label{eq:fixed_effects}
\bm{y}_j = \bm{R}_j \bs\alpha + \bm{S}_j \bs\gamma + \bs\epsilon_j,
\end{equation}
where $\bm{R}_j$ is an $n_j \times r$ matrix of covariates, $\bm{S}_j$ is an $n_j \times s$ matrix describing the fixed effects specification, $\bm{X}_j = \left[\bm{R}_j \ \bm{S}_j\right]$, $\bs\beta = \left(\bs\alpha', \bs\gamma'\right)'$, and $p = r + s$. 

In fixed effects panel models, inferential interest is confined to $\bs\alpha$ and the fixed effects are treated as nuisance parameters. 
If the dimension of the fixed effects specification is large, it is computationally inefficient (and can be numerically inaccurate) to estimate $\bs\beta$ by ordinary or weighted least squares. 
Instead, it is useful to first absorb the fixed effects and then estimate $\bs\alpha$ on the reduced covariate vector.
Although both approaches yield algebraically equivalent estimators of $\bs\alpha$, the small-sample adjustments to the CRVEs can differ depending on whether they are calculated based on the full covariate matrix or after absorbing the fixed effects. 
We view absorption is a computational device, rather than a distinct approach to estimation, and so it is useful to describe how to calculate the CRVEs when $\bs\alpha$ is estimated using absorption.

Let $\bm{H_S} = \bm{S}\left(\bm{S}'\bm{W}\bm{S}\right)^{-1} \bm{S}'\bm{W}$, $\bm{\ddot{Y}} = \left(\bm{I} - \bm{H_S}\right)\bm{Y}$, $\bm{\ddot{R}} = \left(\bm{I} - \bm{H_S}\right)\bm{R}$, $\bm{M_{\ddot{R}}} = \left(\bm{\ddot{R}}' \bm{W} \bm{\ddot{R}}\right)^{-1}$, and $\bm{H_{\ddot{R}}} = \bm{\ddot{R}}\bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W}$. 
Using absorption, the WLS estimator of $\bs\alpha$ can be calculated as \[
\bs{\hat\alpha} = \bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W} \bm{\ddot{Y}}. \]
This estimator is algebraically equivalent to the corresponding sub-vector of $\bs{\hat\beta}$  calculated as in (\ref{eq:WLS}), based on the full covariate matrix $\bm{X}$. 
Furthermore, the residuals can be calculated from the absorbed model using $\bm{e} = \bm{\ddot{y}} - \bm{\ddot{R}} \bs{\hat\alpha}$.
Let $\bm{\ddot{V}}^{CR0}$ denote the CR0 estimator calculated using $\bm{\ddot{R}}$ in place of $\bm{X}$, $\bm{M_{\ddot{R}}}$ in place of $\bm{M}$, and $\bm{\ddot{e}} = $ in place of $\bm{e}$. It can be shown that $\bm{\ddot{V}}^{CR0}$ is algebraically equivalent to $\bm{V}^{CR0}$ calculated based on the full covariate matrix, as in (\ref{eq:V_CR0}). 
Because CR1 differs from CR0 by the constant factor $m / (m - 1)$, it too is invariant to how $\bs{\hat\alpha}$ is calculated. 

In contrast to CR0 and CR1, the CR2a estimator will differ depending on whether it is calculated based on the quantities from the absorbed model or those from the full WLS model. 
It is thus useful to define CR2a in such a way that the calculations based on the absorbed model yield algrebraically identical results to the calculations from the full WLS model. 
This can be accomplished by ensuring that the adjustment matrices given in Equation (\ref{eq:CR2a_adjustment}) are calculated based on the full covariate matrix $\bm{X}$. Specifically, in models with fixed effects, the adjustment matrices are calculated as 
\begin{equation}
\label{eq:CR2_panel_adjustment}
\bm{A}_j = {\bs{\hat\Phi}_j^C}' \left[\hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}' \right]^{-1/2}\hat{\bs\Phi}_j^C.
\end{equation}
This formula avoids the need to calculate $\bm{H}$, which would involve inverting a $p \times p$ matrix.\todo{Comment on whether this matters when fixed effects include only cluster indicators.}

Like CR2a, the CR2b estimator is affected by whether it is calculated based on the absorbed model or the full WLS model. However, the structure of the adjustment matrices is such that their dimension can be reduced by focusing on the subset of covariates that are of inferential interest: instead of using $p \times p$ adjustment matrices, one can reduce their dimension to $r \times r$. Thus, in models with fixed effects, we propose to calculate CR2b as 
\begin{equation}
\label{eq:V_CR2b_FE}
\bm{\ddot{V}}^{CR2b} = \bm{M_{\ddot{R}}} \left(\sum_{j=1}^m \bm{\ddot{D}}_j \bm{\ddot{R}}_j' \bm{W}_j \bm{e}_j \bm{e}_j' \bm{W}_j \bm{\ddot{R}}_j \bm{\ddot{D}}_j'\right) \bm{M_{\ddot{R}}},
\end{equation}
where the adjustment matrices are given by 
\begin{equation}
\bm{\ddot{D}}_j = \bm{\ddot{F}}_j^C{}'\left[\bm{\ddot{F}}_j^C \bm{\ddot{G}}_j \bm{\ddot{F}}_j^C{}'\right]^{-1/2}\bm{\ddot{F}}_j^C,
\end{equation}
$\bm{\ddot{F}}_j = \bm{\ddot{R}}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j \bm{\ddot{R}}_j$ and $\bm{\ddot{G}}_j = \bm{\ddot{R}}_j' \bm{W}_j \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j  \left(\bm{I} - \bm{H_s}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_s}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' \bm{W}_j \bm{\ddot{R}}_j$. The CR2b estimator then has the property that $\E\left(\bm{\ddot{V}}^{CR2b}\right) = \Var\left(\bs{\hat\alpha}\right)$ when the working model is correct.

In panel models that include fixed effects for each cluster, the CR3 variance estimator does not work when the model is estimated by WLS because $\bm{I}_j - \bm{H}_{jj}$ is not of full rank. 
However, the jackknife estimator of $\Var\left(\bs{\hat\alpha}\right)$ remains well-defined, as 
\begin{equation}
\bm{V}^{JK} = \frac{m - 1}{m} \sum_{j=1}^m \left(\bs{\hat\alpha}_{(j)} - \bs{\hat\alpha}\right) \left(\bs{\hat\alpha}_{(j)} - \bs{\hat\alpha}\right)',
\end{equation}
where $\bs{\hat\alpha}_{(j)}$ is calculated by omitting cluster $j$. 
If the fixed effects specification consists entirely of cluster-level effects, or more generally, if $\bm{S}_j'\bm{S}_k = \bm{0}$ when $j \neq k$ for all $j,k=1,...,m$, then $\bm{V}^{JK} = [(m - 1) / m] \bm{\ddot{V}^{CR3}}$, where 
\begin{equation}
\label{eq:V_CR3_FE}
\bm{\ddot{V}}^{CR3} = \bm{M_{\ddot{R}}} \left(\sum_{j=1}^m \bm{\ddot{R}}_j' \bm{W}_j \left(\bm{I}_j - \bm{H_{\ddot{R}}}_{jj}\right)^{-1}\bm{e}_j \bm{e}_j' \left(\bm{I}_j - \bm{H_{\ddot{R}}}_{jj}'\right)^{-1}\bm{W}_j \bm{\ddot{R}}_j\right) \bm{M_{\ddot{R}}}
\end{equation}
and $\bm{H_{\ddot{R}}}_{jj} = \bm{\ddot{R}}_j \bm{M_{\ddot{R}}} \bm{\ddot{R}}_j\bm{W}_j$. 
If the fixed effects specification includes terms that are not strictly nested within clusters, then $\bm{\ddot{V}}^{CR3}$ will not be exactly equivalent to $\bm{V}^{JK}$, and the former will depend to some extent on which terms are absorbed.\todo{So we would recommend what?}

\section{SINGLE-CONSTRAINT TESTS}
\label{sec:testing}

Wald-type test statistics based on CRVEs are often used to test hypotheses regarding and construct confidence intervals for the coefficients in the regression specification. 
Such procedures are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., $m \to \infty$). 
However, evidence from a wide variety of contexts indicates that the asymptotic results can be a very poor approximation when the number of clusters is small, even when small-sample corrections such as CR2a or CR3 are used \citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. 
Furthermore, the accuracy of asymptotic approximations depends on design features such as the degree of imbalance in the covariates, skewness of the covariates, and similarity of cluster sizes \citep{McCaffrey2001generalizations, Tipton2015small, Webb2013wild}. 
Consequently, no simple rule-of-thumb exists for what constitutes an adequate sample size to trust the asymptotic test. 

We first consider testing single linear constraints (i.e., t-tests) on the parameter $\bs\beta$, in which the null hypothesis has the form $H_0: \bm{c}'\bs\beta = d$ for fixed $p \times 1$ vector $\bm{c}$ and scalar constant $d$. 
The Wald test statistic is then of the form 
\begin{equation}
\label{eq:Wald_z}
Z = \left(\bm{c}'\bs{\hat\beta} - d\right) / \sqrt{\bm{c}'\bm{V}^{CR}\bm{c}},
\end{equation}
where $\bm{V}^{CR}$ is one of the CRVEs described in the previous section. 
An asymptotically valid test rejects $H_0$ at level $\alpha$ if $|Z|$ exceeds the $\alpha / 2$ critical value of a standard normal distribution. 
However, this test tends to have actual rejection rates higher than $\alpha$ when $m$ is not large. 

\subsection{Small-sample corrections}

Four approaches to small-sample correction have been proposed for Wald-type t-tests. 
The first and surely most common approach is to compare $|Z|$ to the appropriate critical value from a $t$ distribution with $m - 1$ degrees of freedom. 
\citet{Hansen2007asymptotic} provided one justification for the use of a $t(m-1)$ reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \citep[see also][]{Donald2007inference}. 
\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values would be conservative (leading to rejection rates less than or equal to $\alpha$).
However, both of these arguments require that $\bm{c}'\bs\beta$ be separately identified within each cluster. 
Outside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \citep{Cameron2015practitioners}. 
Furthermore, this correction does not take into account that the distribution of $\bm{V}^{CR}$ is affected by the structure of the covariate matrix. 

A second approach, proposed by \citet{McCaffrey2001generalizations}, is to use a Satterthwaite approximation \citep{Satterthwaite1946approximate} to the distribution of $Z$.
This approach compares $Z$ to a $t$ reference distribution, with degrees of freedom $\nu$ that are estimated from the data. 
Theoretically, the degrees of freedom should be 
\begin{equation}
\label{eq:nu_Satterthwaite}
\nu = \frac{2\left[\E\left(\bm{c}'\bm{V}^{CR}\bm{c}\right)\right]^2}{\Var\left(\bm{c}'\bm{V}^{CR}\bm{c}\right)}.
\end{equation}
Expressions for the first two moments of $\bm{c}'\bm{V}^{CR}\bm{c}$ can be derived under the assumption that the errors $\bs\epsilon_1,...,\bs\epsilon_m$ are normally distributed; see Appendix \ref{app:VCR_dist}. 
In practice, both moments involve the variance structure $\bs\Sigma$, which is unknown. 
\citet{McCaffrey2001generalizations} proposed to estimate the moments based on a working variance model; for the CR2a and CR2b estimators, the same working model, $\bs\Phi$, is used to derive the adjustment matrices for the CRVE and to estimate the moments of that estimator. 
A ``model-based'' estimate of the degrees of freedom is then calculated as 
\begin{equation}
\nu_{M} = \frac{\left(\sum_{j=1}^m \bm{s}_j' \hat{\bs\Phi} \bm{s}_j\right)^2}{\sum_{i=1}^m \sum_{j=1}^m \left(\bm{s}_i' \hat{\bs\Phi} \bm{s}_j\right)^2},
\end{equation}
where $\bm{s}_j = \left(\bm{I} - \bm{H}\right)_j'\bm{A}_j'\bm{W}_j\bm{X}_j\bm{M}\bm{c}$ for CR2a or $\bm{s}_j = \left(\bm{I} - \bm{H}\right)_j'\bm{W}_j\bm{X}_j\bm{D}_j'\bm{M}\bm{c}$ for CR2b. 
Alternately, for any of the CRVEs one could instead use an empirical, ``plug-in'' estimate of the degrees of freedom, constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$. 
(This is similar to the Welch-Satterthwaite degrees of freedom estimate typically used for the two-sample t-test with unequal variances \citep{Satterthwaite1946approximate}.) 
However, \citet{Bell2002bias} found using simulation that the plug-in degrees of freedom estimate produced very conservative rejection rates. 
As a more refined empirical degrees of freedom estimate, we consider using $\bm{c}'\bm{V}^{CR}\bm{c}$ as an estimate of its own expectation and estimating its variance by scaling the squares of the residual cross-products to account for their kurtosis. 
Specifically, we propose to use 
\begin{equation}
\nu_{E} = \frac{\left(\bm{c}'\bm{V}^{CR}\bm{c}\right)^2}{\sum_{h,i,j,k=1}^m a_{jk}\bm{s}_{hj}' \bm{e}_j \bm{e}_j' \bm{s}_{ij} \bm{s}_{hk} \bm{e}_k \bm{e}_k' \bm{s}_{ik}} - 2,
\end{equation}
where $a_{jk} = \left(1 + 2(j=k)\right)^{-1}$ and $\bm{s}_{hj}$ is the $n_j \times 1$ sub-vector of $\bm{s}_h$ corresponding to cluster $j$.\todo{Note that Pan and Wall use a different degrees-of-freedom estimator, based on the sample variance of the variance contribution from each cluster. This makes sense in the GEE context because it avoids relying on normality of the errors.} 

Third, \citet{McCaffrey2006improved} proposed to use a saddlepoint approximation to the distribution of $Z$. 
Like the Satterthwaite approximation, the saddlepoint approximation is derived under the assumption that the errors are normally distributed. 
Rather than using the moments of $\bm{c}'\bm{V}^{CR}\bm{c}$, the saddlepoint instead uses the fact that it is distributed as a weighted sum of $\chi^2_1$ random variables. 
The weights depend on $\bs\Sigma$, and so must be estimated. \citet{McCaffrey2006improved} did so based on a working model for the variance, in which case the weights are given by the eigen-values of the $m \times m$ matrix with $(i,j)^{th}$ entry $\bm{s}_i'\hat{\bs\Phi} \bm{s}_j$. 
An empirical approach could also be considered, in which case the weights are given by the eigen-values of the $m \times m$ matrix with $(i,j)^{th}$ entry $\sum_{k=1}^m \bm{s}_{ik}'\bm{e}_k \bm{e}_k' \bm{s}_{ij}$.

A final approach is to use a bootstrap re-sampling technique that leads to small-sample refinements in the test rejection rates. 
Not all bootstrap re-sampling methods work well in small samples. 
Among the alternatives, \citet{Webb2013wild} describe a wild boostrap procedure that performs well even when $m$ is very small and when clusters are of unequal size.  

\subsection{Examples}
\label{subsec:examples_t}

\subsection{Simulation evidence}
\label{subsec:simulation_t}

\section{MULTIPLE-CONSTRAINT TESTS}

While t-tests of single coefficients are surely more common, tests of multiple constraints are also of interest for empirical data analysis. 
Examples of such tests include robust Hausmann-type endogeneity tests \citep{Arellano1993on}, tests for non-linearities in exogeneous variables in OLS models, tests for pre-treatment balance on covariates in randomized experiments, and tests of parameter restrictions in seemingly unrelated regression.
We will consider linear constraints on $\bs\beta$, where the null hypothesis has the form $H_0: \bm{C}\bs\beta = \bm{d}$ for fixed $q \times p$ matrix $\bm{C}$ and $q \times 1$ vector $\bm{d}$. 
The Wald statistic is then \[
Q = \left(\bm{C}\bs{\hat\beta} - \bm{d}\right)'\left(\bm{C} \bm{V}^{CR} \bm{C}'\right)^{-1}\left(\bm{C}\bs{\hat\beta} - \bm{d}\right),
\]
where $\bm{V}^{CR}$ is one of the CRVEs described in the previous section. The asymptotically valid Wald test rejects $H_0$ at level $\alpha$ if $Q$ exceeds $\chi^2(\alpha; q)$, the $\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom.\todo{Citations to evidence that asymptotic test is way too liberal}

\subsection{Small-sample correction}

Compared to single-constraint tests involving $t$, fewer approaches to small-sample correction are available for multiple-constraint tests. 
The saddlepoint approximation is not applicable due to the more complex structure of $Q$, which involves the matrix inverse of $\bm{V}^{CR}$. 
A simple correction, analogous to the first approach for t-tests, would be to compare $Q / q$ to an $F(q, m - 1)$ reference distribution. 
The wild bootstrap for clustered data \citep{Webb2013wild} is also directly applicable to multiple-constraint tests, though to our knowledge its small-sample performance has not been assessed.\todo{Worth mentioning the Cameron and Miller ad hoc approximation?} 

Several small-sample corrections for multiple-constraint Wald tests have been proposed that involve an $F$ reference distribution with denominator degrees of freedom that are determined from the data. 
These approximations can thus be seen as generalizations (loosely speaking) of the Satterthwaite approximation. 
Working in the context of CRVE for generalized estimating equations, \cite{Pan2002small} proposed to approximate the distribution of $\bm{C}\bm{V}^{CR} \bm{C}'$ by a multiple of a Wishart distribution, from which it follows that $Q$ approximately follows a multiple of an F distribution. 
Specifically, if $\eta \bm{C}\bm{V}^{CR} \bm{C}'$ approximately follows a Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C} \Var\left(\bm{C}\bs{\hat\beta}\right)\bm{C}'$, then 
\begin{equation}
\label{eq:AHT}
\left(\frac{\eta - q + 1}{\eta q}\right) Q \ \dot\sim \ F(q, \eta - q + 1).
\end{equation}
We will refer to this as the approximate Hotelling's $T^2$ (AHT) test.

Just as in the Satterthwaite approximation, the degrees of freedom of the Wishart distribution are chosen to match the mean and variance of $\bm{C}\bm{V}^{CR} \bm{C}'$. 
However, when $q > 1$ it is not possible to exactly match both moments. 
\cite{Pan2002small} propose to use as degrees of freedom the value that minimizes the squared differences between the covariances among the entries of $\eta \bm{C}\bm{V}^{CR}\bm{C}'$ and the covariances of the Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C}\bm{V}^{CR}\bm{C}'$. 
\citet{Zhang2012two-wayANOVA, Zhang2012MANOVA, Zhang2013tests} proposed a simpler method in the context of heteroskedastic and multivariate analysis of variance models, which is a special case of the linear regression model considered here. 
The simpler approach involves matching the mean and total variance of $\bm{C}\bm{V}^{CR}\bm{C}'$ (i.e., the sum of the variances of its entries), which avoids the need to calculate any covariances.
Let $\bm{c}_1,...,\bm{c}_q$ denote the $p \times 1$ row-vectors of $\bm{C}$.
Denote the entries of $\bm{C}\bm{V}^{CR}\bm{C}'$ as $v_{st} = \bm{c}_s'\bm{V}^{CR}\bm{c}_t$, for $s,t=1,...,q$. 
The degrees of freedom are then given by 
\begin{equation}
\label{eq:eta_totalvariance}
\eta = \frac{\sum_{s,t=1}^q \left[\E^2\left(v_{st}\right) + \E\left(v_{ss}\right)\E\left(v_{tt}\right)\right]}{\sum_{s,t=1}^q \Var\left(v_{st}\right)},
\end{equation}
which reduces to (\ref{eq:nu_Satterthwaite}) if $q = 1$.

In practice, the moments of $v_{st}$ must be estimated. 
As with single-case tests, both model-based and empirical estimates can be considered.
Let $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h'\bm{A}_h'\bm{W}_h\bm{X}_h\bm{M}\bm{c}_s$ for CR2a or $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h'\bm{W}_h\bm{X}_h\bm{D}_h'\bm{M}\bm{c}_s$ for CR2b, for $s = 1,...,q$ and $h = 1,...,m$. 
Let $\bm{t}_{shj}$ be the $n_j \times 1$ sub-vector of $\bm{t}_{sh}$ corresponding to cluster $j$, for $j = 1,...,m$. Assuming that the errors $\bs\epsilon_1,...,\bs\epsilon_m$ are normally distributed with variance $\bs\Phi_j$, the model-based degrees of freedom are then given by
\begin{equation}
\label{eq:eta_model}
\eta_M = \frac{\sum_{s,t=1}^q \sum_{h,i=1}^m b_{st} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{th} \bm{t}_{si}'\hat{\bs\Omega}\bm{t}_{ti}}{\sum_{s,t=1}^q \sum_{h,i=1}^m \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} + \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{si} \bm{t}_{th}'\hat{\bs\Omega}\bm{t}_{ti}},
\end{equation}
where $b_{st} = 1 + (s=t)$ for $s,t=1,..,q$.
Still assuming normality of the errors, an empirical degrees of freedom estimate is given by using $v_{st}$ as an estimate of $\E\left(v_{st}\right)$ and estimating $\bs\Sigma$ using the residuals: 
\begin{equation}
\label{eq:eta_empirical}
\eta_E = \frac{\sum_{s,t=1}^q b_{st} v_{st}^2}{\sum_{s,t=1}^q \sum_{h,i,j,k=1}^m a_{jk}\left(\bm{t}_{shj}'\bm{e}_j \bm{e}_j'\bm{t}_{tij} \bm{t}_{shk}' \bm{e}_k \bm{e}_k' \bm{t}_{tik} + \bm{t}_{shj}'\bm{e}_j \bm{e}_j'\bm{t}_{sij} \bm{t}_{thk}' \bm{e}_k \bm{e}_k' \bm{t}_{tik}\right)},
\end{equation}
with $a_{jk} = \left(1 + 2(j=k)\right)^{-1}$.
\cite{Pan2002small} use a different method for estimating $\Var(v_{st})$, based on the sample variance of the contribution of each individual cluster to $v_{st}$; this method thus avoids relying on the assumption that the errors are normally distributed.\todo{Which makes sense in the GEE context.} 
Following their approach to estimating the total variance, a final degrees of freedom estimate is
\begin{equation}
\label{eq:eta_sample}
\eta_S = \frac{\sum_{s,t=1}^q b_{st} v_{st}^2}{\frac{m}{m-1}\sum_{s,t=1}^q \sum_{j=1}^m \left(\bm{u}_{sj}'\bm{e}_j \bm{e}_j'\bm{u}_{tj} - v_{st} / m\right)^2},
\end{equation}
where $\bm{u}_{sj} = \bm{W}_j \bm{X}_j \bm{M}\bm{c}_s$ for CR0, $\bm{u}_{sj} = \bm{A}_j'\bm{W}_j \bm{X}_j \bm{M}\bm{c}_s$ for CR2a, $\bm{u}_{sj} = \bm{W}_j \bm{X}_j \bm{D}_j \bm{M}\bm{c}_s$ for CR2b, or $\bm{u}_{sj} = \left(\bm{I}_j - \bm{H}_{jj}\right)^{-1} \bm{W}_j \bm{X}_j \bm{M}\bm{c}_s$ for CR3.

\subsection{Examples}
\label{subsec:examples_F}

In this section we examine three short examples of F-tests, spanning a variety of applied contexts. In each example, we illustrate how the proposed small-sample tests can be used and how they can differ from the conventional asymptotic Wald tests. R code and data files are available for each analysis as an online supplement.

\subsubsection{Tennessee STAR class-size experiment.} 

The Tennessee STAR class size experiment is one of the most well studied interventions in education.  In the experiment, K – 3 students and teachers were randomized within each of 79 schools to one of three conditions: small class-size (targetted to have 13-17 students), regular class-size, or regular class-size with an aide (see Schazenbach, 2006 for a review). Analyses of the original study and follow up waves have found that being in a small class improves a variety of outcomes, including higher test scores (e.g., Krueger, 1999), increased likelihood of taking college entrance exams \citep{Krueger2001effect}, and increased rates of home ownership and earnings (e.g., Chetty et al, 2010). 

The class-size experiment consists of three treatment conditions and multiple, student-level outcomes of possible interest. A  – to illustrate the use of F-tests in seemingly unrelated regression (SUR) models. SUR models are commonly used to address multiple comparison problems, making them particularly useful in the analysis of experimental results.  In this example, we focus on testing if there are treatment effects across three Kindergarten outcomes using three models (k = 1,2,3) of the form,
	Yijk = Zi’γk + Xi’β + ϵijk
For outcome k, in each model student i is found in classroom j; Zi includes dummies for the small-class and regular-plus-aide conditions; and the vector Xi includes a set of student demographics (i.e., free or reduced lunch status; race; gender; age). In order to make the scales comparable across the three outcomes, following Krueger (1999) we converted the reading, word recognition, and math scores to percentile rankings based upon their distributions in the control condition. Finally, instead of including school fixed effects, we centered both the outcomes and covariates (a “within” specification).

\subsection{Simulation evidence}
\label{subsec:simulation_F}

\section{DISCUSSION}
\label{sec:discussion}

\appendix
\section{Distribution theory for $\bm{V}^{CR}$}
\label{app:VCR_dist}

The small-sample approximations for t-tests and F-tests both involve the distribution of the entries of $\bm{V}^{CR}$. This section explains the relevant distribution theory.

First, note that any of the CRVEs can be written in the form $\bm{V}^{CR} = \sum_{j=1}^M \bm{T}_j \bm{e}_j \bm{e}_j' \bm{T}_j'$ for some $p \times n_j$ matrices $\bm{T}_j$. 
The form of the $\bm{T}_j$ matrices depends on which variance estimator is used: $\bm{T}_j = \bm{M}\bm{X}_j' \bm{W}_j$ for CR0, $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \bm{A}_j$ for CR2a, $\bm{T}_j = \bm{M} \bm{D}_j \bm{X}_j' \bm{W}_j$ for CR2b, and $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \left(\bm{I}_j - \bm{H}_{jj}\right)^{-1}$ for CR3.

Next, let $\bm{u}_1,\bm{u}_2,\bm{u}_3,\bm{u}_4$ be fixed, $p \times 1$ vectors and consider the linear combination $\bm{u}_1' \bm{V}^{CR} \bm{u}_2$. 
\citet[Theorem 4]{Bell2002bias} show that the linear combination is a quadratic form in $\bm{Y}$: \[
\bm{u}_1' \bm{V}^{CR} \bm{u}_2 = \bm{Y}'\left(\sum_{j=1}^m \bm{t}_{2j} \bm{t}_{1j}'\right) \bm{Y}, \]
for $N \times 1$ vectors $\bm{t}_{xj} = \left(\bm{I} - \bm{H}\right)_j' \bm{T}_j' \bm{u}_x$, $x = 1,...,4$, and $j = 1,...,m$. 

Standard results regarding quadratic forms can be used to derive the moments of the linear combination. We now assume that $\bs\epsilon_1,...,\bs\epsilon_m$ are multivariate normal with zero mean and variance $\bs\Sigma$. It follows that 
\begin{align}
\label{eq:CRVE_expectation}
\E\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2\right) &= \sum_{j=1}^m \bm{t}_{1j}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_variance}
\Var\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2\right) &= \sum_{i=1}^m \sum_{j=1}^m \left(\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}\right)^2 + \bm{t}_{1i}' \bs\Sigma \bm{t}_{1j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_covariance}
\Cov\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2, \bm{u}_3' \bm{V}^{CR} \bm{u}_4\right) &= \sum_{i=1}^m \sum_{j=1}^m \bm{t}_{1i}' \bs\Sigma \bm{t}_{4j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{3j} + \bm{t}_{1i}' \bs\Sigma \bm{t}_{3j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{4j}.
\end{align}
Furthermore, the distribution of $\bm{u}_1' \bm{V}^{CR} \bm{u}_2$ can be expressed as a weighted sum of $\chi^2_1$ distributions, with weights given by the eigen-values of the $m \times m$ matrix with $\left(i,j\right)^{th}$ entry $\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}$, $i,j=1,...,m$.

\section{CR2 invariance}
\label{app:theorem1}

Under certain circumstances, the use of (\ref{eq:CR2_panel_adjustment}) is unnecessary and CR2 can be calculated based solely on the absorbed model. In models estimated by WLS with weights that are the inverse of the working model, if absorption is performed only for the fixed effects that are equivalent to (or nested within) the units on which clustering is defined, then the adjustment matrices can be calculated directly from Equations (\ref{eq:CR2a_adjustment}) and (\ref{eq:CR2a_Bmatrix}), using $\bm{H_{\ddot{R}}}$ in place of $\bm{H}$. This result is formalized in the following theorem.

\paragraph{Theorem 1.} Consider model (\ref{eq:fixed_effects}) and assume that $\bm{W}_j = \bs{\hat\Phi}_j$ for $j = 1,...,m$. Let $\bm{\ddot{V}}^{CR2}$ be the CR2 matrix calculated based on the absorbed model, i.e., 
\[
\bm{\ddot{V}}^{CR2} = \bm{M_{\ddot{R}}}\left(\sum_{j=1}^m \bm{\ddot{R}}_j'\bm{W}_j \bm{\ddot{A}}_j \bm{e}_j \bm{e}_j' \bm{\ddot{A}}_j' \bm{W}_j \bm{\ddot{R}}_j\right) \bm{M_{\ddot{R}}},
\]
where \[
\bm{\ddot{A}}_j = {\bs{\hat\Phi}_j^C}' \bm{\ddot{B}}_j^{-1/2}\hat{\bs\Phi}_j^C \]
and $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'$.
Let $\bm{J}$ be the $p \times r$ matrix that selects the covariates of interest, i.e., $\bm{X}\bm{J} = \bm{R}$ and $\bm{J}'\bs\beta = \bs\alpha$. 
Assume that $\bm{S}_i \bm{M_S}\bm{S}_j'\bm{W}_j = \bm{0}$ for every $i \neq j$. Then $\bm{\ddot{V}}^{CR2} = \bm{J}'\bm{V}^{CR2}\bm{J}$.\todo{Is this even true?}

\paragraph{Proof.} Formulas for the inverse of a partitioned matrix can be used to demonstrate that $\bm{X}_j\bm{M}\bm{J} = \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}$. Thus, equivalence of $\bm{\ddot{V}}^{CR2}$ and $\bm{J}'\bm{V}^{CR2}\bm{J}$ follows if $\bm{A}_j = \bm{\ddot{A}}_j$ for $j = 1,...,m$.

From the fact that $\bm{\ddot{R}}_j'\bm{W}_j\bm{S}_j = \bm{0}$ for $j = 1,...,m$, it follows that \begin{align*}
\bm{B}_j &= \bs{\hat\Phi}_j^C \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'\\
&= \bs{\hat\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j' {\bs{\hat\Phi}_j^C}' \\
&= \bs{\hat\Phi}_j^C \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right){\bs{\hat\Phi}_j^C}'
\end{align*}
and 
\begin{equation}
\label{eq:B_j_inverse}
\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1}\left(\bs{\hat\Phi}_j^C\right)^{-1}.
\end{equation}
Let $\bm{U}_j = \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j'\right)^{-1}$.
From the Sherman-Morrison-Woodbury identity, $\bm{U}_j = \bm{W}_j - \bm{W}_j \bm{\ddot{R}}_j\bm{M}_{\ddot{R}(j)}\bm{\ddot{R}}_j'\bm{W}_j$, where $\bm{M}_{\ddot{R}(j)} = \left(\bm{\ddot{R}}'\bm{W}\bm{\ddot{R}} - \bm{\ddot{R}}_j'\bm{W}_j\bm{\ddot{R}}_j\right)^{-1}$.\todo{Does this hold when $\bm{M}_{\ddot{R}(j)}$ is not of full rank?}
It follows that $\bm{U}_j \bm{S}_j = \bm{W}_j \bm{S}_j$. 
Another application of the Sherman-Morrison-Woodbury identity gives 
\begin{align*}
\left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1} &= \bm{U}_j + \bm{U}_j \bm{S}_j\left(\bm{S}_j' \bm{U}_j \bm{S}_j - \bm{S}'\bm{W}\bm{S}\right)^{-1} \bm{S}_j\bm{U}_j \\
&= \bm{U}_j - \bm{W}_j \bm{S}_j \bm{M_{S(j)}} \bm{S}_j'\bm{W}_j \\
&= \bm{U}_j
\end{align*}
where $\bm{M_{S(j)}} = \left(\bm{S}'\bm{W}\bm{S} - \bm{S}_j' \bm{W}_j \bm{S}_j\right)^{-1}$. The last equality follows from the fact that $\bm{S}_j \bm{M_{S(j)}}\bm{S}_j' = \bm{0}$ because the fixed effects are nested within clusters. Substituting into (\ref{eq:B_j_inverse}), we then have that \[
\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \bm{U}_j \left(\bs{\hat\Phi}_j^C\right)^{-1}.\]
Now, $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C \bm{U}_j^{-1} {\bs{\hat\Phi}_j^C}'$ and so $\bm{\ddot{B}}_j^{-1} = \bm{B}_j^{1}$. It follows that $\bm{\ddot{A}}_j = \bm{A}_j$ for $j = 1,...,m$. 

\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}
