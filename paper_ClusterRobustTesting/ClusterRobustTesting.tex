\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.7in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage[textwidth=1in, textsize=tiny]{todonotes}
%\usepackage[disable]{todonotes}

\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\usepackage{Sweave}
\begin{document}
\input{ClusterRobustTesting-concordance}
%\SweaveOpts{concordance=TRUE}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Small sample hypothesis testing using cluster-robust variance estimation}
  \author{\\James E. Pustejovsky\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Educational Psychology \\ 
    University of Texas at Austin\\ \\
    and \\ \\
    Elizabeth Tipton \\
    Department of Human Development \\ 
    Teachers College, Columbia University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Small sample hypothesis testing using cluster-robust variance estimations}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{INTRODUCTION}
\label{sec:intro}

\begin{Schunk}
\begin{Sinput}
> library(knitr)
> library(xtable)
> # set global chunk options
> opts_chunk$set(echo = FALSE, cache = FALSE, fig.path='CR_fig/', fig.align='center', fig.show='hold')
\end{Sinput}
\end{Schunk}

While the focus of much economics research is on understanding the causes and correlates of the behaviors of individuals, the data encountered in empirical applications is often clustered. 
For example, individuals are often clustered by countries, regions, or states; by firms, organizations, or schools; or by time-periods or follow-up waves.\todo{Are we trying to reference the panel data case here? Clustering by time period doesn't seem quite right.} 
This clustering is typically accounted for in analyses through the use of cluster robust variance estimation (CRVE), an analog to the heteroscedasticity robust standard errors developed by Huber (1967)\todo{Add citation}, \citet{eicker1967limit}, and \citet{White1980heteroskedasticity} to account for non-constant variance in ordinary least squares. 
The use of CRVE is widespread, as evidenced by the large number of citations to key articles in the field (e.g., 849 cites for \citet{Wooldridge2003cluster}), 
the large number of citations overall (i.e., over 11,000 for "clustered standard errors" in Google Scholar), and the large number of articles employing the methods in economics journals (i.e., over 500 citations). 

CRVE is routinely used in order to test hypotheses involving either individual coefficients or sets of multiple constraints on the regression specification. 
The theory behind CRVE is asymptotic in the number of clusters, and recently, researchers have turned attention to the performance of these tests in small and moderate samples. 
\citet{Cameron2015practitioners} provide a thorough review of this literature, including a discussion of current practice, possible solutions, and open problems. 
They highlight well known results that in small samples, conventional CRVE has a downward bias and hypothesis tests based on CRVEs can have Type-I error rates that are far from the nominal level of the test.
Moreover, they review recent research showing that the small-sample corrections for t-tests typically found in software such as Stata and SAS are inadequate. 
In the course of reviewing a variety of recent proposals for addressing these problems, Cameron and Miller highlight a potentially promising method called bias-reduced linearization method (BRL), introduced by \citet{McCaffrey2001generalizations} and \citet{Bell2002bias}. 
BRL entails correcting the downward bias of the most common CRVE so that it is exactly unbiased under a working model specified by the analyst, while also remaining asymptotically consistent under arbitrary true variance structures. 
Simulations reported \citet{Bell2002bias} demonstrate that the BRL correction serves to reduce the bias of the CRVE even when the working model is mis-specified. 
The same authors also proposed and studied small-sample corrections to t-tests based on the BRL variance estimator, based on Satterthwaite \citet{Bell2002bias} or saddlepoint approximations \citep{McCaffrey2006improved}.

Despite promising simulation evidence that BRL performs well \citep[e.g.,][]{Imbens2012robust}, several problems arise in implementing the method in practice. 
Two of these problems arise in the analysis of panel data, where, following Bertrand et al (2004), it is considered best practice to account for clustering both as a fixed effect and a random effect (through CRVE) in analyses. 
One approach to do so is to include dummies for each cluster are in the analysis. However, \citet{Angrist2009mostly} argue, if this approach is used, the BRL adjustment breaks down and cannot be implemented.
A second approach (which is computationally more efficient) is to instead absorb the fixed effects. As \citet{Cameron2015practitioners} highlight, however, this absorption approach can lead to sometimes substantially different standard errors. 
Finally, while \citet{Bell2002bias} provide a method for conducting single parameter tests, no such small-sample method has been provided for multiparameter tests.
These tests occur commonly in the broader economics literature and are found not only in panel data (e.g., the Hausman test), but also more broadly in seemingly unrelated regression models, and when analyzing experimental data (e.g., baseline equivalence), particularly when there are multiple treatment groups. 

In this paper, we address each of these three concerns, in the end articulating a BRL methodology that is suitable for everyday econometric practice. 
To do so, we begin by reviewing the the small sample CRVE method that is standard in practice in most software applications. 
In the next section of the paper, we review the BRL correction to the CRVE estimator, and present advances that address the first two concerns above. 
First, we demonstrate that using generalized inverses to calculate the BRL adjustment matrices address the rank-deficiency problem that arises when including cluster fixed effects, while retaining the property that CRVEs based on the adjustment matrices are unbiased under a working model. 
Second, we describe how to apply BRL when fitting models that absorb the cluster fixed effects prior to parameter estimation. 
Here we prove that under a particular parameterization, the BRL based CRVE estimator is the same regardless of the estimation method used.
In the next section of the paper, we address the use of CRVE for hypothesis testing.
Here we propose a method of testing multiple-constraint hypotheses (i.e., F-tests) based on CRVE with the BRL adjustments, and show that the t-test proposed by \citet{Bell2002bias} is a special case. 
In support of this third aim, we provide simulation evidence that the proposed small-sample F-test offers drastic improvements over commonly implemented alternatives and performs comparably with current state-of-the-art methods such as the cluster-wild bootstrap procedure described by \citet{Cameron2008bootstrap} and \citet{Webb2013wild}. 
To date, the Wild bootstrap (and other resampling methods) are the 'best practice' with small samples, and we show that the BRL method performs just as well statistically.\todo{Does it?}
We conclude the paper with a set of three examples comparing results from these three approaches to illustrate the breadth of application, and a discussion of important considerations for practice.  
In the discussion section that follows, we then highlight why the BRL approach given here is potentially more useful in practice, and should become the standard default CRVE method used in all analyses in econometrics.

%\begin{itemize}
%\item \citet{Brewer2013inference}
%\item \citet{Carter2013asymptotic}
%\item \citet{Ibragimov2010tstatistic}
%\item \citet{Kezdi2004robust}
%\item \citet{Kline2012score}
%\end{itemize}

\section{STANDARD CLUSTER-ROBUST VARIANCE ESTIMATION}
\label{sec:CRVE}

\subsection{Econometric framework}

We will consider linear regression models in which the errors within a cluster have an unknown variance structure. 
Suppose that there are $j=1,...,m$ clusters, each with $n_j$ observations. In cluster $j$ and observation $i$, assume that the outcome $y_{ij}$ is related to a vector of $p$ covariates $\bm{x}_{ij}$ by
\begin{equation}
\label{eq:model_vector}
y_{ij} = \bm{x}_{ij}' \bs\beta + \bs\epsilon_{ij}.
\end{equation}
By stacking the outcomes, covariate vectors, and errors, the model can be written more compactly as,
\begin{equation}
\label{eq:model_vector}
\bm{Y}_j = \bm{X}_j \bs\beta + \bs\epsilon_j,
\end{equation}
where $\bm{Y}_j$ is $n_j \times 1$, $\bm{X}_j$ is an $n_j \times p$ matrix of regressors for cluster $j$, $\bs\beta$ is a $p \times 1$ vector, and $\bs\epsilon_j$ is an $n_j \times 1$ vector of errors. 
Importantly, the covariate matrix $\bm{X}_j$ can include a wide variety of covariate forms, including those that vary at the cluster or observation level, as well as fixed effects for each cluster (or groups within each cluster).

We assume that $\E\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bm{0}$ and $\Var\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bs\Sigma_j$, for $j = 1,...,m$, where the form of $\bs\Sigma_1,...,\bs\Sigma_m$ may be unknown but the errors are independent across clusters. 
In many cases, the errors are assumed to follow some known structure, $\Var\left(\bm{e}_j\left|\bm{X}_j\right.\right) = \bs\Phi_j$, where $\bs\Phi_j$ is a known function of a low-dimensional parameter.\todo{Is this the right place to introduce the working model?} %and $\bs\Phi = \bigoplus_{j=1}^m \bs\Phi_j$. 

%let $\bm{I}$ denote an $N \times N$ identity matrix, 
%Let $\bm{X} = \left(\bm{X}_1',\bm{X}_2',...,\bm{X}_m'\right)'$ and $\bs\Sigma = \bigoplus_{j=1}^m \bs\Sigma_j$. 
%Additionally, let $N = \sum_{j=1}^m n_j$, and let $\bm{I}_j$ denote an $n_j \times n_j$ identity matrix.

We shall consider estimating the vector of regression coefficients $\bm\beta$ using weighted least squares (WLS). For each cluster $j$, let $\bm{W}_j$ be a symmetric, $n_j \times n_j$ weighting matrix. The WLS estimate can then be written as
\begin{equation}
\label{eq:WLS}
\bs{\hat\beta} = \bm{M} \sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{Y}_j, 
\end{equation}
where $\bm{M} = \left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{X}_j\right)^{-1}$. %Let $\bm{W} = \bigoplus_{j=1}^m \bm{W}_j$. 
This WLS framework includes the unweighted case (where $\bm{W}_j = \bm{I}_j$, the identity matrix), as well as feasible GLS. In the latter case, the weighting matrices are then taken to be $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$, where the $\hat{\bs\Phi}_j$ are constructed from estimates of the variance parameter.\footnote{
The WLS estimator also encompasses the estimator proposed by \citet{Ibragimov2010tstatistic} for clustered data. 
Assuming that $\bm{X}_j$ has rank $p$ for $j = 1,...,m$, their proposed approach involves estimating $\bs\beta$ separately within each cluster and taking the simple average of these estimates. 
The resulting average is equivalent to the WLS estimator with weights $\bm{W}_j = \bm{X}_j \left(\bm{X}_j'\bm{X}_j\right)^{-2} \bm{X}_j$.}

The variance of the WLS estimator is 
\begin{equation}
\label{eq:var_WLS}
\Var\left(\bs{\hat\beta}\right) = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bs\Sigma_j \bm{W}_j\bm{X}_j\right) \bm{M},
\end{equation}
which depends upon the unknown variance matrices $\bm\Sigma_j$. 
One approach to estimating this variance is model-based. 
In this approach, it is assumed that $\Var\left(\bm{e}_j\left|\bm{X}_j\right.\right) = \bs\Phi_j$, where $\bs\Phi_j$ is a known function of a low-dimensional parameter, which is then estimated. 
For example, a hierarchical error structure is common, wherein observations in the same cluster share a random effect. 
If this approach is used, each $\bs\Sigma_j$ is substituted with the estimate $\hat{\bs\Phi}_j$. 
If additionally $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$, the model-based variance estimator can be shown to simplify to $\bm{V}^M = \bm{M}$. 
However, if the working model is mis-specified, the model-based variance estimator will be inconsistent and inferences based upon it will be invalid. 

\subsection{Standard CRVE}

Cluster-robust variance estimators provide a means of estimating $\Var\left(\bs{\hat\beta}\right)$ and testing hypotheses regarding $\hat{\bs\beta}$ in the absence of a valid working model for the error structure, or when the working variance model used to develop weights may be mis-specified. 
They are thus a generalization of heteroskedasticity-consistent (HC) variance estimators \citep{MacKinnon1985some}. 
Like the HC estimators, several different variants have been proposed, with different rationales and different finite-sample properties. Each of these are of the form
\begin{equation}
\label{eq:V_small}
\bm{V}^{R} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{A}_j \bm{e}_j \bm{e}_j' \bm{A}_j' \bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
for some $n_j$ by $n_j$ adjustment matrix $\bm{A}_j$. The form of these adjustments parallels those of the heteroscedastity-consistent (HC) variance estimators proposed by \citet{MacKinnon1985some}. Letting $\bm{A}_j = \bm{I}_j$, the identity matrix, results in the original CRVE estimator; following Cameron and Miller (2015), we refer to this estimator as $\bm{V}^{CR0}$. If instead, we set $\bm{A}_j = c\bm{I}_j$, where $c = \sqrt{(m/(m-1))(N/(N - p))}$, where $N = \sum_{j=1}^m n_j$, this results in the CRV1 estimator, $\bm{V}^{CR1}$. Note that when $N$ is large, here $c \approx \sqrt{m/(m-1)}$; this correction is the most commonly implemented in practice (e.g., including Stata, SAS). Importantly, this correction does not depend on $\bm{X}_j$ and is the same for all hypotheses tested. Like the CR0 estimator, however, this estimator often under-estimates the true variance.

There are two alternative small-sample corrections that are used with CRVE. In addition to CR2 (the BRL approach) which we introduce in the next section, the jackknife estimator (CR3) is also used. Whereas the CR0 and CR1 under-estimate the variance, however, this CR3 estimator over-estimates the variance (see Bell and McCaffrey).
As we show next, the BRL approach thus sits between the CR1 and CR3 estimators, providing a nearly unbiased method for estimating the variance. 


\section{BIAS REDUCED LINEARIZATION}
\label{sec:BRL}

The BRL method provided by Bell and McCaffrey (2002) can be seen as a generalization of the CR2 estimator provided by MacKinnon and White in the heteroscedastic error case. In this approach, the The $\bm{V}^{CR2}$ estimator defines $\bm{A}_j$ as the matrix that satisfies,
\begin{equation}
\label{eq:CR2_criterion}
\bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \bs{\Sigma} \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j =  \bm{W}_j \bs{\Sigma}_j \bm{W}_j,
\end{equation}
where $\bm{H} = \bm{X}\bm{M}\bm{X}'\bm{W}$, and $\left(\bm{I} - \bm{H}\right)_j$ denotes the rows of $\bm{I} - \bm{H}$ corresponding to cluster $j$. \todo{Need to define X,W}. 
%\bm{X}_j' \bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \bs{\Sigma} \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j \bm{X}_j = \bm{X}_j' \bm{W}_j \bs{\Sigma}_j \bm{W}_j \bm{X}_j,
Importantly, determining $\bm{A}_j$ depends on knowledge of $\bs{\Sigma}_j$, which is unknown (and thus the reason for using the CRVE approach). 
In order to make progress, Bell and McCaffrey proposed to define $\bm{A}_j$ under an assumed structure to $\bs{\Sigma}_j$, known as a "working" model. 
When this working model (which we now call $\bs\Phi_j$) is correct and $\bm{A}_j$ is defined following (Eqn), then it can be shown that the $\bm{V}^{CR2}$ estimator is unbiased for $\bm{V}$ (see Eqn X of BM 2002). 
When the assumed structure deviates from the true covariance $\bs{\Sigma}_j$, the estimator remains biased, though Bell and McCaffrey show that the bias is greatly reduced (thus the name "bias reduced linearization"). 
Furthermore, as the number of clusters increases, the reliance on this working model diminishes. 
One way to think of this approach then, is that it provides scaffolding that while necessary in the small sample case, falls away when there is sufficient data.
Importantly, extensive simulation results indicate that this bias is typically minimal, even for large deviations from the assumed structure (CITE).

Following previous notation, this focus on a working model means we can write $\bs{\Sigma}_j = \bs{\Phi}_j$, which is a low-level function of variance parameters that can be estimated. Bell and McCaffrey further note that the criterion (\ref{eq:CR2_criterion}) does not uniquely define $\bm{A}_j$. Based on extensive simulations, \citet{McCaffrey2001generalizations} found that a symmetric solution worked well, with 
\begin{equation}
\label{eq:CR2_adjustment}
\bm{A}_j = \left(\hat{\bs\Phi}_j^C\right)' \bm{B}_j^{-1/2}\hat{\bs\Phi}_j^C,
\end{equation}
where $\hat{\bs\Phi}_j^C$ is the upper triangular Cholesky factorization of $\hat{\bs\Phi}_j$, 
\begin{equation}
\label{eq:CR2_Bmatrix}
\bm{B}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \left(\hat{\bs\Phi}_j^C\right)',
\end{equation}
and $\bm{B}_j^{-1/2}$ is the inverse of the symmetric square root of $\bm{B}_j $. To be more concrete, in the simplest case of ordinary (unweighted) least squares in which the working variance model posits that the errors are all independent and homoskedastic, then we can show that $\bm{W} = \bs\Phi = \bm{I}$ and $\bm{A}_j = \left(\bm{I}_j - \bm{X}_j\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}_j'\right)^{-1/2}$. In the remainder of this paper, we will focus on this BRL approach, using the $\bm{V}^{CR2}$ estimator throughout. 


\subsection{Panel data with fixed effects}
CRVE is commonly used in the analysis of panel data. Bertrand et al (2004) argued convincingly that when data is clustered, the clusters should be accounted for both as fixed effects and random effects (though use of CRVE). 
In doing so, the use of CRVE accounts for any remaining correlation not accounted for by mean differences, acting as a safeguard against model misspecification.
As \citet{Angrist2009mostly} show, however, if this approach is followed and clusters are included as fixed effects (i.e., dummies), the matrices $\bm{B}_1,...,\bm{B}_m$ may not be positive definite, so that $\bm{B}_j^{-1/2}$ cannot be calculated for every cluster.
This makes the BRL adjustment difficult to impossible to implement in practice.

A simple solution to this problem is instead of defining the $\bm{A}_j$ in relation to \ref{eq: CR2_criterion}, to instead define them in relation to the equation
\begin{equation}
\label{eq:CR2_criterion_new}
\bm{X}_j' \bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \bs{\Sigma} \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j \bm{X}_j = \bm{X}_j' \bm{W}_j \bs{\Sigma}_j \bm{W}_j \bm{X}_j.
\end{equation}
This equation, while similar to \ref{eq:CR2_criterion}, differs in that on either side of the equality, the terms are sandwiched within $\bm{X}$ matrices. The over-identification problem can thus be overcome by using a generalized inverse of $\bm{B}_j$. 



%A second, computational difficulty with CR2 is that it requires the inversion (or pseudo-inversion) of $m$ matrices, each of dimension $n_j \times n_j$. 
%Consequently, computation of CR2a will be slow if some clusters contain a large number of of individual units. 


\subsection{Absorbtion}
While the inclusion of cluster fixed effects is important for reduction of bias, doing so increases the number of parameters estimated in the model by $m$. 
A computational approach commonly used in software, therefore, is to instead "absorb" the fixed effects (also called "demeaning"). 
In this approach, both the left and right hand side of the equation is demeaned, with the final estimation based on the residualized versions of both the outcome and covariates. 

More formally, we can write this as,
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \epsilon_{jt} \]
for $j=1,...m$ and $t = 1,...,n_j$, where $\bm{r}_{ij}$ is an $r \times 1$ row vector of covariates. If the number and timing of the measurements is identical across cases, then the panel is balanced. Another common specification for balanced panels includes additional effects for each unique measurement occassion:
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \nu_t + \epsilon_{jt} \]
for $j=1,...,m$ and $t = 1,...,n$. 
In what follows, we consider a generic fixed effects model in which
\begin{equation}
\label{eq:fixed_effects}
\bm{y}_j = \bm{R}_j \bs\alpha + \bm{S}_j \bs\gamma + \bs\epsilon_j,
\end{equation}
where $\bm{R}_j$ is an $n_j \times r$ matrix of covariates, $\bm{S}_j$ is an $n_j \times s$ matrix describing the fixed effects specification, $\bm{X}_j = \left[\bm{R}_j \ \bm{S}_j\right]$, $\bs\beta = \left(\bs\alpha', \bs\gamma'\right)'$, and $p = r + s$. 
In this model, inferential interest is confined to $\bs{\alpha}$ and the fixed effects are treated as nuisance parameters. 

While models inclusion clusters as fixed effects or using absorbtion are known to give identical estimates of the $\bs{\alpha}$ of interest, as Cameron and Miller (2015) show, when using the CRVE estimator that is standard in software (CR1), the CRVE estimates themselves can differ. 
To see why, recall that in CR1 adjustments, $\bm{A}_j = \sqrt{((m/(m-1))(N/(N-p)))}$. Following this approach, the adjustment depends on $p$, which is larger when the estimates are includes as fixed effects (i.e., = $p1$ + $m$) and smaller when instead absorbtion is used (i.e., = $p1$). 
In cases in which the number of observations per cluster is small the differences can be quite large. 
For example, as Cameron and Miller indicate, when $n_j$ = 2 for all clusters, this can result in (CR1 based) standard errors over twice as large when using cluster fixed effects versus absorbtion. 
This difference leads researchers to make ad hoc decisions regarding the best standard errors to report.

As we will show here, under a particular parameterization, a benefit of using the BRL approach is that the CR2 estimator is not affected by the inclusion of clusters as fixed effects or through absorption. 
To see how, begin by noting that this problem is particular to small samples. 
That is, the CR0 estimator is algebraically equivelent whether clusters are included as fixed effects or absorbed. 
To see how, let $\bm{H_S} = \bm{S}\left(\bm{S}'\bm{W}\bm{S}\right)^{-1} \bm{S}'\bm{W}$, $\bm{\ddot{Y}} = \left(\bm{I} - \bm{H_S}\right)\bm{Y}$, $\bm{\ddot{R}} = \left(\bm{I} - \bm{H_S}\right)\bm{R}$, $\bm{M_{\ddot{R}}} = \left(\bm{\ddot{R}}' \bm{W} \bm{\ddot{R}}\right)^{-1}$, and $\bm{H_{\ddot{R}}} = \bm{\ddot{R}}\bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W}$. 
Using absorption, the WLS estimator of $\bs\alpha$ can be calculated as \[
\bs{\hat\alpha} = \bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W} \bm{\ddot{Y}}. \]
This estimator is algebraically equivalent to the corresponding sub-vector of $\bs{\hat\beta}$  calculated as in (\ref{eq:WLS}), based on the full covariate matrix $\bm{X}$. 
Furthermore, the residuals can be calculated from the absorbed model using $\bm{e} = \bm{\ddot{y}} - \bm{\ddot{R}} \bs{\hat\alpha}$.
Let $\bm{\ddot{V}}^{CR0}$ denote the CR0 estimator calculated using $\bm{\ddot{R}}$ in place of $\bm{X}$, $\bm{M_{\ddot{R}}}$ in place of $\bm{M}$, and $\bm{\ddot{e}} = $ in place of $\bm{e}$. It can be shown that $\bm{\ddot{V}}^{CR0}$ is algebraically equivalent to $\bm{V}^{CR0}$ calculated based on the full covariate matrix, as in CITE.

In small samples, this equivalence is not given. Like the standard CR1 approach, it is possible that the CR2 estimator will differ depending on whether it is calculated based on the quantities from the absorbed model or those from the full WLS model. 
It is thus useful to define it in such a way that the calculations based on the absorbed model yield algrebraically identical results to the calculations from the full WLS model. 
This can be accomplished by ensuring that the adjustment matrices given in Equation (\ref{eq:CR2_adjustment}) are calculated based on the full covariate matrix $\bm{X}$. Specifically, in models with fixed effects, the adjustment matrices are calculated as in (\ref{eq:CR2_adjustment}), but with 
\begin{equation}
\label{eq:CR2_panel_adjustment}
\bm{B}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' \left(\hat{\bs\Phi}_j^C\right)'.
\end{equation}
This formula avoids the need to calculate $\bm{H}$, which would involve inverting a $p \times p$ matrix. 

It is unnecessary to account for absorption of fixed effects under certain commonly occurring circumstances. Specifically, if the model is estimated using weighted least-squares with working inverse-variance weights, and if absorption is performed only for fixed effects that are equivalent to or nested within the units on which clusters are defined, then the adjustment matrices can be calculated directly from Equations (\ref{eq:CR2_adjustment}) and (\ref{eq:CR2_Bmatrix}), using $\bm{H_{\ddot{R}}}$ in place of $\bm{H}$. This result is formalized in the following theorem:

\paragraph{Theorem.} Consider model (\ref{eq:fixed_effects}) and let $\bm{\ddot{V}}^{CR2}$ be the CR2 matrix calculated based on the absorbed model, i.e., 
\[
\bm{\ddot{V}}^{CR2} = \bm{M_{\ddot{R}}}\left(\sum_{j=1}^m \bm{\ddot{R}}_j'\bm{W}_j \bm{\ddot{A}}_j \bm{e}_j \bm{e}_j' \bm{\ddot{A}}_j' \bm{W}_j \bm{\ddot{R}}_j\right) \bm{M_{\ddot{R}}},
\]
where $\bm{\ddot{A}}_j = {\bs{\hat\Phi}_j^C}' \bm{\ddot{B}}_j^{-1/2}\hat{\bs\Phi}_j^C$ and $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'$.
Let $\bm{J}$ be the $p \times r$ matrix that selects the covariates of interest, i.e., $\bm{X}\bm{J} = \bm{R}$ and $\bm{J}'\bs\beta = \bs\alpha$. 
Assume that $\bm{W}_j = \bs{\hat\Phi}_j^{-1}$ for $j = 1,...,m$ and that $\bm{S}_i \bm{M_S}\bm{S}_j'\bm{W}_j = \bm{0}$ for every $i \neq j$. Then $\bm{\ddot{V}}^{CR2} = \bm{J}'\bm{V}^{CR2}\bm{J}$.

Appendix \ref{app:theorem1} provides a proof. When the necessary conditions hold, this approach is preferable for reasons of numerical precision.


\section{HYPOTHESIS TESTING}
\label{sec:testing}

Wald-type test statistics based on CRVEs are often used to test hypotheses regarding the coefficients in the regression specification. 
Such procedures are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., $m \to \infty$). 
However, evidence from a wide variety of contexts indicates that the asymptotic results can be a very poor approximation when the number of clusters is small, even when small-sample corrections such as CR2 are employed \citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. 
Furthermore, the accuracy of asymptotic approximations depends on design features such as the degree of imbalance in the covariates, skewness of the covariates, and similarity of cluster sizes \citep{McCaffrey2001generalizations, Tipton2015small, Webb2013wild}. 
Consequently, no simple rule-of-thumb exists for what constitutes an adequate sample size to trust the asymptotic test. 

We will consider linear constraints on $\bs\beta$, where the null hypothesis has the form $H_0: \bm{C}\bs\beta = \bm{d}$ for fixed $q \times p$ matrix $\bm{C}$ and $q \times 1$ vector $\bm{d}$. 
For a general CRVE estimator, the Wald statistic  is then \[
Q = \left(\bm{C}\bs{\hat\beta} - \bm{d}\right)'\left(\bm{C} \bm{V}^{CR} \bm{C}'\right)^{-1}\left(\bm{C}\bs{\hat\beta} - \bm{d}\right).
\]
The asymptotically valid Wald test rejects $H_0$ at level $\alpha$ if $Q$ exceeds $\chi^2(\alpha; q)$, the $\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom. \todo{Citations to evidence that asymptotic test is way too liberal?} When samples are small, in standard practice instead the test $F = Q/q$ is often used with the CR1 estimator and the reference distribution $F(q, m - 1)$. 

The small-sample $F$ test given above can be seen as a parallel to the small-sample $t$-test commonly used in single-parameter hypothesis tests (e.g., $H_0: \beta_j$ = 0) using CRVE. That case is a special case of the $Q$ test, wherein \[
Z = \hat\beta_j/\sqrt{V_j^{CR}} 
\]
which, while asymptotically following a standard normal distribution, in small samples is instead assumed to follow a $t(m - 1)$ distribution. 
Bell and McCaffrey (2002) show, however, that even if CR2 is substituted instead, this test does not follow a $t(m - 1)$ distribution in small samples, and instead provide a method to empirically estimate the degrees of freedom using a Satterthwaite approximation. 
In this section, our approach is thus to extend this empirical degrees of freedom approach to develop an F-test with better small sample properties.

\subsection{Small-sample corrections for t-tests}
Our approach to developing a better small-sample F-test parallels that the t-test developed by Bell and McCaffrey (2002). 
In this section, we therefore review this approach. As noted, the standard test implemented in software uses the CR1 correction with the $t(m - 1)$ reference distribution. 
The first and surely most common approach is to compare $|Z|$ to the appropriate critical value from a $t$ distribution with $m - 1$ degrees of freedom. 
\citet{Hansen2007asymptotic} provided one justification for the use of a $t(m-1)$ reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \citep[see also][]{Donald2007inference}. 
\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values would be conservative (leading to rejection rates less than or equal to $\alpha$).
However, both of these arguments require that $\bm{c}'\bs\beta$ be separately identified within each cluster. 
Outside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \citep{Cameron2015practitioners}. 
Furthermore, this correction does not take into account that the distribution of $\bm{V}^{CR}$ is affected by the structure of the covariate matrix. 

In contrast, the approach developed by \citet{McCaffrey2001generalizations} is to instead estimate the degrees of freedom of the t-test using a Satterthwaite approximation \citep{Satterthwaite1946approximate}.
This approach compares $Z$ to a $t$ reference distribution, with degrees of freedom $\nu$ that are estimated from the data. 
Theoretically, the degrees of freedom should be 
\begin{equation}
\label{eq:nu_Satterthwaite}
\nu = \frac{2\left[\E\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)\right]^2}{\Var\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)}.
\end{equation}
Expressions for the first two moments of $\bm{c}'\bm{V}^{CR2}\bm{c}$ can be derived under the assumption that the errors $\bs\epsilon_1,...,\bs\epsilon_m$ are normally distributed; see Appendix \ref{app:VCR_dist}. 


In practice, both moments involve the variance structure $\bs\Sigma$, which is unknown. 
\citet{McCaffrey2001generalizations} proposed to estimate the moments based on the same working model as used to derive the adjustment matrices. 
A ``model-based'' estimate of the degrees of freedom is then calculated as 
\begin{equation}
\nu_{M} = \frac{\left(\sum_{j=1}^m \bm{s}_j' \hat{\bs\Phi} \bm{s}_j\right)^2}{\sum_{i=1}^m \sum_{j=1}^m \left(\bm{s}_i' \hat{\bs\Phi} \bm{s}_j\right)^2},
\end{equation}
where $\bm{s}_j = \left(\bm{I} - \bm{H}\right)_j'\bm{A}_j'\bm{W}_j\bm{X}_j\bm{M}\bm{c}$. 
Alternately, for any of the CRVEs one could instead use an empirical estimate of the degrees of freedom, constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$. 
However, \citet{Bell2002bias} found using simulation that the plug-in degrees of freedom estimate produced very conservative rejection rates. 

The \citet{McCaffrey2001generalizations} approach has been shown to perform well in a variety of conditions (CITE simulation studies). These studies encompass a variety of data generation processes and covariate types. Importantly, a key finding is that the degrees of freedom depend not only on the number of clusters $m$, but also on features of the covariates. When the covariate is balanced -- as occurs in balanced panels with a dichotomous covariate with the same proportion of ones in each cluster -- the degrees of freedom are $m - 1$ even in small samples. However, when the covariate exhibits large imbalances -- as occurs when the panel is not balanced or if the proportion of ones varies considerably from cluster to cluster -- these degrees of freedom can be tremendously smaller. Similarly, covariates with large leverage points can exhibit similar losses in terms of degrees of freedom. The result is that the small-sample corrections are required even when the number of clusters seems large, suggesting that this CR2 t-test be applied as a default in all CRVE based analyses. 

%Not sure where to put this stuff that follows. Doesn't belong here:
%Third, \citet{McCaffrey2006improved} proposed to use a saddlepoint approximation to the distribution of $Z$. 
%Like the Satterthwaite approximation, the saddlepoint approximation is derived under the assumption that the errors are normally distributed. 
%Rather than using the moments of $\bm{c}'\bm{V}^{CR}\bm{c}$, the saddlepoint instead uses the fact that it is distributed as a weighted sum of $\chi^2_1$ random variables. 
%The weights depend on $\bs\Sigma$, and so must be estimated. \citet{McCaffrey2006improved} did so based on a working model for the variance, in which case the weights are given by the eigen-values of the $m \times m$ matrix with $(i,j)^{th}$ entry $\bm{s}_i'\hat{\bs\Phi} \bm{s}_j$. 

%A final approach is to use a bootstrap re-sampling technique that leads to small-sample refinements in the test rejection rates. 
%Not all bootstrap re-sampling methods work well in small samples. 
%Among the alternatives, \citet{Webb2013wild} describe a wild boostrap procedure that performs well even when $m$ is very small and when clusters are of unequal size.\todo{Need to describe the bootstrap in more detail.}

\subsection{Small-sample corrections for F-tests}

%Compared to single-constraint tests involving $t$, fewer approaches to small-sample correction are available for multiple-constraint tests. 
%A simple correction, analogous to the CR1 approach for t-tests, is to compare $Q / q$ to an $F(q, m - 1)$ reference distribution. 
%The saddlepoint approximation is not applicable due to the more complex structure of $Q$, which involves the matrix inverse of $\bm{V}^{CR}$. 
%The wild bootstrap for clustered data \citep{Webb2013wild} is also directly applicable to multiple-constraint tests, though to our knowledge its small-sample performance has not been assessed.\todo{Worth mentioning the Cameron and Miller ad hoc approximation?} 


Compared to single-constraint tests, fewer approaches to small-sample correction are available for multiple-constraint tests. 
A simple correction, analogous to the CR1 for t-tests, would be to compare $Q / q$ to an $F(q, m - 1)$ reference distribution. 
As we will show in our simulation study, like the t-test case, this test tends to be overly liberal. 
The ideal adjustment, therefore, would be to determine empirically the degrees of freedom of the $F$ distribution using an approach similar to that for the BRL t-test. In the broad literature, several small-sample corrections for multiple-constraint Wald tests of this form have been proposed. 
While this broader literature includes methods based on spectral decomposition (CITE), as well as several methods based on the Wishart distribution (which we focus in on here), we ultimately focus here on the development of a single test that performs well under a vareity of conditions (see Tipton Pustejovsky 2015). 


Following the approach of \cite{Pan2002small}, who developed a similar method in the context of CRVE for generalized estimating equations, the method we propose ivolves approximating the distribution of $\bm{C}\bm{V}^{CR2} \bm{C}'$ by a multiple of a Wishart distribution. From this it follows that $Q$ approximately follows a multiple of an F distribution. Specifically, if $\eta \bm{C}\bm{V}^{CR2} \bm{C}'$ approximately follows a Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C} \Var\left(\bm{C}\bs{\hat\beta}\right)\bm{C}'$, then 
\begin{equation}
\label{eq:AHT}
\left(\frac{\eta - q + 1}{\eta q}\right) Q \ \dot\sim \ F(q, \eta - q + 1).
\end{equation}
We will refer to this as the approximate Hotelling's $T^2$ (AHT) test, and the remainder of this section will develop this test in greater detail.

Just as in the t-test case, our goal is to develop a strategy to estimate the degrees of freedom of this F-test (through the parameter $\eta$). 
To do so, we estimate the degrees of freedom of the Wishart distribution so that they match the mean and variance of $\bm{C}\bm{V}^{CR} \bm{C}'$. 
A problem that arises in doing so is that when $q > 1$ it is not possible to exactly match both moments. 
In developing the test, we therefore borrow strategies from the literature on CRVE found more broadly. 
One approach, developed by \cite{Pan2002small}, is to use as degrees of freedom the value that minimizes the squared differences between the covariances among the entries of $\eta \bm{C}\bm{V}^{CR}\bm{C}'$ and the covariances of the Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C}\bm{V}^{CR}\bm{C}'$. Another approach, developed by \citet{Zhang2012two-wayANOVA, Zhang2012MANOVA, Zhang2013tests} in the context of heteroskedastic and multivariate analysis of variance models, is to instead match the mean and total variance of $\bm{C}\bm{V}^{CR}\bm{C}'$ (i.e., the sum of the variances of its entries), which avoids the need to calculate any covariances. In what follows we focus on this latter approach, which we find performs best in practice (see Tipton Pustejovsky 2015).

Let $\bm{c}_1,...,\bm{c}_q$ denote the $p \times 1$ row-vectors of $\bm{C}$. 
Let $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h'\bm{A}_h'\bm{W}_h\bm{X}_h\bm{M}\bm{c}_s$ for $s = 1,...,q$ and $h = 1,...,m$. 
The degrees of freedom are then estimated under the working model as
\begin{equation}
\label{eq:eta_model}
\eta_M = \frac{\sum_{s,t=1}^q \sum_{h,i=1}^m b_{st} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{th} \bm{t}_{si}'\hat{\bs\Omega}\bm{t}_{ti}}{\sum_{s,t=1}^q \sum_{h,i=1}^m \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} + \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{si} \bm{t}_{th}'\hat{\bs\Omega}\bm{t}_{ti}},
\end{equation}
where $b_{st} = 1 + (s=t)$ for $s,t=1,..,q$.
Note that $\eta_M$ reduces to $\nu_M$ if $q = 1$.

This F-test shares features with the t-test developed by Bell and McCaffrey. Like the t-test, the degrees of freedom of this F-test depend non only on the number of clusters, but also on features of the covariates being tested. Again, these degrees of freedom can be much smaller than $m - 1$, and are particularly smaller when the covariates being tested exhibit high imbalances or leverage. Unlike the t-test case, however, in multi-parameter case, it is often more difficult to diagnose the cause of these small degrees of freedom. In some situations, however, these are straightforward extensions to the findings in t-tests. For example, if the goal is to test if there are differences across a four-arm treatment study, the degrees of freedom are largest (and close to $m - 1$) when the treatment is allocated equally across the four groups within each cluster. When the proportion varies across clusters, these degrees of freedom fall, often leading to degrees of freedom in the "small sample" territory even when the number of clusters is large. In the next section, we will illustrate these principles in a simulation study.



\section{Simulation evidence}
\label{subsec:simulations}

\section{EXAMPLES}
\label{subsec:examples_F}
%In each example, show a t-test and an F-test.

In this section we examine three short examples of the use of CRVE with small samples, spanning a variety of applied contexts. In the first example, the effects of substantive interest are identified within each cluster. In the second example, the effects involve between-cluster contrasts. The third example involves a cluster-robust Hausmann test for differences between within- and across-cluster information. In each example, we illustrate how the proposed small-sample t- and F-tests can be used and how they can differ from both the standard CR1 and Wild bootstrap tests. R code and data files are available for each analysis as an online supplement.

\subsubsection{Tennessee STAR class-size experiment.} 

The Tennessee STAR class size experiment is one of the most well studied interventions in education.  In the experiment, K – 3 students and teachers were randomized within each of 79 schools to one of three conditions: small class-size (targetted to have 13-17 students), regular class-size, or regular class-size with an aide (see Schazenbach, 2006 for a review). Analyses of the original study and follow up waves have found that being in a small class improves a variety of outcomes, including higher test scores \citep{Schanzenbach2006what}, increased likelihood of taking college entrance exams \citep{Krueger2001effect}, and increased rates of home ownership and earnings \citep{Chetty2011how}. 

The class-size experiment consists of three treatment conditions and multiple, student-level outcomes of possible interest. The analytic model is 
\begin{equation}
Y_{ijk} = \bm{z}_{jk}'\bs\alpha_i + \bm{x}_{jk}'\bs\beta + \gamma_k + \epsilon_{ijk}
\end{equation}
For outcome $i$, student $j$ is found in school $k$; $\bm{z}_{jk}$ includes dummies for the small-class and regular-plus-aide conditions; and the vector $\bm{x}_{jk}$ includes a set of student demographics (i.e., free or reduced lunch status; race; gender; age). Following Krueger (1999), we put the the reading, word recognition, and math scores on comparable scales by converting each outcome to percentile rankings based upon their distributions in the control condition.

We estimated the model in two ways. First, we estimated $\bs\alpha_i$ separately for each outcome $i$ and tested the null hypothesis that $\bs\alpha_i = \bm{0}$. Second, we use the seemingly unrelated regression (SUR) framework to test for treatment effects across conditions, using a simultaneous test across outcomes. In the SUR model, separate treatment effects are estimated for each outcome, but the student demographic effects and school fixed effects are pooled across outcomes. An overall test of the differences between conditions thus amounts to testing the null hypothesis that $\bs\alpha_1 = \bs\alpha_2 = \bs\alpha_3 = \bm{0}$. In all models, we estimated $\bs\alpha_i$ and $\bs\beta$ after absorbing the school fixed effects and clustered the errors by school.

\subsubsection{Heterogeneous treatment impacts} 

\citet{Angrist2009effects} reported results from a randomized trial in Israel aimed at increasing matriculation certification for post-secondary education among low achievers. 
In the Achievement Awards demonstration, 40 non-vocational high schools with the lowest 1999 certification rates nationally were selected (but with a  minimum threshold of 3\%). This included 10 Arab and 10 Jewish religious schools and 20 Jewish secular schools. The 40 schools were then pair-matched based on the 1999 certification rates, and within each pair one school was randomized to receive a cash-transfer program. In these treatment schools, every student who completed certification was eligible for a payment. The total amount at stake for a student who passed all the milestones was just under \$2,400.   

Baseline data was collected in January 2001 with follow up data collected in June 2001 and 2002. Following \citet{Angrist2009effects}, we focus on the number of certification tests taken as the outcome and report results separately for girls, for boys, and for the combined sample. Given that the program took place in three different types of schools, in this example we focus on determining if there is evidence of variation in treatment impacts across types of schools (i.e., Jewish secular, Jewish religious, and Arab). We use the analytic model:
\begin{equation}
Y_{ij} = \bm{z}_j'\bs\alpha + T_j \bm{z}_j \bs\delta + \bm{x}_{ij}'\bs\beta + \epsilon_{ij}
\end{equation}
In this model for student $i$ in school $j$, $\bm{z}_j$ is a vector of dummies indicating school type; $T_j$ is a treatment dummy indicating if school $j$ was assigned to the treatment condition; and $\bm{x}_{ij}$ contains individual student demographics (i.e., mother’s and father’s education; immigration status; number of siblings; and an indicator for the quartile of their pre-test achievement from previous years). The components of $\bs\delta$ represent the average treatment impacts in Jewish secular, Jewish religious, and Arab schools. We test the null hypothesis that $\delta_1 = \delta_2 = \delta_3$ to determine if the treatment impact differs across school types. In the second panel of Table 1 we provide the results of this test separately for boys and girls and by year. Importantly, note that the 2000 results are baseline tests, while the 2001 and 2002 results measure the effectiveness of the program.\todo{Add note about program being discontinued in 2002} 

\subsubsection{Robust Hausmann test} 

In this final example, we shift focus from analyses of experiments to panel data. Here we build off of an example first developed in \citet{Bertrand2004how} using Current Population Survey (CPS) data to relate demographics to earnings. Following \citet{Cameron2015practitioners}, we aggregated the data from the individual level to the time period, producing a balanced panel with 36 time points within 51 states (including the District of Columbia). We focus on the model,
\begin{equation}
Y_{tj} = \bm{r}_{tj}'\bs\alpha + \gamma_j + \epsilon_{ij}.
\end{equation}
In this model, time-point $t$ is nested within state $j$; the outcome $Y_{tj}$ is log-earnings, which are reported in 1999 dollars; $\bm{r}_{tj}$ includes a vector of demographic covariates specific to the time point (i.e., dummy variables for female and white; age and age-squared); and $\gamma_j$ is a fixed effect for state $j$. 

For sake of example, we focus here on determining whether to use a fixed effects (FE) estimator or a random effects (RE) estimator the four parameters in $\bs\alpha$, based on a Hausmann test. In an OLS model with uncorrelated, the Hausmann test directly compares the vectors of FE and RE estimates using a chi-squared test. However, this specification fails when cluster-robust standard errors are employed, and instead an artificial-Hausman test \citep{Arellano1993on} is typically used \citep[pp. 290-291]{Wooldridge2002econometric}. This test instead amends the model to additionally include within-cluster deviations (or cluster aggregates) of the variables of interest. In our example, this becomes,
\begin{equation}
Y_{tj} = \bm{r}_{tj}'\bs\alpha + \bm{\ddot{r}}_{tj}\bs\beta + \gamma_j + \epsilon_{tj},
\end{equation}
where $\bm{\ddot{r}}_{tj}$ denotes the vector of within-cluster deviations of the covariates (i.e., $\bm{\ddot{r}}_{tj} = \bm{r}_{tj} - \frac{1}{T}\sum_{t=1}^T \bm{r}_{tj}$).
The four parameters in $\bs\beta$ represent the differences between the within-panel and between-panel estimates of $\bs\alpha$. The artificial Hausmann test therefore reduces to testing the null hypothesis that $\bs\beta = \bm{0}$ using an F test with $q = 4$. We estimate the model using WLS with weights derived under the assumption that  $\gamma_1,...,\gamma_J$ are mutually independent, normally distributed, and independent of $\epsilon_{tj}$.




\section{DISCUSSION}
\label{sec:discussion}

While it's odd to think about using a working model in combination with CRVE, it does put a little bit more emphasis on attending to modeling assumptions, which is probably a good thing. 

%Current take-home points:
%\begin{itemize}
%\item How much does the working matrix matter?
%\item Role of degrees of freedom
%\item Logical consistency: t-test and F-test; FE and absorbtion
%\item Comparing these to CR1
%\item Comparing these to Wild bootstrap


%Future research:
%\begin{itemize}
%\item ``empirical'' degrees of freedom estimation
%\item use of other CR estimators
%\item computational issues with CR2 (especially when $n_j$'s are large)
%\item saddlepoint methods for $q > 1$
%\end{itemize}

\appendix

\section{Distribution theory for $\bm{V}^{CR}$}
\label{app:VCR_dist}

The small-sample approximations for t-tests and F-tests both involve the distribution of the entries of $\bm{V}^{CR2}$. This section explains the relevant distribution theory.

First, note that the CR2 estimator can be written in the form $\bm{V}^{CR2} = \sum_{j=1}^M \bm{T}_j \bm{e}_j \bm{e}_j' \bm{T}_j'$ for $p \times n_j$ matrices $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \bm{A}_j$.
Let $\bm{c}_1,\bm{c}_2,\bm{c}_3,\bm{c}_4$ be fixed, $p \times 1$ vectors and consider the linear combination $\bm{c}_1' \bm{V}^{CR2} \bm{c}_2$. 
\citet[Theorem 4]{Bell2002bias} show that the linear combination is a quadratic form in $\bm{Y}$: \[
\bm{c}_1' \bm{V}^{CR2} \bm{c}_2 = \bm{Y}'\left(\sum_{j=1}^m \bm{t}_{2j} \bm{t}_{1j}'\right) \bm{Y}, \]
for $N \times 1$ vectors $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h' \bm{T}_h' \bm{c}_s$, $s = 1,...,4$, and $h = 1,...,m$. 

Standard results regarding quadratic forms can be used to derive the moments of the linear combination. We now assume that $\bs\epsilon_1,...,\bs\epsilon_m$ are multivariate normal with zero mean and variance $\bs\Sigma$. It follows that 
\begin{align}
\label{eq:CRVE_expectation}
\E\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2\right) &= \sum_{j=1}^m \bm{t}_{1j}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_variance}
\Var\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2\right) &= \sum_{i=1}^m \sum_{j=1}^m \left(\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}\right)^2 + \bm{t}_{1i}' \bs\Sigma \bm{t}_{1j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_covariance}
\Cov\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2, \bm{c}_3' \bm{V}^{CR} \bm{c}_4\right) &= \sum_{i=1}^m \sum_{j=1}^m \bm{t}_{1i}' \bs\Sigma \bm{t}_{4j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{3j} + \bm{t}_{1i}' \bs\Sigma \bm{t}_{3j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{4j}.
\end{align}
Furthermore, the distribution of $\bm{c}_1' \bm{V}^{CR2} \bm{c}_2$ can be expressed as a weighted sum of $\chi^2_1$ distributions, with weights given by the eigen-values of the $m \times m$ matrix with $\left(i,j\right)^{th}$ entry $\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}$, $i,j=1,...,m$.


\section{CR2 invariance}
\label{app:theorem1}

This appendix provides a theorem that identifies circumstances under which it is unnecessary to account for fixed effect absorption when calculating the adjustment matrices used in $\bm{V}^{CR2}$. 

Formulas for the inverse of a partitioned matrix can be used to demonstrate that $\bm{X}_j\bm{M}\bm{J} = \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}$. Thus, equivalence of $\bm{\ddot{V}}^{CR2}$ and $\bm{J}'\bm{V}^{CR2}\bm{J}$ follows if $\bm{A}_j = \bm{\ddot{A}}_j$ for $j = 1,...,m$.

From the fact that $\bm{\ddot{R}}_j'\bm{W}_j\bm{S}_j = \bm{0}$ for $j = 1,...,m$, it follows that \begin{align*}
\bm{B}_j &= \bs{\hat\Phi}_j^C \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'\\
&= \bs{\hat\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j' {\bs{\hat\Phi}_j^C}' \\
&= \bs{\hat\Phi}_j^C \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right){\bs{\hat\Phi}_j^C}'
\end{align*}
and 
\begin{equation}
\label{eq:B_j_inverse}
\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1}\left(\bs{\hat\Phi}_j^C\right)^{-1}.
\end{equation}
Let $\bm{U}_j = \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j'\right)^{-1}$.
Using a generalized Woodbury identity \citep{Henderson1981on}, \[
\bm{U}_j = \bm{W}_j - \bm{W}_j \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\left(\bm{M_{\ddot{R}}} - \bm{M_{\ddot{R}}} \bm{\ddot{R}}_j \bm{W}_j \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\right)^{-} \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j'\bm{W}_j, \]
where ${M}^{-}$ is a generalized inverse of $\bm{M}$. 
It follows that $\bm{U}_j \bm{S}_j = \bm{W}_j \bm{S}_j$. 
Another application of the generalized Woodbury identity gives 
\begin{align*}
\left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1} &= \bm{U}_j - \bm{U}_j \bm{S}_j \bm{M_S}\left(\bm{M_S} - \bm{M_S}\bm{S}_j' \bm{U}_j \bm{S}_j\bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' \bm{U}_j \\
&= \bm{U}_j - \bm{W}_j \bm{S}_j \bm{M_S}\left(\bm{M_S} - \bm{M_S}\bm{S}_j' \bm{W}_j \bm{S}_j\bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' \bm{W}_j \\
&= \bm{U}_j.
\end{align*}
The last equality follows from the fact that $\bm{S}_j \bm{M_S}\left(\bm{M_S}\bm{S}_j' \bm{W}_j \bm{S}_j\bm{M_S} - \bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' = \bm{0}$ because the fixed effects are nested within clusters. 
Substituting into (\ref{eq:B_j_inverse}), we then have that $\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \bm{U}_j \left(\bs{\hat\Phi}_j^C\right)^{-1}$. 
Now, $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C \bm{U}_j^{-1} {\bs{\hat\Phi}_j^C}'$ and so $\bm{\ddot{B}}_j^{-1} = \bm{B}_j^{1}$. It follows that $\bm{\ddot{A}}_j = \bm{A}_j$ for $j = 1,...,m$. 



\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}

