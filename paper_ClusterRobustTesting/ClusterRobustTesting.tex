\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.7in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage[textwidth=1in, textsize=tiny]{todonotes}
%\usepackage[disable]{todonotes}

\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Small sample correction methods for cluster-robust variance estimators and hypothesis tests}
  \author{\\James E. Pustejovsky\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Educational Psychology \\ 
    University of Texas at Austin\\ \\
    and \\ \\
    Elizabeth Tipton \\
    Department of Human Development \\ 
    Teachers College, Columbia University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Small sample correction methods for cluster-robust variance estimators and hypothesis tests}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{Introduction}
\label{sec:intro}



Cluster-robust variance estimators (CRVE) and hypothesis tests based upon such estimators are ubiquitous in applied econometric work. Nearly every respectable paper in the past 15 years uses cluster-robust variance estimators because to do otherwise would be to risk being seen as insufficiently rigorous (or anti-conservative....ughh....how gauche!).

There's been a lot of fretting recently that even CRVE may actually not be rigorous enough. Cite the following people so as not to get their ire up:
\begin{itemize}
\item \citet{Brewer2013inference}
\item \citet{Cameron2008bootstrap}
\item \citet{Cameron2015practitioners}
\item \citet{Carter2013asymptotic}
\item \citet{Ibragimov2010tstatistic}
\item \citet{Imbens2012robust}
\item \citet{Kezdi2004robust}
\item \citet{McCaffrey2001generalizations, Bell2002bias}
\item \citet{McCaffrey2006improved}
\item \citet{Webb2013wild}
\item \citet{Kline2012score}
\end{itemize}

\subsection{Econometric framework}

We will consider linear regression models in which the errors within a cluster have an unknown variance structure. 
The model is
\begin{equation}
\label{eq:model_vector}
\bm{Y}_j = \bm{X}_j \bs\beta + \bs\epsilon_j,
\end{equation}
for $j=1,...,m$, where $\bm{Y}_j$ is $n_j \times 1$, $\bm{X}_j$ is an $n_j \times p$ matrix of regressors for cluster $j$, $\bs\beta$ is a $p \times 1$ vector, and $\bs\epsilon_j$ is an $n_j \times 1$ vector of errors. 
Assume that $\E\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bm{0}$ and $\Var\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bs\Sigma_j$, for $j = 1,...,m$, where $\bs\Sigma_1,...,\bs\Sigma_m$ may be unknown, and the errors are independent across clusters. 
Let $\bm{X} = \left(\bm{X}_1',\bm{X}_2',...,\bm{X}_m'\right)'$ and $\bs\Sigma = \bigoplus_{j=1}^m \bs\Sigma_j$. Additionally, let $N = \sum_{j=1}^m n_j$, let $\bm{I}$ denote an $N \times N$ identity matrix, and let $\bm{I}_j$ denote an $n_j \times n_j$ identity matrix.

The vector of regression coefficients is estimated by weighted least squares (WLS). 
Given a set of $m$ symmetric weighting matrices $\bm{W}_1,...,\bm{W}_m$, the WLS estimator is 
\begin{equation}
\label{eq:WLS}
\bs{\hat\beta} = \bm{M} \sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{Y}_j, 
\end{equation}
where $\bm{M} = \left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{X}_j\right)^{-1}$. Let $\bm{W} = \bigoplus_{j=1}^m \bm{W}_j$. 

Common choices for weighting include the unweighted case, in which $\bm{W}_j$ is an identity matrix of dimension $n_j \times n_j$, and inverse-variance weighting under a working model. 
In the latter case, the errors are assumed to follow some known structure, $\Var\left(\bm{e}_j\left|\bm{X}_j\right.\right) = \bs\Phi_j$, where $\bs\Phi_j$ is a known function of a low-dimensional parameter and $\bs\Phi = \bigoplus_{j=1}^m \bs\Phi_j$. 
The weighting matrices are then taken to be $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$, where the $\hat{\bs\Phi}_j$ are constructed from estimates of the variance parameters.

The WLS estimator also encompasses the estimator proposed by \citet{Ibragimov2010tstatistic} for clustered data. 
Assuming that $\bm{X}_j$ has rank $p$ for $j = 1,...,m$, their proposed approach involves estimating $\bs\beta$ separately within each cluster and taking the simple average of these estimates. 
The resulting average is equivalent to the WLS estimator with weights $\bm{W}_j = \bm{X}_j \left(\bm{X}_j'\bm{X}_j\right)^{-2} \bm{X}_j$.

\section{Cluster-robust variance estimators}
\label{sec:CRVE}

The variance of the WLS estimator is 
\begin{equation}
\label{eq:var_WLS}
\Var\left(\bs{\hat\beta}\right) = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bs\Sigma_j \bm{W}_j\bm{X}_j\right) \bm{M},
\end{equation}
which includes the unknown variance matrices. 
One approach to estimating this variance would be to posit a working model--typically the same working model used to construct weights--and substitute estimates of the working variance structure in place of $\bs\Sigma$. 
Under a working model $\bs\Phi$, denote this "model-based" variance estimator as
\begin{equation}
\label{eq:V_model}
\bm{V}^M = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j\bm{X}_j\right) \bm{M}.
\end{equation}
If $\bs\beta$ is estimated using inverse-variance weights defined under the same working model, then $\bm{W} = \hat{\bs\Phi}^{-1}$ and the model-based variance estimator simplifies to $\bm{V}^M = \bm{M}$. 

Cluster-robust variance estimators provide a means of estimating $\Var\left(\bs{\hat\beta}\right)$ and testing hypotheses regarding $\hat{\bs\beta}$ in the absence of a valid working model for the error structure, or when the working variance model used to develop weights is mis-specified. 
They are thus a generalization of heteroskedasticity-consistent (HC) variance estimators \citep{MacKinnon1985some}. 
Like the HC estimators, several different variants have been proposed, with different rationales and different finite-sample properties. 

The most widely used estimator is 
\begin{equation}
\label{eq:V_CR0}
\bm{V}^{CR0} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{e}_j \bm{e}_j'\bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
where $\bm{e}_j = \bm{Y}_j - \bm{X}_j \bs{\hat\beta}$. Following the naming conventions used by \citet{Cameron2015practitioners}, we will refer to this estimator as CR0. 
Note that CR0 is constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$ in (\ref{eq:var_WLS}). 
Although the individual squared residuals provide only very crude estimates of the unknown variance matrices, the resulting estimator is asymptotically consistent for the variance of $\bs{\hat\beta}$ as $m$ increases (CITE). 
However, CR0 is known to have a downward bias when the number of independent clusters is small (CITE).

The small-sample bias in CR0 can be seen as analogous to that arising by estimating the variance of a sample of $m$ observations using a denominator of $m$ rather than $m - 1$. 
One approach to correcting this bias is to scale CR0 by a factor of $m / (m - 1)$. Thus, define the CR1 estimator as $\bm{V}^{CR1} = [m / (m - 1)] \bm{V}^{CR0}$.
Some software implementations use a slightly different correction factor. 
For example, the Stata command \texttt{regress} scales CR0 by the factor $m (N - 1) / [(m - 1) (N - p)]$. 

\subsection{Correction based on a working-model}

\citet[see also \citealp{Bell2002bias}]{McCaffrey2001generalizations} proposed to correct the small-sample bias of CR0 so that it is exactly unbiased under a specified working model for the variance structure. 
In their implementation, the residuals from each cluster are multiplied by adjustment matrices $\bm{A}_1,...,\bm{A}_m$ that creates the unbiasedness property. 
The variance estimator, which we will call CR2a, is then 
\begin{equation}
\label{eq:V_CR2a}
\bm{V}^{CR2a} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{A}_j \bm{e}_j \bm{e}_j' \bm{A}_j' \bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
The adjustment matrix $\bm{A}_j$ is of dimension $n_j \times n_j$ and is chosen to satisfy
\begin{equation}
\label{eq:CR2a_criterion}
\bm{X}_j' \bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \bs\Phi \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j \bm{X}_j = \bm{X}_j' \bm{W}_j \bs\Phi_j \bm{W}_j \bm{X}_j,
\end{equation}
where $\bm{H} = \bm{X}\bm{M}\bm{X}'\bm{W}$, and $\left(\bm{I} - \bm{H}\right)_j$ denotes the rows of $\bm{I} - \bm{H}$ corresponding to cluster $j$. 
However, this criterion does not uniquely define $\bm{A}_j$. 
Based on extensive simulations, \citet{McCaffrey2001generalizations} found that a symmetric solution worked well, with 
\begin{equation}
\label{eq:CR2a_adjustment}
\bm{A}_j = \left(\bs\Phi_j^C\right)' \bm{B}_j^{-1/2}\bs\Phi_j^C,
\end{equation}
where $\bs\Phi_j^C$ is the upper triangular Cholesky factorization of $\bs\Phi_j$, 
\begin{equation}
\label{eq:CR2a_Bmatrix}
\bm{B}_j = \bs\Phi_j^C\left(\bm{I} - \bm{H}\right)_j \bs\Phi \left(\bm{I} - \bm{H}\right)_j' \left(\bs\Phi_j^C\right)',
\end{equation}
and $\bm{B}_j^{-1/2}$ is the inverse of the symmetric square root of $\bm{B}_j $. 
If ordinary (unweighted) least squares is used to estimate $\bs{\hat\beta}$ and the working variance model posits that the errors are all independent and homoskedastic, then $\bm{W} = \bs\Phi = \bm{I}$ and $\bm{A}_j = \left(\bm{I}_j - \bm{X}_j\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}_j'\right)^{-1/2}$.

Two difficulties arise in the implementation of CR2a.
First, the matrices $\bm{B}_1,...,\bm{B}_m$ may not be positive definite, so that $\bm{B}_j^{-1/2}$ cannot be calculated for every cluster. 
This occurs, for instance, in balanced panel models when the specification includes fixed effects for each unit and each timepoint and clustering is over the units \citep[p. 320]{Angrist2009mostly}. 
However, this problem can be overcome by using a generalized inverse of $\bm{B}_j$. 
A second, computational difficulty with CR2a is that it requires the inversion (or pseudo-inversion) of $m$ matrices, each of dimension $n_j \times n_j$. 
Consequently, computation of CR2a will be slow if some clusters contain a large number of of individual units. 

\subsection{Another working-model correction}

The criterion (\ref{eq:CR2a_criterion}) is not the only way to obtain a variance estimator that is precisely unbiased under a working model. An alternative approach, which to our knowledge is novel, is to use
\begin{equation}
\label{eq:V_CR2b}
\bm{V}^{CR2b} = \bm{M} \left(\sum_{j=1}^m \bm{D}_j \bm{X}_j' \bm{W}_j \bm{e}_j \bm{e}_j' \bm{W}_j \bm{X}_j \bm{D}_j'\right) \bm{M},
\end{equation}
where the adjustment matrices $\bm{D}_1,...,\bm{D}_m$ are chosen so that
\begin{equation}
\label{eq:CR2b_criterion}
\bm{D}_j \bm{X}_j' \bm{W}_j \left(\bm{I} - \bm{H}\right)_j \bs\Phi \left(\bm{I} - \bm{H}\right)_j' \bm{W}_j \bm{X}_j \bm{D}_j' = \bm{X}_j' \bm{W}_j \bs\Phi_j \bm{W}_j \bm{X}_j.
\end{equation}
Just as with CR2a, there are several different forms of adjustment matrices that satisfy (\ref{eq:CR2b_criterion}). 
A symmetric solution is to take
\begin{equation}
\bm{D}_j = \left(\bm{F}_j^C\right)'\left[\bm{F}_j^C \bm{G}_j \left(\bm{F}_j^C\right)'\right]^{-1/2}\bm{F}_j^C
\end{equation}
where $\bm{F}_j = \bm{X}_j' \bm{W}_j \bs\Phi_j \bm{W}_j \bm{X}_j$ and $\bm{G}_j = \bm{X}_j' \bm{W}_j \left(\bm{I} - \bm{H}\right)_j \bs\Phi \left(\bm{I} - \bm{H}\right)_j' \bm{W}_j \bm{X}_j$. Note that $\bm{F}_j$ and $\bm{G}_j$ might not be positive definite, and so generalized forms of the Cholesky decomposition and symmetric inverse-square root must be used. In datasets where clusters have a large number of individual units, CR2b will be less computationally intensive than CR2a because the adjustment matrices are all of dimension $p \times p$.

\subsection{Jackknife correction}

The Jackknife is an alternative, general-purpose approach to estimating $\Var\left(\bs{\hat\beta}\right)$ under unknown variance structures (CITE). 
The jackknife variance estimator involves re-estimating the vector of regression coefficients $m$ times, each time omitting a single cluster of data. 
Let $\bs{\hat\beta}_{(j)}$ denote the WLS estimator of $\bs\beta$ based on omitting cluster $j$. 
One form of the jackknife estimator is then 
\begin{equation}
\label{eq:V_JK}
\bm{V}^{JK} = \frac{m - 1}{m} \sum_{j=1}^m \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right) \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right)'.
\end{equation}
This estimator can be expressed in the form of a sandwich estimator with adjusted residuals, and therefore generalizes the HC3 estimator in the heteroskedastic case. 
For simplicity, we omit the correction factor $(m - 1) / m$ and define the CR3 estimator as \[
\bm{V}^{CR3} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \left(\bm{I}_j - \bm{H}_{jj}\right)\bm{e}_j \bm{e}_j' \left(\bm{I}_j - \bm{H}_{jj}'\right) \bm{W}_j \bm{X}_j\right) \bm{M}. \]
\citet{Bell2002bias} show that when $\left(\bm{I}_j - \bm{H}_{jj}\right)^{-1}$ exists for each $j = 1,...,m$, then $\bm{V}^{JK} = [(m - 1) / m] \bm{V}^{CR3}$.  

\subsection{Considerations with panel models}

CRVEs are often used in connection with fixed effects panel data models. 
In such models, clusters correspond to repeated measures on individual units (e.g., yearly data describing each of the states in the U.S.), and the regression specification includes separate intercepts for each unit.
One common model is 
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \epsilon_{jt} \]
for $j=1,...m$ and $t = 1,...,n_j$, where $\bm{r}_{ij}$ is an $r \times 1$ row vector of covariates. If the number and timing of the measurements is identical across cases, then the panel is balanced. Another common specification for balanced panels includes additional effects for each unique measurement equation:
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \nu_t + \epsilon_{jt} \]
for $j=1,...,m$ and $t = 1,...,n$. 
In what follows, we consider a generic fixed effects model in which
\begin{equation}
\label{eq:fixed_effects}
\bm{y}_j = \bm{R}_j \bs\alpha + \bm{S}_j \bs\gamma + \bs\epsilon_j,
\end{equation}
where $\bm{R}_j$ is an $n_j \times r$ matrix of covariates, $\bm{S}_j$ is an $n_j \times s$ matrix describing the fixed effects specification, $\bm{X}_j = \left[\bm{R}_j \ \bm{S}_j\right]$, $\bs\beta = \left(\bs\alpha', \bs\gamma'\right)'$, and $p = r + s$. 

In fixed effects panel models, inferential interest is typically confined to $\bs\alpha$ and the fixed effects are treated as nuisance parameters. 
If the dimension of the fixed effects specification, it is computationally inefficient to estimate $\bs\beta$ by ordinary or weighted least squares. 
Instead, it is useful to first absorb the fixed effects and then estimate $\bs\alpha$ on the reduced covariate vector.
Although both approaches yield algebraically equivalent estimators of $\bs\alpha$, the small-sample adjustments to the CRVEs can differ depending on whether they are calculated based on the full covariate matrix or after absorbing the fixed effects. 
We view absorption is a computational device, rather than a distinct approach to estimation, and so it is useful to describe how to calculate the CRVEs so that they are invariant to how the regression estimates are calculated.

Let $\bm{H_S} = \bm{S}\left(\bm{S}'\bm{W}\bm{S}\right)^{-1} \bm{S}'\bm{W}$, $\bm{\ddot{Y}} = \left(\bm{I} - \bm{H_S}\right)\bm{Y}$, $\bm{\ddot{R}} = \left(\bm{I} - \bm{H_S}\right)\bm{R}$, $\bm{M_{\ddot{R}}} = \left(\bm{\ddot{R}}' \bm{W} \bm{\ddot{R}}\right)^{-1}$, and $\bm{H_{\ddot{R}}} = \bm{\ddot{R}}\bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W}$. 
Using absorption, the WLS estimator of $\bs\alpha$ can be calculated as \[
\bs{\hat\alpha} = \bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W} \bm{\ddot{Y}}. \]
This estimator is algebraically equivalent to the corresponding sub-vector of $\bs{\hat\beta}$  calculated as in (\ref{eq:WLS}), based on the full covariate matrix $\bm{X}$. 
Furthermore, the residuals can be calculated from the absorbed model using $\bm{e} = \bm{\ddot{y}} - \bm{\ddot{R}} \bs{\hat\alpha}$.
Let $\bm{\ddot{V}}^{CR0}$ denote the CR0 estimator calculated using $\bm{\ddot{R}}$ in place of $\bm{X}$, $\bm{M_{\ddot{R}}}$ in place of $\bm{M}$, and $\bm{\ddot{e}} = $ in place of $\bm{e}$. It can be shown that $\bm{\ddot{V}}^{CR0}$ is algebraically equivalent to $\bm{V}^{CR0}$ calculated based on the full covariate matrix, as in (\ref{eq:V_CR0}). 
Because CR1 differs from CR0 by the constant factor $m / (m - 1)$, it too is invariant to how $\bs{\hat\alpha}$ is calculated. 

In contrast to CR0 and CR1, the CR2a estimator will differ depending on whether it is calculated based on the quantities from the absorbed model or those from the full WLS model. 
It is thus useful to define CR2a in such a way that the calculations based on the absorbed model yield algrebraically identical results to the calculations from the full WLS model. 
This can be accomplished by ensuring that the adjustment matrices given in Equation (\ref{eq:CR2a_adjustment}) are calculated based on the full covariate matrix $\bm{X}$. Specifically, in models with fixed effects, the adjustment matrices are calculated as \[
\bm{A}_j = \left(\bs\Phi_j^C\right)' \left[\bs\Phi_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \bs\Phi \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' \left(\bs\Phi_j^C\right)' \right]^{-1/2}\bs\Phi_j^C.
\]
This formula avoids the need to calculate $\bm{H}$, which would involve inverting a $p \times p$ matrix.\todo{Comment on whether this matters when fixed effects include only cluster indicators.}

Like CR2a, the CR2b estimator is affected by whether it is calculated based on the absorbed model or the full WLS model. However, the structure of the adjustment matrices is such that their dimension can be reduced by focusing on the subset of covariates that are of inferential interest: instead of using $p \times p$ adjustment matrices, one can reduce their dimension to $r \times r$. Thus, in models with fixed effects, we propose to calculate CR2b as 
\begin{equation}
\label{eq:V_CR2b_FE}
\bm{\ddot{V}}^{CR2b} = \bm{M_{\ddot{R}}} \left(\sum_{j=1}^m \bm{\ddot{D}}_j \bm{\ddot{R}}_j' \bm{W}_j \bm{e}_j \bm{e}_j' \bm{W}_j \bm{\ddot{R}}_j \bm{\ddot{D}}_j'\right) \bm{M_{\ddot{R}}},
\end{equation}
where the adjustment matrices are given by 
\begin{equation}
\bm{\ddot{D}}_j = \left(\bm{\ddot{F}}_j^C\right)'\left[\bm{\ddot{F}}_j^C \bm{\ddot{G}}_j \left(\bm{\ddot{F}}_j^C\right)'\right]^{-1/2}\bm{\ddot{F}}_j^C,
\end{equation}
$\bm{\ddot{F}}_j = \bm{\ddot{R}}_j' \bm{W}_j \bs\Phi_j \bm{W}_j \bm{\ddot{R}}_j$ and $\bm{\ddot{G}}_j = \bm{\ddot{R}}_j' \bm{W}_j \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j  \left(\bm{I} - \bm{H_s}\right) \bs\Phi \left(\bm{I} - \bm{H_s}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' \bm{W}_j \bm{\ddot{R}}_j$. The CR2b estimator then has the property that $\E\left(\bm{\ddot{V}}^{CR2b}\right) = \Var\left(\bs{\hat\alpha}\right)$ when the working model is correct.

In panel models that include fixed effects for each cluster, the CR3 variance estimator does not work when the model is estimated by WLS because $\bm{I}_j - \bm{H}_{jj}$ is not of full rank. 
However, the jackknife estimator of $\Var\left(\bs{\hat\alpha}\right)$ remains well-defined, as 
\begin{equation}
\bm{V}^{JK} = \frac{m - 1}{m} \sum_{j=1}^m \left(\bs{\hat\alpha}_{(j)} - \bs{\hat\alpha}\right) \left(\bs{\hat\alpha}_{(j)} - \bs{\hat\alpha}\right)',
\end{equation}
where $\bs{\hat\alpha}_{(j)}$ is calculated by omitting cluster $j$. 
If the fixed effects specification consists entirely of cluster-level effects, or more generally, if $\bm{S}_j'\bm{S}_k = \bm{0}$ when $j \neq k$ for all $j,k=1,...,m$, then $\bm{V}^{JK} = [(m - 1) / m] \bm{\ddot{V}^{CR3}}$, where 
\begin{equation}
\label{eq:V_CR3_FE}
\bm{\ddot{V}}^{CR3} = \bm{M_{\ddot{R}}} \left(\sum_{j=1}^m \bm{\ddot{R}}_j' \bm{W}_j \left(\bm{I}_j - \bm{H_{\ddot{R}}}_{jj}\right)^{-1}\bm{e}_j \bm{e}_j' \left(\bm{I}_j - \bm{H_{\ddot{R}}}_{jj}'\right)^{-1}\bm{W}_j \bm{\ddot{R}}_j\right) \bm{M_{\ddot{R}}}
\end{equation}
and $\bm{H_{\ddot{R}}}_{jj} = \bm{\ddot{R}}_j \bm{M_{\ddot{R}}} \bm{\ddot{R}}_j\bm{W}_j$. 
If the fixed effects specification includes terms that are not strictly nested within clusters, then $\bm{\ddot{V}}^{CR3}$ will not be exactly equivalent to $\bm{V}^{JK}$, and the former will depend to some extent on which terms are absorbed.\todo{So we would recommend what?}

\section{Hypothesis testing}
\label{sec:testing}

Wald-type test statistics based on CRVEs are often used to test hypotheses regarding and construct confidence intervals for the coefficients in the regression specification. 
Such procedures are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., $m \to \infty$). 
However, evidence from a wide variety of contexts indicates that the asymptotic results can be a very poor approximation when the number of clusters is small, even when small-sample corrections such as CR2a or CR3 are used \citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. 
Furthermore, the accuracy of asymptotic approximations depends on design features such as the degree of imbalance in the covariates, skewness of the covariates, and similarity of cluster sizes \citep{McCaffrey2001generalizations, Tipton2015small, Webb2013wild}. 
Consequently, no simple rule-of-thumb exists for what constitutes an adequate sample size to use an asymptotic test. 

In this section, we consider methods for obtaining hypothesis tests with more accurate rejection rates. 
We first review extant procedures for single-constraint hypotheses (i.e., t-tests) and then propose a method for testing multiple-constraint hypotheses (i.e., F-tests). 
For both types of tests, small-sample approximations involve the distribution of the entries of $\bm{V}^{CR}$. 
The following results will be used in the remainder. 

First, note that any of the CRVEs described in the previous section can be written in the form $\bm{V}^{CR} = \sum_{j=1}^M \bm{T}_j \bm{e}_j \bm{e}_j' \bm{T}_j'$ for some $p \times n_j$ matrices $\bm{T}_j$. 
The form of the $\bm{T}_j$ matrices depends on which variance estimator is used: $\bm{T}_j = \bm{M}\bm{X}_j' \bm{W}_j$ for CR0, $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \bm{A}_j$ for CR2a, $\bm{T}_j = \bm{M} \bm{D}_j \bm{X}_j' \bm{W}_j$ for CR2b, and $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \left(\bm{I}_j - \bm{H}_{jj}\right)^{-1}$ for CR3.

Next, let $\bm{u}_1,\bm{u}_2,\bm{u}_3,\bm{u}_4$ be fixed, $p \times 1$ vectors and consider the linear combination $\bm{u}_1' \bm{V}^{CR} \bm{u}_2$. 
\citet[Theorem 4]{Bell2002bias} show that the linear combination is a quadratic form in $\bm{Y}$: \[
\bm{u}_1' \bm{V}^{CR} \bm{u}_2 = \bm{Y}'\left(\sum_{j=1}^m \bm{t}_{2j} \bm{t}_{1j}'\right) \bm{Y}, \]
for $N \times 1$ vectors $\bm{t}_{xj} = \left(\bm{I} - \bm{H}\right)_j' \bm{T}_j' \bm{u}_x$, $x = 1,...,4$, and $j = 1,...,m$. 

Standard results regarding quadratic forms can be used to derive the moments of the linear combination. We now assume that $\bs\epsilon_1,...,\bs\epsilon_m$ are multivariate normal with zero mean and variance $\bs\Sigma$. It follows that 
\begin{align}
\E\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2\right) &= \sum_{j=1}^m \bm{t}_{1j}' \bs\Sigma \bm{t}_{2j} \\
\Var\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2\right) &= \sum_{i=1}^m \sum_{j=1}^m \left(\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}\right)^2 + \bm{t}_{1i}' \bs\Sigma \bm{1}_{1j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{2j} \\
\Cov\left(\bm{u}_1' \bm{V}^{CR} \bm{u}_2, \bm{u}_3' \bm{V}^{CR} \bm{u}_4\right) &= \sum_{i=1}^m \sum_{j=1}^m \bm{t}_{1i}' \bs\Sigma \bm{t}_{4j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{3j} + \bm{t}_{1i}' \bs\Sigma \bm{t}_{3j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{4j}.
\end{align}


\subsection{Single-constraint tests}

\subsection{Multiple-constraint tests}

\section{Examples}
\label{sec:examples}

\section{Simulation evidence}
\label{sec:simulations}

\section{Discussion}
\label{sec:discussion}

\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}
