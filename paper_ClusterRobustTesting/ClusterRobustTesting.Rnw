\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.7in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage[textwidth=1in, textsize=tiny]{todonotes}
%\usepackage[disable]{todonotes}

\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Small sample hypothesis testing using cluster-robust variance estimation}
  \author{\\James E. Pustejovsky\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Educational Psychology \\ 
    University of Texas at Austin\\ \\
    and \\ \\
    Elizabeth Tipton \\
    Department of Human Development \\ 
    Teachers College, Columbia University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Small sample hypothesis testing using cluster-robust variance estimations}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{INTRODUCTION}
\label{sec:intro}

<<setup, include=FALSE, warning=FALSE, cache=FALSE>>=
library(knitr)
library(xtable)

# set global chunk options
opts_chunk$set(echo = FALSE, cache = FALSE, fig.path='CR_fig/', fig.align='center', fig.show='hold')
@

Cluster-robust variance estimators (CRVE) and hypothesis tests based upon such estimators are ubiquitous in applied econometric work. Nearly every respectable paper in the past 15 years uses cluster-robust variance estimators because to do otherwise would be to risk being seen as insufficiently rigorous (or anti-conservative....ughh....how gauche!).

There's been a lot of fretting recently that even CRVE may actually not be rigorous enough. Cite the following people so as not to get their ire up:
\begin{itemize}
\item \citet{Brewer2013inference}
\item \citet{Cameron2008bootstrap}
\item \citet{Cameron2015practitioners}
\item \citet{Carter2013asymptotic}
\item \citet{Ibragimov2010tstatistic}
\item \citet{Imbens2012robust}
\item \citet{Kezdi2004robust}
\item \citet{McCaffrey2001generalizations, Bell2002bias}
\item \citet{McCaffrey2006improved}
\item \citet{Webb2013wild}
\item \citet{Kline2012score}
\end{itemize}

\subsection{Econometric framework}

We will consider linear regression models in which the errors within a cluster have an unknown variance structure. 
The model is
\begin{equation}
\label{eq:model_vector}
\bm{Y}_j = \bm{X}_j \bs\beta + \bs\epsilon_j,
\end{equation}
for $j=1,...,m$, where $\bm{Y}_j$ is $n_j \times 1$, $\bm{X}_j$ is an $n_j \times p$ matrix of regressors for cluster $j$, $\bs\beta$ is a $p \times 1$ vector, and $\bs\epsilon_j$ is an $n_j \times 1$ vector of errors. 
Assume that $\E\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bm{0}$ and $\Var\left(\bs\epsilon_j\left|\bm{X}_j\right.\right) = \bs\Sigma_j$, for $j = 1,...,m$, where $\bs\Sigma_1,...,\bs\Sigma_m$ may be unknown, and the errors are independent across clusters. 
Let $\bm{X} = \left(\bm{X}_1',\bm{X}_2',...,\bm{X}_m'\right)'$ and $\bs\Sigma = \bigoplus_{j=1}^m \bs\Sigma_j$. Additionally, let $N = \sum_{j=1}^m n_j$, let $\bm{I}$ denote an $N \times N$ identity matrix, and let $\bm{I}_j$ denote an $n_j \times n_j$ identity matrix.

The vector of regression coefficients is estimated by weighted least squares (WLS). 
Given a set of $m$ symmetric weighting matrices $\bm{W}_1,...,\bm{W}_m$, the WLS estimator is 
\begin{equation}
\label{eq:WLS}
\bs{\hat\beta} = \bm{M} \sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{Y}_j, 
\end{equation}
where $\bm{M} = \left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bm{X}_j\right)^{-1}$. Let $\bm{W} = \bigoplus_{j=1}^m \bm{W}_j$. 

Common choices for weighting include the unweighted case, in which $\bm{W}_j = \bm{I}_j$ for $j = 1,...,m$, and inverse-variance weighting under a working model. 
In the latter case, the errors are assumed to follow some known structure, $\Var\left(\bm{e}_j\left|\bm{X}_j\right.\right) = \bs\Phi_j$, where $\bs\Phi_j$ is a known function of a low-dimensional parameter and $\bs\Phi = \bigoplus_{j=1}^m \bs\Phi_j$. 
The weighting matrices are then taken to be $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$, where the $\hat{\bs\Phi}_j$ are constructed from estimates of the variance parameter.

The WLS estimator also encompasses the estimator proposed by \citet{Ibragimov2010tstatistic} for clustered data. 
Assuming that $\bm{X}_j$ has rank $p$ for $j = 1,...,m$, their proposed approach involves estimating $\bs\beta$ separately within each cluster and taking the simple average of these estimates. 
The resulting average is equivalent to the WLS estimator with weights $\bm{W}_j = \bm{X}_j \left(\bm{X}_j'\bm{X}_j\right)^{-2} \bm{X}_j$.

\section{CLUSTER-ROBUST VARIANCE ESTIMATION}
\label{sec:CRVE}

The variance of the WLS estimator is 
\begin{equation}
\label{eq:var_WLS}
\Var\left(\bs{\hat\beta}\right) = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \bs\Sigma_j \bm{W}_j\bm{X}_j\right) \bm{M},
\end{equation}
which depends upon the unknown variance matrices. 
One approach to estimating this variance would be to posit a working model---typically the same working model used to construct weights---and substitute estimates of the working variance structure in place of $\bs\Sigma$. 
Under working model $\bs\Phi$, denote this "model-based" variance estimator as
\begin{equation}
\label{eq:V_model}
\bm{V}^M = \bm{M}\left(\sum_{j=1}^m \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j\bm{X}_j\right) \bm{M}.
\end{equation}
If $\bs\beta$ is estimated using inverse-variance weights defined under the same working model, then $\bm{W}_j = \hat{\bs\Phi}_j^{-1}$ and the model-based variance estimator simplifies to $\bm{V}^M = \bm{M}$. 

Cluster-robust variance estimators provide a means of estimating $\Var\left(\bs{\hat\beta}\right)$ and testing hypotheses regarding $\hat{\bs\beta}$ in the absence of a valid working model for the error structure, or when the working variance model used to develop weights is mis-specified. 
They are thus a generalization of heteroskedasticity-consistent (HC) variance estimators \citep{MacKinnon1985some}. 
Like the HC estimators, several different variants have been proposed, with different rationales and different finite-sample properties. 

The most widely used estimator is 
\begin{equation}
\label{eq:V_CR0}
\bm{V}^{CR0} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{e}_j \bm{e}_j'\bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
where $\bm{e}_j = \bm{Y}_j - \bm{X}_j \bs{\hat\beta}$. Following the naming conventions used by \citet{Cameron2015practitioners}, we will refer to this estimator as CR0. 
Note that CR0 is constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$ in (\ref{eq:var_WLS}). 
Although the individual squared residuals provide only very crude estimates of the unknown variance matrices, the resulting estimator is asymptotically consistent for the variance of $\bs{\hat\beta}$ as $m$ increases (CITE). 
However, CR0 is known to have a downward bias when the number of independent clusters is small (CITE).

\subsection{CR2}

\citet[see also \citealp{Bell2002bias}]{McCaffrey2001generalizations} proposed to correct the small-sample bias of CR0 so that it is exactly unbiased under a specified working model. 
In their implementation, the residuals from each cluster are multiplied by adjustment matrices $\bm{A}_1,...,\bm{A}_m$ that are chosen to lead to the unbiasedness property. 
The variance estimator, which we will call CR2, is then 
\begin{equation}
\label{eq:V_CR2}
\bm{V}^{CR2} = \bm{M}\left(\sum_{j=1}^m \bm{X}_j'\bm{W}_j \bm{A}_j \bm{e}_j \bm{e}_j' \bm{A}_j' \bm{W}_j \bm{X}_j\right) \bm{M},
\end{equation}
The adjustment matrix $\bm{A}_j$ is of dimension $n_j \times n_j$ and satisfies
\begin{equation}
\label{eq:CR2_criterion}
\bm{X}_j' \bm{W}_j \bm{A}_j' \left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \bm{A}_j \bm{W}_j \bm{X}_j = \bm{X}_j' \bm{W}_j \hat{\bs\Phi}_j \bm{W}_j \bm{X}_j,
\end{equation}
where $\bm{H} = \bm{X}\bm{M}\bm{X}'\bm{W}$, and $\left(\bm{I} - \bm{H}\right)_j$ denotes the rows of $\bm{I} - \bm{H}$ corresponding to cluster $j$. 

The criterion (\ref{eq:CR2_criterion}) does not uniquely define $\bm{A}_j$. 
Based on extensive simulations, \citet{McCaffrey2001generalizations} found that a symmetric solution worked well, with 
\begin{equation}
\label{eq:CR2_adjustment}
\bm{A}_j = \left(\hat{\bs\Phi}_j^C\right)' \bm{B}_j^{-1/2}\hat{\bs\Phi}_j^C,
\end{equation}
where $\hat{\bs\Phi}_j^C$ is the upper triangular Cholesky factorization of $\hat{\bs\Phi}_j$, 
\begin{equation}
\label{eq:CR2_Bmatrix}
\bm{B}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H}\right)_j' \left(\hat{\bs\Phi}_j^C\right)',
\end{equation}
and $\bm{B}_j^{-1/2}$ is the inverse of the symmetric square root of $\bm{B}_j $. 
If ordinary (unweighted) least squares is used to estimate $\bs\beta$ and the working variance model posits that the errors are all independent and homoskedastic, then $\bm{W} = \bs\Phi = \bm{I}$ and $\bm{A}_j = \left(\bm{I}_j - \bm{X}_j\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}_j'\right)^{-1/2}$.

Two difficulties arise in the implementation of CR2.
First, the matrices $\bm{B}_1,...,\bm{B}_m$ may not be positive definite, so that $\bm{B}_j^{-1/2}$ cannot be calculated for every cluster. 
This occurs, for instance, in balanced panel models when the specification includes fixed effects for each unit and each timepoint and clustering is over the units \citep[p. 320]{Angrist2009mostly}. 
However, this problem can be overcome by using a generalized inverse of $\bm{B}_j$.\todo{Expand on this.}
A second, computational difficulty with CR2 is that it requires the inversion (or pseudo-inversion) of $m$ matrices, each of dimension $n_j \times n_j$. 
Consequently, computation of CR2a will be slow if some clusters contain a large number of of individual units. 

\subsection{Considerations with panel models}

CRVEs are often used in connection with fixed effects panel data models. 
In such models, clusters correspond to repeated measures on individual units (e.g., yearly data describing each of the states in the U.S.), and the regression specification includes separate intercepts for each unit.
One common model is 
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \epsilon_{jt} \]
for $j=1,...m$ and $t = 1,...,n_j$, where $\bm{r}_{ij}$ is an $r \times 1$ row vector of covariates. If the number and timing of the measurements is identical across cases, then the panel is balanced. Another common specification for balanced panels includes additional effects for each unique measurement occassion:
\[
y_{jt} = \bm{r}_{jt} \bs\alpha + \gamma_j + \nu_t + \epsilon_{jt} \]
for $j=1,...,m$ and $t = 1,...,n$. 
In what follows, we consider a generic fixed effects model in which
\begin{equation}
\label{eq:fixed_effects}
\bm{y}_j = \bm{R}_j \bs\alpha + \bm{S}_j \bs\gamma + \bs\epsilon_j,
\end{equation}
where $\bm{R}_j$ is an $n_j \times r$ matrix of covariates, $\bm{S}_j$ is an $n_j \times s$ matrix describing the fixed effects specification, $\bm{X}_j = \left[\bm{R}_j \ \bm{S}_j\right]$, $\bs\beta = \left(\bs\alpha', \bs\gamma'\right)'$, and $p = r + s$. 

In fixed effects panel models, inferential interest is confined to $\bs\alpha$ and the fixed effects are treated as nuisance parameters. 
If the dimension of the fixed effects specification is large, it is computationally inefficient (and can be numerically inaccurate) to estimate $\bs\beta$ by ordinary or weighted least squares. 
Instead, it is useful to first absorb the fixed effects and then estimate $\bs\alpha$ on the reduced covariate vector.
Although both approaches yield algebraically equivalent estimators of $\bs\alpha$, the small-sample adjustments to the CRVEs can differ depending on whether they are calculated based on the full covariate matrix or after absorbing the fixed effects. 
We view absorption is a computational device, rather than a distinct approach to estimation, and so it is useful to describe how to calculate CR2 when $\bs\alpha$ is estimated using absorption.

Let $\bm{M_S} = \left(\bm{S}'\bm{W}\bm{S}\right)^{-1}$, $\bm{H_S} = \bm{S} \bm{M_S} \bm{S}'\bm{W}$, $\bm{\ddot{Y}} = \left(\bm{I} - \bm{H_S}\right)\bm{Y}$, $\bm{\ddot{R}} = \left(\bm{I} - \bm{H_S}\right)\bm{R}$, $\bm{M_{\ddot{R}}} = \left(\bm{\ddot{R}}' \bm{W} \bm{\ddot{R}}\right)^{-1}$, and $\bm{H_{\ddot{R}}} = \bm{\ddot{R}}\bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W}$. 
Using absorption, the WLS estimator of $\bs\alpha$ can be calculated as \[
\bs{\hat\alpha} = \bm{M_{\ddot{R}}} \bm{\ddot{R}}' \bm{W} \bm{\ddot{Y}}. \]
This estimator is algebraically equivalent to the corresponding sub-vector of $\bs{\hat\beta}$  calculated as in (\ref{eq:WLS}), based on the full covariate matrix $\bm{X}$. 
Furthermore, the residuals can be calculated from the absorbed model using $\bm{e} = \bm{\ddot{y}} - \bm{\ddot{R}} \bs{\hat\alpha}$.
Let $\bm{\ddot{V}}^{CR0}$ denote the CR0 estimator calculated using $\bm{\ddot{R}}$ in place of $\bm{X}$, $\bm{M_{\ddot{R}}}$ in place of $\bm{M}$, and $\bm{\ddot{e}} = $ in place of $\bm{e}$. It can be shown that $\bm{\ddot{V}}^{CR0}$ is algebraically equivalent to $\bm{V}^{CR0}$ calculated based on the full covariate matrix, as in (\ref{eq:V_CR0}). 

In contrast to CR0, it is possible that the CR2 estimator will differ depending on whether it is calculated based on the quantities from the absorbed model or those from the full WLS model. 
It is thus useful to define it in such a way that the calculations based on the absorbed model yield algrebraically identical results to the calculations from the full WLS model. 
This can be accomplished by ensuring that the adjustment matrices given in Equation (\ref{eq:CR2_adjustment}) are calculated based on the full covariate matrix $\bm{X}$. Specifically, in models with fixed effects, the adjustment matrices are calculated as in (\ref{eq:CR2_adjustment}), but with 
\begin{equation}
\label{eq:CR2_panel_adjustment}
\bm{B}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' \left(\hat{\bs\Phi}_j^C\right)'.
\end{equation}
This formula avoids the need to calculate $\bm{H}$, which would involve inverting a $p \times p$ matrix. 

It is unnecessary to account for absorption of fixed effects under certain commonly occurring circumstances. Specifically, if the model is estimated using weighted least-squares with working inverse-variance weights, and if absorption is performed only for fixed effects that are equivalent to or nested within the units on which clusters are defined, then the adjustment matrices can be calculated directly from Equations (\ref{eq:CR2_adjustment}) and (\ref{eq:CR2_Bmatrix}), using $\bm{H_{\ddot{R}}}$ in place of $\bm{H}$. This result is formalized as a theorem in Appendix \ref{app:theorem1}. When the necessary conditions hold, this approach is preferable for reasons of numerical precision.

\section{SINGLE-CONSTRAINT TESTS}
\label{sec:testing}

Wald-type test statistics based on CRVEs are often used to test hypotheses regarding and construct confidence intervals for the coefficients in the regression specification. 
Such procedures are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., $m \to \infty$). 
However, evidence from a wide variety of contexts indicates that the asymptotic results can be a very poor approximation when the number of clusters is small, even when small-sample corrections such as CR2 are employed \citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. 
Furthermore, the accuracy of asymptotic approximations depends on design features such as the degree of imbalance in the covariates, skewness of the covariates, and similarity of cluster sizes \citep{McCaffrey2001generalizations, Tipton2015small, Webb2013wild}. 
Consequently, no simple rule-of-thumb exists for what constitutes an adequate sample size to trust the asymptotic test. 

We first consider testing single linear constraints (i.e., t-tests) on the parameter $\bs\beta$, in which the null hypothesis has the form $H_0: \bm{c}'\bs\beta = d$ for fixed $p \times 1$ vector $\bm{c}$ and scalar constant $d$. 
For simplicity, we consider Wald test statistics based on the CR2 variance estimator, which have the form
\begin{equation}
\label{eq:Wald_z}
Z = \left(\bm{c}'\bs{\hat\beta} - d\right) / \sqrt{\bm{c}'\bm{V}^{CR2}\bm{c}}.
\end{equation}
An asymptotically valid test rejects $H_0$ at level $\alpha$ if $|Z|$ exceeds the $\alpha / 2$ critical value of a standard normal distribution. 
However, this test tends to have actual rejection rates higher than $\alpha$ when $m$ is not large. 

\subsection{Small-sample corrections}

Four approaches to small-sample correction have been proposed for Wald-type t-tests. 
The first and surely most common approach is to compare $|Z|$ to the appropriate critical value from a $t$ distribution with $m - 1$ degrees of freedom. 
\citet{Hansen2007asymptotic} provided one justification for the use of a $t(m-1)$ reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \citep[see also][]{Donald2007inference}. 
\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values would be conservative (leading to rejection rates less than or equal to $\alpha$).
However, both of these arguments require that $\bm{c}'\bs\beta$ be separately identified within each cluster. 
Outside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \citep{Cameron2015practitioners}. 
Furthermore, this correction does not take into account that the distribution of $\bm{V}^{CR}$ is affected by the structure of the covariate matrix. 

A second approach, proposed by \citet{McCaffrey2001generalizations}, is to use a Satterthwaite approximation \citep{Satterthwaite1946approximate} to the distribution of $Z$.
This approach compares $Z$ to a $t$ reference distribution, with degrees of freedom $\nu$ that are estimated from the data. 
Theoretically, the degrees of freedom should be 
\begin{equation}
\label{eq:nu_Satterthwaite}
\nu = \frac{2\left[\E\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)\right]^2}{\Var\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)}.
\end{equation}
Expressions for the first two moments of $\bm{c}'\bm{V}^{CR2}\bm{c}$ can be derived under the assumption that the errors $\bs\epsilon_1,...,\bs\epsilon_m$ are normally distributed; see Appendix \ref{app:VCR_dist}. 
In practice, both moments involve the variance structure $\bs\Sigma$, which is unknown. 
\citet{McCaffrey2001generalizations} proposed to estimate the moments based on the same working model as used to derive the adjustment matrices. 
A ``model-based'' estimate of the degrees of freedom is then calculated as 
\begin{equation}
\nu_{M} = \frac{\left(\sum_{j=1}^m \bm{s}_j' \hat{\bs\Phi} \bm{s}_j\right)^2}{\sum_{i=1}^m \sum_{j=1}^m \left(\bm{s}_i' \hat{\bs\Phi} \bm{s}_j\right)^2},
\end{equation}
where $\bm{s}_j = \left(\bm{I} - \bm{H}\right)_j'\bm{A}_j'\bm{W}_j\bm{X}_j\bm{M}\bm{c}$. 
Alternately, for any of the CRVEs one could instead use an empirical estimate of the degrees of freedom, constructed by substituting $\bm{e}_j \bm{e}_j'$ in place of $\bs\Sigma_j$. 
However, \citet{Bell2002bias} found using simulation that the plug-in degrees of freedom estimate produced very conservative rejection rates. 

Third, \citet{McCaffrey2006improved} proposed to use a saddlepoint approximation to the distribution of $Z$. 
Like the Satterthwaite approximation, the saddlepoint approximation is derived under the assumption that the errors are normally distributed. 
Rather than using the moments of $\bm{c}'\bm{V}^{CR}\bm{c}$, the saddlepoint instead uses the fact that it is distributed as a weighted sum of $\chi^2_1$ random variables. 
The weights depend on $\bs\Sigma$, and so must be estimated. \citet{McCaffrey2006improved} did so based on a working model for the variance, in which case the weights are given by the eigen-values of the $m \times m$ matrix with $(i,j)^{th}$ entry $\bm{s}_i'\hat{\bs\Phi} \bm{s}_j$. 

A final approach is to use a bootstrap re-sampling technique that leads to small-sample refinements in the test rejection rates. 
Not all bootstrap re-sampling methods work well in small samples. 
Among the alternatives, \citet{Webb2013wild} describe a wild boostrap procedure that performs well even when $m$ is very small and when clusters are of unequal size.\todo{Need to describe the bootstrap in more detail.}

\subsection{Examples}
\label{subsec:examples_t}

\subsection{Simulation evidence}
\label{subsec:simulation_t}

\section{MULTIPLE-CONSTRAINT TESTS}

While t-tests of single coefficients are surely more common, tests of multiple constraints are also of interest for empirical data analysis. 
Examples of such tests include robust Hausmann-type endogeneity tests \citep{Arellano1993on}, tests for non-linearities in exogeneous variables in OLS models, tests for pre-treatment balance on covariates in randomized experiments, and tests of parameter restrictions in seemingly unrelated regression.
We will consider linear constraints on $\bs\beta$, where the null hypothesis has the form $H_0: \bm{C}\bs\beta = \bm{d}$ for fixed $q \times p$ matrix $\bm{C}$ and $q \times 1$ vector $\bm{d}$. 
The Wald statistic based on CR2 is then \[
Q = \left(\bm{C}\bs{\hat\beta} - \bm{d}\right)'\left(\bm{C} \bm{V}^{CR2} \bm{C}'\right)^{-1}\left(\bm{C}\bs{\hat\beta} - \bm{d}\right).
\]
The asymptotically valid Wald test rejects $H_0$ at level $\alpha$ if $Q$ exceeds $\chi^2(\alpha; q)$, the $\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom.\todo{Citations to evidence that asymptotic test is way too liberal?}

\subsection{Small-sample correction}

Compared to single-constraint tests involving $t$, fewer approaches to small-sample correction are available for multiple-constraint tests. 
The saddlepoint approximation is not applicable due to the more complex structure of $Q$, which involves the matrix inverse of $\bm{V}^{CR}$. 
A simple correction, analogous to the first approach for t-tests, would be to compare $Q / q$ to an $F(q, m - 1)$ reference distribution. 
The wild bootstrap for clustered data \citep{Webb2013wild} is also directly applicable to multiple-constraint tests, though to our knowledge its small-sample performance has not been assessed.\todo{Worth mentioning the Cameron and Miller ad hoc approximation?} 

Several small-sample corrections for multiple-constraint Wald tests have been proposed that involve an $F$ reference distribution with denominator degrees of freedom that are determined from the data. 
These approximations can thus be seen as generalizations (loosely speaking) of the Satterthwaite approximation. 
Working in the context of CRVE for generalized estimating equations, \cite{Pan2002small} proposed to approximate the distribution of $\bm{C}\bm{V}^{CR2} \bm{C}'$ by a multiple of a Wishart distribution, from which it follows that $Q$ approximately follows a multiple of an F distribution. 
Specifically, if $\eta \bm{C}\bm{V}^{CR2} \bm{C}'$ approximately follows a Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C} \Var\left(\bm{C}\bs{\hat\beta}\right)\bm{C}'$, then 
\begin{equation}
\label{eq:AHT}
\left(\frac{\eta - q + 1}{\eta q}\right) Q \ \dot\sim \ F(q, \eta - q + 1).
\end{equation}
We will refer to this as the approximate Hotelling's $T^2$ (AHT) test.

Just as in the Satterthwaite approximation, the degrees of freedom of the Wishart distribution are chosen to match the mean and variance of $\bm{C}\bm{V}^{CR} \bm{C}'$. 
However, when $q > 1$ it is not possible to exactly match both moments. 
\cite{Pan2002small} propose to use as degrees of freedom the value that minimizes the squared differences between the covariances among the entries of $\eta \bm{C}\bm{V}^{CR}\bm{C}'$ and the covariances of the Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C}\bm{V}^{CR}\bm{C}'$. 
\citet{Zhang2012two-wayANOVA, Zhang2012MANOVA, Zhang2013tests} proposed a simpler method in the context of heteroskedastic and multivariate analysis of variance models, which is a special case of the linear regression model considered here. 
The simpler approach involves matching the mean and total variance of $\bm{C}\bm{V}^{CR}\bm{C}'$ (i.e., the sum of the variances of its entries), which avoids the need to calculate any covariances.
Let $\bm{c}_1,...,\bm{c}_q$ denote the $p \times 1$ row-vectors of $\bm{C}$.
et $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h'\bm{A}_h'\bm{W}_h\bm{X}_h\bm{M}\bm{c}_s$ for $s = 1,...,q$ and $h = 1,...,m$. 
The degrees of freedom are then estimated under the working model as
\begin{equation}
\label{eq:eta_model}
\eta_M = \frac{\sum_{s,t=1}^q \sum_{h,i=1}^m b_{st} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{th} \bm{t}_{si}'\hat{\bs\Omega}\bm{t}_{ti}}{\sum_{s,t=1}^q \sum_{h,i=1}^m \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{ti} + \bm{t}_{sh}'\hat{\bs\Omega}\bm{t}_{si} \bm{t}_{th}'\hat{\bs\Omega}\bm{t}_{ti}},
\end{equation}
where $b_{st} = 1 + (s=t)$ for $s,t=1,..,q$.
Note that $\eta_M$ reduces to $\nu_M$ if $q = 1$.

\subsection{Examples}
\label{subsec:examples_F}

In this section we examine three short examples of F-tests, spanning a variety of applied contexts. In the first example, the effects of substantive interest are identified within each cluster. In the second example, the effects involve between-cluster contrasts. The third example involves a cluster-robust Hausmann test for differences between within- and across-cluster information. In each example, we illustrate how the proposed small-sample tests can be used and how they can differ from the conventional asymptotic Wald tests. R code and data files are available for each analysis as an online supplement.

\subsubsection{Tennessee STAR class-size experiment.} 

The Tennessee STAR class size experiment is one of the most well studied interventions in education.  In the experiment, K – 3 students and teachers were randomized within each of 79 schools to one of three conditions: small class-size (targetted to have 13-17 students), regular class-size, or regular class-size with an aide (see Schazenbach, 2006 for a review). Analyses of the original study and follow up waves have found that being in a small class improves a variety of outcomes, including higher test scores \citep{Schanzenbach2006what}, increased likelihood of taking college entrance exams \citep{Krueger2001effect}, and increased rates of home ownership and earnings \citep{Chetty2011how}. 

The class-size experiment consists of three treatment conditions and multiple, student-level outcomes of possible interest. The analytic model is 
\begin{equation}
Y_{ijk} = \bm{z}_{jk}'\bs\alpha_i + \bm{x}_{jk}'\bs\beta + \gamma_k + \epsilon_{ijk}
\end{equation}
For outcome $i$, student $j$ is found in school $k$; $\bm{z}_{jk}$ includes dummies for the small-class and regular-plus-aide conditions; and the vector $\bm{x}_{jk}$ includes a set of student demographics (i.e., free or reduced lunch status; race; gender; age). Following Krueger (1999), we put the the reading, word recognition, and math scores on comparable scales by converting each outcome to percentile rankings based upon their distributions in the control condition.

We estimated the model in two ways. First, we estimated $\bs\alpha_i$ separately for each outcome $i$ and tested the null hypothesis that $\bs\alpha_i = \bm{0}$. Second, we use the seemingly unrelated regression (SUR) framework to test for treatment effects across conditions, using a simultaneous test across outcomes. In the SUR model, separate treatment effects are estimated for each outcome, but the student demographic effects and school fixed effects are pooled across outcomes. An overall test of the differences between conditions thus amounts to testing the null hypothesis that $\bs\alpha_1 = \bs\alpha_2 = \bs\alpha_3 = \bm{0}$. In all models, we estimated $\bs\alpha_i$ and $\bs\beta$ after absorbing the school fixed effects and clustered the errors by school.

\subsubsection{Heterogeneous treatment impacts} 

\citet{Angrist2009effects} reported results from a randomized trial in Israel aimed at increasing matriculation certification for post-secondary education among low achievers. 
In the Achievement Awards demonstration, 40 non-vocational high schools with the lowest 1999 certification rates nationally were selected (but with a  minimum threshold of 3\%). This included 10 Arab and 10 Jewish religious schools and 20 Jewish secular schools. The 40 schools were then pair-matched based on the 1999 certification rates, and within each pair one school was randomized to receive a cash-transfer program. In these treatment schools, every student who completed certification was eligible for a payment. The total amount at stake for a student who passed all the milestones was just under \$2,400. 	

Baseline data was collected in January 2001 with follow up data collected in June 2001 and 2002. Following \citet{Angrist2009effects}, we focus on the number of certification tests taken as the outcome and report results separately for girls, for boys, and for the combined sample. Given that the program took place in three different types of schools, in this example we focus on determining if there is evidence of variation in treatment impacts across types of schools (i.e., Jewish secular, Jewish religious, and Arab). We use the analytic model:
\begin{equation}
Y_{ij} = \bm{z}_j'\bs\alpha + T_j \bm{z}_j \bs\delta + \bm{x}_{ij}'\bs\beta + \epsilon_{ij}
\end{equation}
In this model for student $i$ in school $j$, $\bm{z}_j$ is a vector of dummies indicating school type; $T_j$ is a treatment dummy indicating if school $j$ was assigned to the treatment condition; and $\bm{x}_{ij}$ contains individual student demographics (i.e., mother’s and father’s education; immigration status; number of siblings; and an indicator for the quartile of their pre-test achievement from previous years). The components of $\bs\delta$ represent the average treatment impacts in Jewish secular, Jewish religious, and Arab schools. We test the null hypothesis that $\delta_1 = \delta_2 = \delta_3$ to determine if the treatment impact differs across school types. In the second panel of Table 1 we provide the results of this test separately for boys and girls and by year. Importantly, note that the 2000 results are baseline tests, while the 2001 and 2002 results measure the effectiveness of the program.\todo{Add note about program being discontinued in 2002} 

\subsubsection{Robust Hausmann test} 

In this final example, we shift focus from analyses of experiments to panel data. Here we build off of an example first developed in \citet{Bertrand2004how} using Current Population Survey (CPS) data to relate demographics to earnings. Following \citet{Cameron2015practitioners}, we aggregated the data from the individual level to the time period, producing a balanced panel with 36 time points within 51 states (including the District of Columbia). We focus on the model,
\begin{equation}
Y_{tj} = \bm{r}_{tj}'\bs\alpha + \gamma_j + \epsilon_{ij}.
\end{equation}
In this model, time-point $t$ is nested within state $j$; the outcome $Y_{tj}$ is log-earnings, which are reported in 1999 dollars; $\bm{r}_{tj}$ includes a vector of demographic covariates specific to the time point (i.e., dummy variables for female and white; age and age-squared); and $\gamma_j$ is a fixed effect for state $j$. 

For sake of example, we focus here on determining whether to use a fixed effects (FE) estimator or a random effects (RE) estimator the four parameters in $\bs\alpha$, based on a Hausmann test. In an OLS model with uncorrelated, the Hausmann test directly compares the vectors of FE and RE estimates using a chi-squared test. However, this specification fails when cluster-robust standard errors are employed, and instead an artificial-Hausman test \citep{Arellano1993on} is typically used \citep[pp. 290-291]{Wooldridge2002econometric}. This test instead amends the model to additionally include within-cluster deviations (or cluster aggregates) of the variables of interest. In our example, this becomes,
\begin{equation}
Y_{tj} = \bm{r}_{tj}'\bs\alpha + \bm{\ddot{r}}_{tj}\bs\beta + \gamma_j + \epsilon_{tj},
\end{equation}
where $\bm{\ddot{r}}_{tj}$ denotes the vector of within-cluster deviations of the covariates (i.e., $\bm{\ddot{r}}_{tj} = \bm{r}_{tj} - \frac{1}{T}\sum_{t=1}^T \bm{r}_{tj}$).
The four parameters in $\bs\beta$ represent the differences between the within-panel and between-panel estimates of $\bs\alpha$. The artificial Hausmann test therefore reduces to testing the null hypothesis that $\bs\beta = \bm{0}$ using an F test with $q = 4$. We estimate the model using WLS with weights derived under the assumption that  $\gamma_1,...,\gamma_J$ are mutually independent, normally distributed, and independent of $\epsilon_{tj}$.



\subsection{Simulation evidence}
\label{subsec:simulation_F}

\section{DISCUSSION}
\label{sec:discussion}

While it's odd to think about using a working model in combination with CRVE, it does put a little bit more emphasis on attending to modeling assumptions, which is probably a good thing. 

Further investigation of
\begin{itemize}
\item ``empirical'' degrees of freedom estimation
\item use of other CR estimators
\item computational issues with CR2 (especially when $n_j$'s are large)
\item saddlepoint methods for $q > 1$
\end{itemize}

\appendix

\section{Distribution theory for $\bm{V}^{CR}$}
\label{app:VCR_dist}

The small-sample approximations for t-tests and F-tests both involve the distribution of the entries of $\bm{V}^{CR2}$. This appendix explains the relevant distribution theory.

First, note that the CR2 estimator can be written in the form $\bm{V}^{CR2} = \sum_{j=1}^M \bm{T}_j \bm{e}_j \bm{e}_j' \bm{T}_j'$ for $p \times n_j$ matrices $\bm{T}_j = \bm{M} \bm{X}_j' \bm{W}_j \bm{A}_j$.
Let $\bm{c}_1,\bm{c}_2,\bm{c}_3,\bm{c}_4$ be fixed, $p \times 1$ vectors and consider the linear combination $\bm{c}_1' \bm{V}^{CR2} \bm{c}_2$. 
\citet[Theorem 4]{Bell2002bias} show that the linear combination is a quadratic form in $\bm{Y}$: \[
\bm{c}_1' \bm{V}^{CR2} \bm{c}_2 = \bm{Y}'\left(\sum_{j=1}^m \bm{t}_{2j} \bm{t}_{1j}'\right) \bm{Y}, \]
for $N \times 1$ vectors $\bm{t}_{sh} = \left(\bm{I} - \bm{H}\right)_h' \bm{T}_h' \bm{c}_s$, $s = 1,...,4$, and $h = 1,...,m$. 

Standard results regarding quadratic forms can be used to derive the moments of the linear combination. We now assume that $\bs\epsilon_1,...,\bs\epsilon_m$ are multivariate normal with zero mean and variance $\bs\Sigma$. It follows that 
\begin{align}
\label{eq:CRVE_expectation}
\E\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2\right) &= \sum_{j=1}^m \bm{t}_{1j}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_variance}
\Var\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2\right) &= \sum_{i=1}^m \sum_{j=1}^m \left(\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}\right)^2 + \bm{t}_{1i}' \bs\Sigma \bm{t}_{1j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{2j} \\
\label{eq:CRVE_covariance}
\Cov\left(\bm{c}_1' \bm{V}^{CR2} \bm{c}_2, \bm{c}_3' \bm{V}^{CR} \bm{c}_4\right) &= \sum_{i=1}^m \sum_{j=1}^m \bm{t}_{1i}' \bs\Sigma \bm{t}_{4j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{3j} + \bm{t}_{1i}' \bs\Sigma \bm{t}_{3j} \bm{t}_{2i}' \bs\Sigma \bm{t}_{4j}.
\end{align}
Furthermore, the distribution of $\bm{c}_1' \bm{V}^{CR2} \bm{c}_2$ can be expressed as a weighted sum of $\chi^2_1$ distributions, with weights given by the eigen-values of the $m \times m$ matrix with $\left(i,j\right)^{th}$ entry $\bm{t}_{1i}' \bs\Sigma \bm{t}_{2j}$, $i,j=1,...,m$.



\section{CR2 invariance}
\label{app:theorem1}

This appendix provides a theorem that identifies circumstances under which it is unnecessary to account for fixed effect absorption when calculating the adjustment matrices used in $\bm{V}^{CR2}$. 

\paragraph{Theorem.} Consider model (\ref{eq:fixed_effects}) and let $\bm{\ddot{V}}^{CR2}$ be the CR2 matrix calculated based on the absorbed model, i.e., 
\[
\bm{\ddot{V}}^{CR2} = \bm{M_{\ddot{R}}}\left(\sum_{j=1}^m \bm{\ddot{R}}_j'\bm{W}_j \bm{\ddot{A}}_j \bm{e}_j \bm{e}_j' \bm{\ddot{A}}_j' \bm{W}_j \bm{\ddot{R}}_j\right) \bm{M_{\ddot{R}}},
\]
where $\bm{\ddot{A}}_j = {\bs{\hat\Phi}_j^C}' \bm{\ddot{B}}_j^{-1/2}\hat{\bs\Phi}_j^C$ and $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'$.
Let $\bm{J}$ be the $p \times r$ matrix that selects the covariates of interest, i.e., $\bm{X}\bm{J} = \bm{R}$ and $\bm{J}'\bs\beta = \bs\alpha$. 
Assume that $\bm{W}_j = \bs{\hat\Phi}_j^{-1}$ for $j = 1,...,m$ and that $\bm{S}_i \bm{M_S}\bm{S}_j'\bm{W}_j = \bm{0}$ for every $i \neq j$. Then $\bm{\ddot{V}}^{CR2} = \bm{J}'\bm{V}^{CR2}\bm{J}$.

\paragraph{Proof.} Formulas for the inverse of a partitioned matrix can be used to demonstrate that $\bm{X}_j\bm{M}\bm{J} = \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}$. Thus, equivalence of $\bm{\ddot{V}}^{CR2}$ and $\bm{J}'\bm{V}^{CR2}\bm{J}$ follows if $\bm{A}_j = \bm{\ddot{A}}_j$ for $j = 1,...,m$.

From the fact that $\bm{\ddot{R}}_j'\bm{W}_j\bm{S}_j = \bm{0}$ for $j = 1,...,m$, it follows that \begin{align*}
\bm{B}_j &= \bs{\hat\Phi}_j^C \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j \left(\bm{I} - \bm{H_S}\right) \hat{\bs\Phi} \left(\bm{I} - \bm{H_S}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_j' {\bs{\hat\Phi}_j^C}'\\
&= \bs{\hat\Phi}_j^C\left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j \hat{\bs\Phi} \left(\bm{I} - \bm{H_{\ddot{R}}} - \bm{H_S}\right)_j' {\bs{\hat\Phi}_j^C}' \\
&= \bs{\hat\Phi}_j^C \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right){\bs{\hat\Phi}_j^C}'
\end{align*}
and 
\begin{equation}
\label{eq:B_j_inverse}
\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1}\left(\bs{\hat\Phi}_j^C\right)^{-1}.
\end{equation}
Let $\bm{U}_j = \left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j'\right)^{-1}$.
Using a generalized Woodbury identity \citep{Henderson1981on}, \[
\bm{U}_j = \bm{W}_j - \bm{W}_j \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\left(\bm{M_{\ddot{R}}} - \bm{M_{\ddot{R}}} \bm{\ddot{R}}_j \bm{W}_j \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\right)^{-} \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j'\bm{W}_j, \]
where ${M}^{-}$ is a generalized inverse of $\bm{M}$. 
It follows that $\bm{U}_j \bm{S}_j = \bm{W}_j \bm{S}_j$. 
Another application of the generalized Woodbury identity gives 
\begin{align*}
\left(\bs\Phi_j - \bm{\ddot{R}}_j \bm{M_{\ddot{R}}}\bm{\ddot{R}}_j' - \bm{S}_j \bm{M_S}\bm{S}_j'\right)^{-1} &= \bm{U}_j - \bm{U}_j \bm{S}_j \bm{M_S}\left(\bm{M_S} - \bm{M_S}\bm{S}_j' \bm{U}_j \bm{S}_j\bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' \bm{U}_j \\
&= \bm{U}_j - \bm{W}_j \bm{S}_j \bm{M_S}\left(\bm{M_S} - \bm{M_S}\bm{S}_j' \bm{W}_j \bm{S}_j\bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' \bm{W}_j \\
&= \bm{U}_j.
\end{align*}
The last equality follows from the fact that $\bm{S}_j \bm{M_S}\left(\bm{M_S}\bm{S}_j' \bm{W}_j \bm{S}_j\bm{M_S} - \bm{M_S}\right)^{-} \bm{M_S} \bm{S}_j' = \bm{0}$ because the fixed effects are nested within clusters. 
Substituting into (\ref{eq:B_j_inverse}), we then have that $\bm{B}_j^{-1} = \left({\bs{\hat\Phi}_j^C}'\right)^{-1} \bm{U}_j \left(\bs{\hat\Phi}_j^C\right)^{-1}$. 
Now, $\bm{\ddot{B}}_j = \hat{\bs\Phi}_j^C \bm{U}_j^{-1} {\bs{\hat\Phi}_j^C}'$ and so $\bm{\ddot{B}}_j^{-1} = \bm{B}_j^{1}$. It follows that $\bm{\ddot{A}}_j = \bm{A}_j$ for $j = 1,...,m$. 

\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}
