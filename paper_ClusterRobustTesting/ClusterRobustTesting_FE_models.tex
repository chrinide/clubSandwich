\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{booktabs}
%\usepackage{natbib}
\RequirePackage[natbibapa]{apacite}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.7in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage{float}    % for fig.pos='H'
\usepackage{rotfloat} % for sidewaysfigure

\usepackage[textwidth=1in, textsize=tiny]{todonotes}
%\usepackage[disable]{todonotes}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}

\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\usepackage{Sweave}
\begin{document}
\input{ClusterRobustTesting_FE_models-concordance}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed-effect models}
  \author{\\James E. Pustejovsky\thanks{
    The authors thank Dan Knopf for helpful discussions about the linear algebra behind the cluster-robust variance estimator. Coady Wing,...}\hspace{.2cm}\\
    Department of Educational Psychology \\ 
    University of Texas at Austin\\ \\
    and \\ \\
    Elizabeth Tipton \\
    Department of Human Development \\ 
    Teachers College, Columbia University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed-effect models}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{INTRODUCTION}
\label{sec:intro}

%Problems with standard CRVE.
%Recent solutions. 

The use of cluster-robust variance estimation [CRVE] \citep{Arellano1987computing, Liang1986longitudinal, white1984asymptotic} is now common across a wide range of economic analyses.
Standard errors are routinely ``clustered'' to account for correlations arising from the sampling of aggregate units (e.g., countries, regions, states, villages), each containing multiple observations. 
Likewise, CRVE is now routinly used in the analysis of panel data to account for correlation of measurements on the same individual units across time periods. 
The method is an extension to another economic mainstay, heteroscedasticity-robust standard errors \citep{eicker1967limit, Huber1967behavior, White1980heteroskedasticity}, which are used to account for non-constant variance in regression models. 
Although CRVE has existed for over 30 years, it has become standard practice only in the last decade, as illustrated by its coverage in major textbooks and review articles \citep[e.g.,][]{Wooldridge2010econometric, Angrist2009mostly, Cameron2015practitioners}.

CRVE allows analysts to estimate causal and structural models using ordinary least squares (OLS) or weighted least squares (WLS), while adjusting standard errors thereafter.
These standard errors can also be used as the basis for both single- or multiple-parameter hypothesis tests.
For example, an analyst may wish to understand the effects on employment outcomes of several state-level policy shifts, where the policies were implemented at different time-points in each state. 
A standard approach to estimating such effects is via a regression model that includes indicator variables for each policy shift, demographic controls, and fixed effects for states and time-periods in order to control for unobserved confounding in each of these dimensions. 
The model might be estimated by OLS, with the fixed effects included as indicator variables; more commonly, the effects of the policy indicators might be estimated after absorbing the fixed effects, a computational technique that is also known as the fixed effects or within transformation \citep{Wooldridge2010econometric}. 
Standard errors would then be clustered by state to account for dependence in the residual errors from a given state, and these clustered standard errors would be used to test hypotheses regarding each policy (i.e., a t-test) or across policies (i.e., an F-test).
The need to cluster the standard errors by state, even when including state fixed effects, was highlighted by \citet{Bertrand2004how}, who showed that to do otherwise can lead to inappropriately small standard errors and hypothesis tests with incorrect rejection rates. 

In CRVE, standard errors are estimated empirically, thus not requiring analysts to assume a particular correlation structure.
The standard errors produced are consistent estimates of the true standard errors, leading to appropriate hypotheses tests when the number of clusters is large.
As the method has become more common, however, researchers have turned attention to the performance of these tests in small and moderate samples. 
\citet{Cameron2015practitioners} provide a thorough review of this literature, including a discussion of current practice, possible solutions, and open problems. 
They highlight the well-known phenomenon that in small samples, CRVE has a downward bias and that hypothesis tests based on CRVEs can have Type-I error rates that are considerably larger the nominal level of the test.
Moreover, they review recent research showing that the small-sample corrections for t-tests typically found in software such as Stata and SAS are inadequate. 

Cameron and Miller also point to a potentially promising solution to these problems, the bias-reduced linearization (BRL) method, which was introduced by \citet{McCaffrey2001generalizations} and \citet{Bell2002bias}. 
BRL entails correcting the bias of the CRVE so that it is exactly unbiased under a working model specified by the analyst, while also remaining asymptotically consistent under arbitrary true variance structures. 
Simulations reported by \citet{Bell2002bias} demonstrate that the BRL correction serves to reduce the bias of the CRVE even when the working model is mis-specified. 
The same authors also proposed and studied small-sample corrections to single-parameter hypothesis tests using the BRL variance estimator, based on Satterthwaite \citep{Bell2002bias} or saddlepoint approximations \citep{McCaffrey2006improved}.

Despite promising simulation evidence that BRL performs well \citep[e.g.,][]{Imbens2012robust}, several problems arise making it difficult to implement in the kinds of analyses common in economics. 
First, as \citet{Angrist2009mostly} argue, when clustering is doubly accounted for (both through fixed effects and clustered standard errors) the BRL adjustment breaks down and cannot be implemented.
Second, however, as \citet{Cameron2015practitioners} highlight, even when the BRL adjustment can be computed, the standard errors that result from absorption can be substantially different than those produced using dummy fixed effects. 
This problem also plagues the small sample corrections more commonly implemented, and leads analysts to have to choose the appropriate method for accounting for fixed effects on an ad hoc basis, with little guidance regarding the best strategy.
Third---and more generally---although \citet{Bell2002bias} provide a method for conducting single parameter tests, small-sample methods for multiple-parameter tests are lacking.
These tests occur commonly in the broader economics literature and are found not only in panel data (e.g., the Hausman test), but also more broadly in seemingly unrelated regression models, and when analyzing experimental data (e.g., baseline equivalence), particularly when there are multiple treatment groups. 

\todo[inline]{Briefly review alternative small-sample stuff.}

In this paper, we address each of these three concerns, in the end articulating a BRL methodology that is suitable for everyday econometric practice. 
In the remainder of this section, we introduce our econometric framework and review the standard CRVE methods, as implemented in most software applications. 
In Section 2, we review the original BRL correction, and propose modifications that make it possible to implement BRL in a broad class of models with fixed effects.
In Section 3, we propose a method for testing multiple-constraint hypotheses (i.e., F-tests) based on CRVE with the BRL adjustments, and show that the t-test proposed by \citet{Bell2002bias} is a special case. 
We then provide simulation evidence that this small-sample F-test offers drastic improvements over commonly implemented alternatives.
%and performs comparably with current state-of-the-art methods such as the cluster-wild bootstrap procedure described by \citet{Cameron2008bootstrap} and \citet{Webb2013wild}. 
In Section 4, we illustrate the use of CRVE in small samples, implementing the proposed hypothesis tests in three examples that cover a variety of contexts where CRVE is commonly used. 
We conclude the paper with a discussion (Section 5), where we argue that the BRL approach given here is not only superior to CRVE more generally, but also that it is potentially more useful in practice than other resampling based methods.\todo{Bold!} 


\subsection{Econometric framework}

We begin by considering a generic model of the form,
\begin{equation}
\label{eq:fixed_effects_ij}
\ {y}_{ij} = \bm{r}_{ij}' \bs\beta + \bm{s}_{ij}' \bs\gamma + \bm{t}_{ij}' \bs\mu + \epsilon_{ij} 
\end{equation}
where for observation $j$ in cluster $i$, $\bm{r}_{ij}$ is a vector of $r$ predictors of primary interest in an analysis (e.g., policy variables) as well as additional control variables, $\bm{s}_{ij}$ is a vector of $s$ fixed effects that vary across clusters, and $\bm{t}_{ij}$ is a vector of $t$ fixed effects that are identified within clusters. In the state-policy example described in the introduction, the $\bm{r}_{ij}$ would include indicators for each policy under study and additional demographic controls, $\bm{s}_{ij}$ would include year fixed effects, and $\bm{t}_{ij}$ would indicate state fixed effects. Interest would focus on testing hypotheses regarding the coefficients in $\bs\beta$ that correspond to the policy indicators, while $\bs\gamma$ and $\bs\mu$ would be considered incidental. 

In developing theory, it is often easier to work with the matrix version of this model, where now
\begin{equation}
\label{eq:fixed_effects}
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i,
\end{equation}
where for cluster $j$, $\bm{R}_i$ is an $n_i \times r$ matrix of focal predictors and controls; $\bm{S}_i$ is an $n_i \times s$ matrix describing fixed effects that vary across clusters, and $\bm{T}_i$ is an $n_i \times t$ matrix describing fixed effects that are identified only within clusters. 

We assume that $\E\left(\bs\epsilon_i\left|\bm{R}_i,\bm{S}_i, \bm{T}_i\right.\right) = \bm{0}$ and $\Var\left(\bs\epsilon_i\left|\bm{R}_i,\bm{S}_i,\bm{T}_i\right.\right) = \bs\Sigma_i$, for $i = 1,...,m$, where the form of $\bs\Sigma_1,...,\bs\Sigma_m$ may be unknown but the errors are independent across clusters. 
For notational convenience, let $\bm{U}_i = \left[\bm{R}_i \ \bm{S}_i \right]$ denote the set of predictors that vary across clusters, $\bm{X}_i = \left[\bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]$ denote the full set of predictors, $\bs\alpha = \left(\bs\beta', \bs\gamma', \bs\mu' \right)'$, and $p = r + s + t$.
Denote the total number of individual observations by $N = \sum_{i=1}^m n_i$.
Let $\bm{y}$, $\bm{R}$, $\bm{S}$, $\bm{T}$, $\bm{U}$, and $\bm{X}$ denote the matrices obtained by stacking their corresponding components, as in $\bm{R} = \left(\bm{R}_1' \ \bm{R}_2' \ \cdots \ \bm{R}_m'\right)'$. 

In this model, inferential interest is confined to $\bs\beta$ and the fixed effects $\bs\gamma$ and $\bs\mu$ are treated as nuisance parameters. The distinction between the covariates $\bm{R}_i$ versus the fixed effects $\left[\bm{S}_i \ \bm{T}_i\right]$ thus depends on context and the analyst's inferential goals. However, the distinction between the two fixed effect matrices $\bm{S}_i$ and $\bm{T}_i$ is unambiguous, in that the within-cluster fixed effects satisfy $\bm{T}_h \bm{T}_i' = \bm{0}$ for $h \neq i$. We further assume that $\left(\bm{U}'\bm{U} - \bm{U}_i'\bm{U}_i\right)$ is of full rank for $i = 1,...,m$.

We can estimate the $\bs\beta$ using weighted least squares (WLS), where for cluster $i$ we define $\bm{W}_i$ to be a symmetric, $n_i \times n_i$ weighting matrix of full rank. 
Importantly, the WLS framework includes the unweighted case (where $\bm{W}_i = \bm{I}_i$, an identity matrix), as well as feasible GLS.\footnote{
The WLS estimator also encompasses the estimator proposed by \citet{Ibragimov2010tstatistic} for clustered data. 
Assuming that $\bm{X}_i$ has rank $p$ for $i = 1,...,m$, their proposed approach involves estimating $\bs\beta$ separately within each cluster and taking the simple average of these estimates. 
The resulting average is equivalent to the WLS estimator with weights $\bm{W}_i = \bm{X}_i \left(\bm{X}_i'\bm{X}_i\right)^{-2} \bm{X}_i$.} 
In the latter case, it is assumed that $\Var\left(\bm{e}_i\left|\bm{X}_i\right.\right) = \bs\Phi_i$, where $\bs\Phi_i$ is a known function of a low-dimensional parameter. 
For example, an auto-regressive error structure might be posited to describe repeated measures on an individual over time. 
The weighting matrices are then taken to be $\bm{W}_i = \hat{\bs\Phi}_i^{-1}$, where the $\hat{\bs\Phi}_i$ are constructed from estimates of the variance parameter.
Finally, for analysis of data from complex survey designs, WLS may be used with sampling weights in order to account for unequal selection probabilities.

\subsection{Absorption}

In most analyses, the goal is to estimate and test hypotheses regarding the parameters in $\bs\beta$. 
This means that the values of $\bs\gamma$ and $\bs\mu$ are nuisance parameters and are not of inferential interest. 
Estimating each of these fixed effects---as occurs if the fixed effects are included as dummy variables in the model---can be computationally intensive and numerically inaccurate if the number of clusters is large (i.e., $s + t$ large). 
In the policy example given above, for example, this could easily result in over 70 parameters (i.e., 50 states, 20 time periods).
An alternative that is commonly implemented, therefore, is to first absorb the fixed effects. 
This amounts to demeaning the data by substracting the cluster-mean value from both the outcomes and covariates; this results in the "within" estimator, which is commonly implemented in panel data analyses. 
By aborbing the fixed effects, only the $r$ parameters in $\bs\beta$ need to be estimated, which results in a more computationally efficient and numerically accurate procedure. 

In Section 2 of this paper, we will discuss more fully comparisons between the dummy variable and aborption approaches to fixed effects. 
In order to do, we now formalize the absorption method.
To begin, denote the full block-diagonal weighting matrix as $\bm{W} = \text{diag}\left(\bm{W}_1,...,\bm{W}_m\right)$.
Let $\bm{K}$ be the $x \times r$ matrix that selects the covariates of interest, so that $\bm{X} \bm{K} = \bm{R}$ and $\bm{K}'\bs\alpha = \bs\beta$.
For a generic matrix $\bm{Z}$ of full column rank, let $\bm{M_Z} = \left(\bm{Z}'\bm{W}\bm{Z}\right)^{-1}$ and $\bm{H_Z} = \bm{Z}\bm{M_Z}\bm{Z}'\bm{W}$. 

The absorption technique involves obtaining the residuals from the regression of $\bm{y}$ on $\bm{T}$ and from the multivariate regressions of $\bm{U} = [\bm{R}\ \bm{S}]$ on $\bm{T}$. 
The $\bm{y}$ residuals and $\bm{R}$ residuals are then regressed on the $\bm{S}$ residuals. 
Finally, these twice-regressed $\bm{y}$ residuals are regressed on the twice-regressed $\bm{R}$ residuals to obtain the WLS estimates of $\bs\beta$. 
Let $\bm{\ddot{S}} = \left(\bm{I} - \bm{H_T}\right)\bm{S}$, $\bm{\ddot{R}} = \left(\bm{I} - \bm{H_{\ddot{S}}}\right)\left(\bm{I} - \bm{H_T}\right)\bm{R}$, and $\bm{\ddot{y}} = \left(\bm{I} - \bm{H_{\ddot{S}}}\right)\left(\bm{I} - \bm{H_T}\right)\bm{y}$. 
In what follows, subscripts on $\bm{\ddot{R}}$, $\bm{\ddot{S}}$,  $\bm{\ddot{U}}$, and $\bm{\ddot{y}}$ refer to the rows of these matrices corresponding to a specific cluster. 
The WLS estimator of $\bs\beta$ can then be written as
\begin{equation}
\label{eq:WLS}
\bs{\hat\beta} = \bm{M_{\ddot{R}}} \sum_{i=1}^m \bm{\ddot{R}}_i' \bm{W}_i \bm{\ddot{y}}_i. 
\end{equation}
This estimator is algebraically identical to the direct WLS estimator based on the full set of predictors, \[
\bs{\hat\beta} = \bm{K}'\bm{M_X} \sum_{i=1}^m \bm{X}_i' \bm{W}_i \bm{y}_i,
\]
but avoids the need to solve a system of $x$ linear equations.

\subsection{Standard CRVE}

\todo[inline]{James -- it says here that this is general (dummies or absorbed), but the math indicates absorbtion. It may be better to start by saying our focus is on absorbing since it is more common, and then later discuss how it's important to show equivalence to the dummy case.}

In the remainder of this paper, we focus on the general case in which fixed effects are either included as dummy variables or absorbed. 
In either case, the goal of the analysis is test hypotheses regarding $\bs\beta$ using the WLS estimator $\bs{\hat\beta}$, which has true variance,
\begin{equation}
\label{eq:var_WLS}
\Var\left(\bs{\hat\beta}\right) = \bm{M_{\ddot{R}}}\left(\sum_{i=1}^m \bm{\ddot{R}}_i' \bm{W}_i \bs\Sigma_i \bm{W}_i\bm{\ddot{R}}_i\right) \bm{M_{\ddot{R}}},
\end{equation}
which depends upon the unknown variance matrices $\bm\Sigma_i$. 
A model-based approach to estimating this variance would involve assuming a structure to this $\bm\Sigma_i$; for example, it may be assumed that the structure was hierarchical or auto-regressive. 
However, if the model is mis-specified, the model-based variance estimator will be inconsistent and inferences based upon it will be invalid.

The CRVE approach is to instead estimate $\Var\left(\bs{\hat\beta}\right)$ empirically. 
While there are several versions of this approach, all can be written in the form
\begin{equation}
\label{eq:V_small}
\bm{V}^{CR} = \bm{M_{\ddot{R}}}\left(\sum_{i=1}^m \bm{\ddot{R}}_i'\bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i' \bm{A}_i' \bm{W}_i \bm{\ddot{R}}_i\right) \bm{M_{\ddot{R}}},
\end{equation}
for some $n_i$ by $n_i$ adjustment matrix $\bm{A}_i$. 
Note that this estimator replaces the unknown $\bm\Sigma_i$ with the cross-product of the the residuals, $\bm{e}_i \bm{e}_i'$. 

The form of these adjustments parallels those of the heteroscedastity-consistent (HC) variance estimators proposed by \citet*{MacKinnon1985some}. 
Setting $\bm{A}_i = \bm{I}_i$, an $n_i \times n_i$ identity matrix, results in the most basic form, described by \citet{Liang1986longitudinal}. 
Following \citet{Cameron2015practitioners}, we refer to this estimator as $\bm{V}^{CR0}$. 
Setting $\bm{A}_i = c\bm{I}_i$, where $c = \sqrt{(m/(m-1))(N/(N - p))}$, results in a slightly larger estimator.
Note that when $N >> p$, $c \approx \sqrt{m/(m-1)}$, and software typically uses the latter approximation (e.g., SAS); for this reason we refer to this approximation as $\bm{V}^{CR1}$.
Both the CR0 and CR1 estimators rely on asymptotic properties of the residuals in order to consistently estimate $\bs\Sigma_i$. 
The CR1 estimator is now standard in most analyses in economics.

In addition to CR1, two other estimators are also currently available for improving small sample properties. 
Unlike the CR1 estimator, these approaches result in adjustments that take into account features of the covariates in $\bm{X}_i$. 
In the next section, we describe in detail the BRL approach, which is an extension of the HC2 estimator for regressions with heteroskedastic but uncorrelated errors; we therefore refer to it as CR2. 
A further alternative is CR3, which uses adjustment matrices given by $\bm{A}_i = \left(\bm{I} - \bm{\ddot{R}}_i \bm{M_{\ddot{R}}}\bm{\ddot{R}}_i'\bm{W}_i\right)^{-1}$. The CR3 estimator closely approximates the jackknife re-sampling variance estimator \citep{Bell2002bias, Mancl2001covariance}. 
As they indicate, however, the CR3 estimator tends to over-correct the downward bias, while the CR1 estimator tends to under-correct. 
The CR2 estimator offers a solution in the middle, and for this reason we focus on it for the remainder of this paper. 

\section{BIAS REDUCED LINEARIZATION}
\label{sec:BRL}

The unadjusted CR0 estimator and, to a smaller degree, the CR1 estimator both tend to under-estimate the true variance of $\hat{\bs\beta}$ \citep{Cameron2015practitioners}. Simulation studies indicate that the degree of this bias, however, depends not only on the number of clusters $m$, but also on features of the covariates in $\bm{X}_i$. 
MacKinnon (2013) shows that this bias is largest when the covariate exhibits large imbalances, skew, or leverage. 
For this reason, it is desirable to develop an adjustment that takes into account features of the covariates. 
As noted above, both the BRL approach (CR2) and the jacknife approach (CR3) meet these requirements.

Unlike CR1 and CR3, however, the BRL approach requires the analyst to specify a ``working'' model for the correlation structure. 
The BRL estimator then defines the adjustment matrices $\bm{A}_i$ so that the CR2 estimator is exactly unbiased when this working model is correct. 
The idea of specifying a model may seem antithetical to the purpose of using CRVE, yet extensive simulation studies have illustrated that the method performs better in small samples than any of the other approaches, even when the working model is incorrect (see Section 4 of this paper for a review). 
Although the CR2 estimator is no longer exactly unbiased when the working model is mis-specified, its bias tends to be greatly reduced compared to CR1 or CR0 (thus the name ``bias reduced linearization''). Furthermore, as the number of clusters increases, reliance on the working model diminishes. 
In a sense, CR2 provides necessary scaffolding in the small-sample case, which falls away when there is sufficient data.

Let $\bs\Phi_i$ denote a working model for the covariance of the errors in cluster $i$, with $\bs\Phi = \text{diag}\left(\bs\Phi_1,...,\bs\Phi_m\right)$. In the original formulation of \citet{Bell2002bias}, the BRL adjustment matrices were chosen to satisfy the criterion
\begin{equation}
\label{eq:CR2_criterion_BM}
\bm{A}_i \left(\bm{I} - \bm{H_X}\right)_i \bs\Phi \left(\bm{I} - \bm{H_X}\right)_i' \bm{A}_i'  =  \bs\Phi_i 
\end{equation}
where $\left(\bm{I} - \bm{H_X}\right)_i$ denotes the rows of $\bm{I} - \bm{H_X}$ corresponding to cluster $i$.
Calculation of the adjustment matrices $\bm{A}_i$ involves taking the inverse of the symmetric square-root of a matrix. 
For example, if the working model and weight matrices are both taken to be identity matrices, then $\bm{A}_i = \left(\bm{I}_i - \bm{\ddot{R}}_i\bm{M_{\ddot{R}}} \bm{\ddot{R}}_i'\right)^{-1/2}$. 
However, this formulation of $\bm{A}_i$ is problematic for some fixed effects models that are common in economic applications. 
In the next two subsections, we address two problems that arise, thereby articulating a BRL methodology that is suitable for a wide range of applications.

\subsection{Generalized Inverse}

The equality defining the $\bm{A}_i$ matrices cannot always be solved because it is possible that some of the matrices involved are not of full rank, and thus cannot be inverted. 
For example, \citet{Angrist2009mostly} note that this problem arises in balanced state-by-year panel models that include fixed effects for states and for years. 
In order to address this concern, we provide an alternative criterion for the adjustment matrices that can always be satisfied. 
Instead of criterion (\ref{eq:CR2_criterion_BM}), we seek adjustment matrices $\bm{A}_i$ that satisfy:
\begin{equation}
\label{eq:CR2_criterion}
\bm{\ddot{R}}_i' \bm{W}_i \bm{A}_i \left(\bm{I} - \bm{H_X}\right)_i \bs\Phi \left(\bm{I} - \bm{H_X}\right)_i' \bm{A}_i' \bm{W}_i \bm{\ddot{R}}_i = \bm{\ddot{R}}_i' \bm{W}_i \bs\Phi_i \bm{W}_i \bm{\ddot{R}}_i.
\end{equation}
A variance estimator that uses such adjustment matrices will be exactly unbiased when the working model is correctly specified.

The above criterion (\ref{eq:CR2_criterion}) does not uniquely define $\bm{A}_i$. Following \citet{McCaffrey2001generalizations}, we propose to use a symmetric solution in which
\begin{equation}
\label{eq:CR2_adjustment}
\bm{A}_i = \bm{D}_i' \bm{B}_i^{+1/2} \bm{D}_i,
\end{equation}
where $\bm{D}_i$ is the upper-right triangular Cholesky factorization of $\bs\Phi_i$, 
\begin{equation}
\label{eq:CR2_Bmatrix}
\bm{B}_i = \bm{D}_i\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_i \left(\bm{I} - \bm{H_{\ddot{S}}}\right) \left(\bm{I} - \bm{H_T}\right) \bs\Phi \left(\bm{I} - \bm{H_T}\right)' \left(\bm{I} - \bm{H_{\ddot{S}}}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_i' \bm{D}_i',
\end{equation}
and $\bm{B}_i^{+1/2}$ is the symmetric square root of the Moore-Penrose inverse of $\bm{B}_i $. 
The Moore-Penrose inverse is well-defined and unique even when $\bm{B}_i$ is not of full rank \citep[][Thm. 9.18]{Banerjee2014linear}. These adjustment matrices satisfy criterion (\ref{eq:CR2_criterion}), as stated in the following theorem.

\begin{thm}
\label{thm:BRL_FE}
Let $\bm{L}_i = \left(\bm{\ddot{U}}'\bm{\ddot{U}} - \bm{\ddot{U}}_i'\bm{\ddot{U}}_i\right)$ and assume that $\bm{L}_1,...,\bm{L}_m$ have full rank $r + s$. Further assume that $\Var\left(\bs\epsilon_i\left|\bm{R}_i,\bm{S}_i,\bm{T}_i\right.\right) = \bs\Phi_i$, for $i = 1,...,m$. Then the adjustment matrix $\bm{A}_i$ defined in (\ref{eq:CR2_adjustment}) and (\ref{eq:CR2_Bmatrix}) satisfies criterion (\ref{eq:CR2_criterion}) and $\bm{V}^{CR2}$ is exactly unbiased.
\end{thm}

Proof is given in Appendix \ref{app:theorems}. If $\bm{B}_i$ is of full rank, then the adjustment matrices also satisfy the original criterion (\ref{eq:CR2_criterion_BM}). Furthermore, because the adjustment matrices are defined in terms of all three components of the predictors ($\bm{R}$, $\bm{S}$, and $\bm{T}$), they are invariant to whether the model is estimated by direct WLS estimation or after absorbing some or all of the fixed effects. 
Thus, these modified BRL adjustment matrices can be calculated in a wider range of applications.

\subsection{Absorption and Dummy Equivalence}

A second problem, highlighted by \citet{Cameron2015practitioners}, is that the small-sample CRVE approach can result in a different estimator depending upon if the fixed effects are included in the model as dummies or absorbed. 
For example, this problem arises with the CR1 estimator, which has the form $\bm{A}_i = c\bm{I}_i$, where $c = \sqrt{(m/(m-1))(N/(N - p))}$. In this estimator, $p$ depends on the total number of covariates estimated in the model.
When fixed effects are included as dummies, $p = r + s + t$, whereas when the fixed effects are absorbed, instead $p = r$. 
Cameron and Miller highlight that this can be particularly problematic if the clusters are small, as when they each include a pair of individuals; in these studies, the correction results in a variance that is over twice as large when using absorption compared to the use of dummies.

This non-equivalence problem given above can also arise when implementing the BRL method.
\todo[inline]{Explain how the non-equivalence stuff works---it's a matter of assuming a working model for the errors or the working model for the residuals (after absorption).}

Below, we show that by defining the problem in a particular way, the adjustment matrices $\bm{A}_i$ are invariant to the method for adjusting for fixed effects.
To see how, begin by noting that in many applications, it will make sense to choose weighting matrices that are the inverses of the working covariance model, so that $\bm{W}_i = \bs\Phi_i^{-1}$. 

\todo[inline]{James-- I find from here on (including above sentence) confusing. It sounds like we are requiring the use of inverse variance weights for this result, but then the last paragraph gives the case in which W = I. Could it be formulated more generally?}

In this case, the adjustment matrices can be calculated using $\bm{\tilde{B}}_i$ in place of $\bm{B}_i$, where
\begin{equation}
\label{eq:CR2_B_tilde}
\bm{\tilde{B}}_i = \bm{D}_i\left(\bm{I} - \bm{H_{\ddot{R}}}\right)_i \left(\bm{I} - \bm{H_{\ddot{S}}}\right) \bs\Phi \left(\bm{I} - \bm{H_{\ddot{S}}}\right)' \left(\bm{I} - \bm{H_{\ddot{R}}}\right)_i' \bm{D}_i'.
\end{equation}

\begin{thm}
\label{thm:absorb}
Let $\bm{\tilde{A}}_i = \bm{D}_i'\bm{\tilde{B}}_i^{+1/2} \bm{D}_i$, where $\bm{\tilde{B}}_i$ is given in (\ref{eq:CR2_B_tilde}). If $\bm{T}_i \bm{T}_k' = \bm{0}$ for $j \neq k$ and $\bm{W} = \bs\Phi^{-1}$, then $\bm{A}_i = \bm{\tilde{A}}_i$. 
\end{thm}
Proof: See Appendix A.

As theorem \ref{thm:absorb} above demonstrates, using $\bm{\tilde{B}}_i$ rather than $\bm{B}_i$ leads to algebraically identical adjustment matrices; the form of $\bm{\tilde{B}}_i$ is simply more convenient for computation.
Interestingly, in the simple case of ordinary (unweighted) least squares, in which the working variance model posits that the errors are all independent and homoskedastic and $\bm{W} = \bs\Phi = \bm{I}$, the adjustment matrices simplify further to \[
\bm{A}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i\left(\bm{\ddot{U}}'\bm{\ddot{U}}\right)^{-1}\bm{\ddot{U}}_i'\right)^{+1/2},\]
where $\bm{\ddot{U}} = \left(\bm{I} - \bm{H_T}\right)\bm{U}$.
Importantly, unlike the CR1 approach, this means that in the CR2 approach the analyst is not left to choose the method for accounting for fixed effects in an ad hoc fashion.
Together, these two reformulations to the BRL method provided here allow for the approach to be implemented in a broad range of economic applications.
In the next section, we address a final set of concerns: how to implement the method in hypothesis testing. 


\section{HYPOTHESIS TESTING}
\label{sec:testing}

Until now, we have focused on different approaches to estimating cluster-robust standard errors in small samples. 
However, standard errors are of limited inherent interest---rather, their main use is for the construction of hypothesis tests and confidence intervals.
Cluster-robust Wald-type test statistics are a function of the parameter estimates $\bs{\hat\beta}$ and the corresponding CRVE matrix.
Such tests are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., as $m \to \infty$). 

Like the research on the bias of the CRVE estimator, evidence from a wide variety of contexts indicates that the asymptotic limiting distribution of these statistics may be a poor approximation when the number of clusters is small, even if corrections such as CR2 are employed \citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. 
Like the bias of the CRVE estimator itself, the accuracy of the asymptotic approximations depends on design features such as the degree of imbalance, skewness, and leverage in the covariates, and similarity of cluster sizes \citep{McCaffrey2001generalizations, Tipton2015small-F, Webb2013wild}. 
This provides motivation for development of general-purpose hypothesis testing procedures that have accurate rejection rates in small samples.

In this section, we develop a general method for conducting hypothesis tests based on CRVE. We consider linear constraints on $\bs\beta$, where the null hypothesis has the form $H_0: \bm{C}\bs\beta = \bm{d}$ for fixed $q \times r$ matrix $\bm{C}$ and $q \times 1$ vector $\bm{d}$. 
The cluster-robust Wald statistic is then
\begin{equation}
\label{eq:Wald_stat}
Q = \left(\bm{C}\bs{\hat\beta} - \bm{d}\right)'\left(\bm{C} \bm{V}^{CR} \bm{C}'\right)^{-1}\left(\bm{C}\bs{\hat\beta} - \bm{d}\right),
\end{equation}
where $\bm{V}^{CR}$ is any of the cluster-robust estimators described above.
In large samples, it can be shown that this Wald test rejects $H_0$ at level $\alpha$ if $Q$ exceeds $\chi^2(\alpha; q)$, the $\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom. 
%When $q = 1$, an equivalent statement is that the test follows a standard normal distribution in large samples.\todo{Can we wait to get into the q = 1 case?}
In practice it is rarely clear how large samples need to be for Wald tests to be implemented.
%In the $q = 1$ case, the standard is therefore to instead use the t-distribution instead, with degrees of freedom $m - 1$ when CR1 is employed.
%Similarly, when $q > 1$, this results in the test $F = Q/q$, which is compared to the $F(q, m - 1)$ reference distribution.
%\todo{Is this really standard?} 

%By moving to the t and F-distributions, the CR1 corrections greatly improve the performance of these Wald-type tests in small samples.
%However, these corrections alone are not adequate, since - like the bias adjustment - the degrees of freedom do not account adequately for features of the covariates that affect performance of the statistics.
%Cameron and Miller highlight this problem, noting that the BRL approach provides an alternative method for estimating the degrees of freedom using a Satterthwaite approximation. 
%This method has been widely tested and shown to perform well; we review the method and findings in the next sub-section.
%Importantly, however, no such small-sample approach has been developed for multi-parameter hypothesist tests. 
%In the sub-section that follows, we develop such a test.
%Our approach is similar to a Satterthwaite approximation, in that it involves approximating the distribution of $Q$ using an $F$ distribution with estimated degrees of freedom. 

\subsection{Small-sample corrections for t-tests}
 
Consider testing the hypothesis $H_0: \bm{c}'\bs\beta = 0$ for some fixed $r \times 1$ contrast vector. 
For this one-dimensional constraint, an equivalent to the Wald F test is to use the test statistic $Z = \bm{c}'\bs{\hat\beta} / \sqrt{\bm{c}'\bm{V}^{CR}\bm{c}}$, which follows a standard normal distribution in large samples. 
In small samples, following the CR1 approach, it is common to instead approximate the distribution of $Z$ by a $t(m - 1)$ distribution. 
\citet{Hansen2007asymptotic} provided one justification for the use of this reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \citep[see also][]{Donald2007inference}. 
\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values would be conservative (leading to rejection rates less than or equal to $\alpha$).
However, both of these arguments require that $\bm{c}'\bs\beta$ be separately identified within each cluster. 
Outside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \citep{Cameron2015practitioners}. 
Furthermore, using these critical values does not take into account that the distribution of $\bm{V}^{CR}$ is affected by the structure of the covariate matrix. 
%An alternative, proposed by \citet{Bell2002bias}, is to approximate the distribution of $Z$ by a $t$ distribution with degrees of freedom determined by a Satterthwaite approximation, under the working covariance model.

An alternative t-test developed by \citet{Bell2002bias} involves using a $t(\nu)$ references distribution with degrees of freedom $\nu$, which are estimated by a Satterthwaite approximation.
The Satterthwaite approximation \citep{Satterthwaite1946approximate} entails using degrees of freedom that are a function of the the first two moments of the sampling distribution of $\bm{c}' \bm{V}^{CR} \bm{c}$.
Theoretically, these degrees of freedom should be 
\begin{equation}
\label{eq:nu_Satterthwaite}
\nu = \frac{2\left[\E\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)\right]^2}{\Var\left(\bm{c}'\bm{V}^{CR2}\bm{c}\right)}.
\end{equation}
Expressions for the first two moments of $\bm{c}'\bm{V}^{CR2}\bm{c}$ can be derived under the assumption that the errors $\bs\epsilon_1,...,\bs\epsilon_m$ are normally distributed; see Appendix \ref{app:VCR_dist}. 

In practice, both moments involve the variance structure $\bs\Sigma$, which is unknown. 
\citet{Bell2002bias} proposed to estimate the moments based on the same working model that is used to derive the adjustment matrices. 
This ``model-assisted'' estimate of the degrees of freedom is then calculated as 
\begin{equation}
\label{eq:nu_model}
\nu_{M} = \frac{\left(\sum_{i=1}^m \bm{p}_i' \hat{\bs\Phi} \bm{p}_i\right)^2}{\sum_{i=1}^m \sum_{i=1}^m \left(\bm{p}_i' \hat{\bs\Phi} \bm{p}_i\right)^2},
\end{equation}
where $\bm{p}_i = \left(\bm{I} - \bm{H_X}\right)_i'\bm{A}_i \bm{W}_i\bm{\ddot{R}}_i\bm{M_{\ddot{R}}} \bm{c}$.
%\todo{Can we use $\bm{H_{\ddot{U}}}$ here instead?} 
Alternately, for any of the CRVEs one could instead use an ``empirical'' estimate of the degrees of freedom, constructed by substituting $\bm{e}_i \bm{e}_i'$ in place of $\bs\Sigma_i$. 
However, \citet{Bell2002bias} found using simulation that this plug-in degrees of freedom estimate led to very conservative rejection rates. 

The \citet{Bell2002bias} approach has been shown to perform well in a variety of conditions (see Section 4 of this paper). 
These studies encompass a variety of data generation processes, covariate types, and weighting procedures. 
A key finding is that the degrees of freedom depend not only on the number of clusters $m$, but also on features of the covariates. 
When the covariate is balanced across clusters---as occurs in balanced panels with a dichotomous covariate with the same proportion of ones in each cluster---the degrees of freedom are $m - 1$ even in small samples. 
However, when the covariate exhibits large imbalances---as occurs when the panel is not balanced or if the proportion of ones varies from cluster to cluster---the degrees of freedom can be considerably smaller. 
Similarly, covariates with large leverage points will tend to exhibit lower of degrees of freedom. 


By adjusting the degrees of freedom to account for these features, the Type I error rate of the test is nearly-always less than or equal to nominal, so long as the degrees of freedom are larger than 4 or 5 \citep{Bell2002bias, Tipton2015small-t}.
This is because when the degrees of freedom are smaller, the t-distribution approximation to the sampling distribution doesn't hold, and the Type I error can be higher than the stated $\alpha$ level.
In comparison, the CR1 degrees of freedom (i.e., $m - 1$) are constant, and the test only performs well when in the cases in which the covariates are balanced.
Importantly, because the degrees of freedom are covariate-dependent, it is not possible to assess whether a small-sample correction is needed based solely on the total number of clusters in the data. 
Consequently, these studies argues that t-tests based on CRVE should routinely use the CR2 variance estimator and the Satterthwaite degrees of freedom, even when $m$ appears to be large.

% In addition to the Satterthwaite approximation, two other promising methods for testing single-constraint hypotheses have been proposed. \citet{McCaffrey2006improved} described a saddlepoint approximation to the distribution of $Z$, providing simulation evidence that a test based on a this approximation offers even more accurate rejection rates than the Satterthwaite approximation. 
% A further method is to use a bootstrap re-sampling technique that provides small-sample refinements in the test rejection rates.
% \citet{Cameron2008bootstrap} studied several such bootstrapping techniques. 
% \citet{Webb2013wild} describe a wild boostrap procedure that performs well even when $m$ is quite small and when clusters are of unequal size.  

\subsection{Small-sample corrections for F-tests}

Little research has considered small-sample corrections for multiple-constraint hypothesis tests based on cluster-robust Wald statistics.
Cameron and Miller highlight this problem, proposing a set of ad-hoc adjustments based on the BRL approach to t-tests, noting that some form of adjustment must be required given the extensive work on single-parameter tests.
In this sub-section, we propose an approach to multi-parameter testing that closely parallels the BRL method for t-tests.
In this approach, we approximate the distribution of $Q / q$ by a multiple of an F distribution with estimated degrees of freedom. 
The sampling distribution of $Q$ is then approximated by Hotelling's $T^2$ distribution, a multiple of an $F$ distribution. 
Specifically, suppose that $\eta \bm{C}\bm{V}^{CR2} \bm{C}'$ approximately follows a Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C} \Var\left(\bs{\hat\beta}\right)\bm{C}'$, then 
\begin{equation}
\label{eq:AHT}
\left(\frac{\eta - q + 1}{\eta q}\right) Q \ \dot\sim \ F(q, \eta - q + 1).
\end{equation}
We will refer to this as the approximate Hotelling's $T^2$ (AHT) test.
We consider how to estimate $\eta$ below.
This approach is conceptually similar to the Satterthwaite approximation for one-dimensional constraints and reduces to it if $q = 1$. 
For $q > 1$, however, the test depends on multivariate features of the covariates, including both CRVE estimates of variances and covariances. 

Tipton and Pustejovsky (in press) recently introduced this test for use in a special case of CRVE used in meta-analysis. 
This test was developed in relation to other literature that focused on approximating the distribution of a robust variance estimator by a Wishart for some simpler models that are special cases of CRVE. 
\citet{Zhang2013tests, Zhang2012twowayANOVA} described an AHT test for contrasts in analysis of variance models with unequal within-cell variance, which are particularly simple cases of linear models with heteroskedastic error terms. 
\citet{Zhang2012MANOVA} extended the method to multivariate analysis of variance models where the covariance of the errors differs across cells, a special case of model (\ref{eq:fixed_effects}) in which the CR2 variance estimator has a particularly simple form. In all of these cases, Zhang demonstrated that the robust variance estimator is a mixture of Wishart distributions that is well-approximated by a Wishart distribution with estimated degrees of freedom. 
Additionally, \citet{Pan2002small} described an F-test based on CR0 for use in GEE models, which also uses the Wishart approximation to the distribution of $\bm{V}^{CR}$ but estimates the degrees of freedom using a different method than the one we describe below.

The contribution of the present paper is to extend the AHT test to the more general setting of linear models with fixed effects. 
The remaining question is how to estimate the parameter $\eta$, which determines scalar multiplier and demoninator degrees of freedom of the F-test. 
To do so, we estimate the degrees of freedom of the Wishart distribution so that they match the mean and variance of $\bm{C}\bm{V}^{CR} \bm{C}'$ under the working variance model $\bs\Phi$, just as in the degrees of freedom for the t-test. 
The problem that arises in doing so is that when $q > 1$ it is not possible to exactly match both moments. 
\cite{Pan2002small} proposed to choose $\eta$ to minimize the squared differences between the covariances among the entries of $\eta \bm{C}\bm{V}^{CR}\bm{C}'$ and the covariances of the Wishart distribution with $\eta$ degrees of freedom and scale matrix $\bm{C}\bm{V}^{CR}\bm{C}'$. 
\citet{Zhang2012MANOVA} instead matches the mean and total variance of $\bm{C}\bm{V}^{CR}\bm{C}'$ (i.e., the sum of the variances of its entries), which avoids the need to calculate any covariances. 
In what follows we focus on this latter approach, which Tipton and Pustejovsky (in press) have found to perform better in practice.

Let $\bm{c}_1,...,\bm{c}_q$ denote the $p \times 1$ row-vectors of $\bm{C}$. 
Let $\bm{p}_{sh} = \left(\bm{I} - \bm{H}\right)_h'\bm{A}_h'\bm{W}_h\bm{X}_h\bm{M}\bm{c}_s$ for $s = 1,...,q$ and $h = 1,...,m$. 
The degrees of freedom are then estimated under the working model as
\begin{equation}
\label{eq:eta_model}
\eta_M = \frac{\sum_{s,t=1}^q \sum_{h,i=1}^m b_{st} \bm{p}_{sh}'\hat{\bs\Phi}\bm{p}_{th} \bm{p}_{si}'\hat{\bs\Phi}\bm{p}_{ti}}{\sum_{s,t=1}^q \sum_{h,i=1}^m \bm{p}_{sh}'\hat{\bs\Phi}\bm{p}_{ti} \bm{p}_{sh}'\hat{\bs\Phi}\bm{p}_{ti} + \bm{p}_{sh}'\hat{\bs\Phi}\bm{p}_{si} \bm{p}_{th}'\hat{\bs\Phi}\bm{p}_{ti}},
\end{equation}
where $b_{st} = 1 + (s=t)$ for $s,t=1,..,q$.\todo{Note that this formulation is different from what we did in the JEBS paper. Are they equivalent? Are they both invariant to transformation of the constraint matrix?}
Note that $\eta_M$ reduces to $\nu_M$ from Equation (\ref{eq:nu_model}) if $q = 1$.

This F-test shares features with the t-test developed by Bell and McCaffrey. Like the t-test, the degrees of freedom of this F-test depend not only on the number of clusters, but also on features of the covariates being tested. 
Again, these degrees of freedom can be much smaller than $m - 1$, and are particularly smaller when the covariates being tested exhibit high imbalances or leverage. 
Unlike the t-test case, however, in multi-parameter case, it is often more difficult to diagnose the cause of these small degrees of freedom. 
In some situations, however, these are straightforward extensions to the findings in t-tests. 
For example, if the goal is to test if there are differences across a four-arm treatment study, the degrees of freedom are largest (and close to $m - 1$) when the treatment is allocated equally across the four groups within each cluster. 
When the proportion varies across clusters, these degrees of freedom fall, often leading to degrees of freedom in the ``small sample'' territory even when the number of clusters is large. 

\section{Simulation Study}

Throughout this paper, we have argued that the CR2 based hypothesis tests perform significantly better than the standard CR1 tests commonly used in economic applications. 
This argument is based on results from several large simulation studies. 
In this section, we review the design of these studies and their results, with particular emphasis on the role of covariate features and sample size on the Type I error rates of these tests.
Throughought, we use "CR2S" test to refer to those tests employing both the BRL correction to the variance and the Satterthwaite degrees of freedom. For t-tests, CR2S thus refers to test described in Section 3.1, while for the F-test CR2S refers the that found in Section 3.2. 

\subsection{Review of previous simulation studies}

To date, four papers have examined the performance of the CR2 t-test and these papers have included nearly 100 parameter combinations. 
The results of these studies are indicated in TABLE 1. 
As Table 1 illustrates, these simulation studies have included a range of applications; for example, Cameron and Miller (2015) and Imbens and Kolesar (2015) have focused on conditions common in economics, while Bell and McCaffrey (2002) focused on those common in complex surveys, and Tipton (2015) on those in meta-analysis. 
Some of these studies have focused on policy dummies in the balanced case, while others have varied the degree of balance, and yet others have examined continous covariates that are symmetrically distributed, as well as those with high skew and leverage.
These studies have also examined the role of the number of clusters, with values ranging from 6 to 50, as well as the number of observations per clusters (from 1 to roughly 260).
Finally, the studies have examined a range of both true error structures---including various combinations of heteroscedasticity and clustering---and estimation strategies, with a focus in particular on different 'working' models. 
By varying the working model from the true error structure, the studies allow for the effect of model mispecification to be studied.
Finally, while most of these studies focus on comparisons in OLS, one study (Tipton, 2015) focuses on the use of WLS.

The last column of Table 1 indicates the range of Type I error rates given over the conditions studied in each of the simulation studies, with values given for both the CR1 and CR2S tests.
Across these studies, this Type I error for the CR1 t-test ranges from 0.01 to 0.34, when the stated $\alpha$ level was 0.05. 
These values are particularly far above nominal when the covariate tested is unbalanced, skewed (i.e., high leverage), or when the number of observations per cluster varies. 
While not illustrated by the table, importantly these high Type I error rates occurred not only when the number of clusters was very small, but also at moderate sizes if the covariate imbalance or skew was large. 

In comparison, the CR2S t-test performs considerably better across the range of conditions studied, with Type I error rates ranging between 0.01 and 0.13.
Notably, the largest value observed here is for the Imbens and Kolesar (2015) study, which does not break results out by degrees of freedom. 
Given the condition studied (30 clusters, with only 3 having a policy dummy), these degrees of freedom are possibly below the 4 or 5 cut-off at which others have shown the t-test approximation to fail. 
Putting this value aside, the maximum Type I error observed in these conditions is 0.06, only slightly higher than nominal.
Importantly, these nearly nominal Type I error rates hold across a wide degree of model mispecification (i.e., when the working model is far from the true error structure) and covariate types.
While not shown in the table, this is because the CR2S degrees of freedom take into account these covariate features in their degrees of freedom, which are often far below those in the CR1 test.

%Table 1: Review of previous literature

In comparison to the t-test, the CR2 F-test has only been recently developed and studied in a single paper focused on the meta-analytic case (Tipton and Pustejovsky, 2015).
While focusing only on the use of CRVE with weights (WLS), however, the simulation study was comprehensive in other regards.
The simulation studies examined the effects of the number of covariates in the full model (up to $p = 5$) and the number of constraints being tested ($q = 2,3,4,5$), including cases in which $p = q$ and in which $p > q$. 
These simulations examined models with various combinations of the 5 covariates found in Tipton (2015), found in the last line of Table 1. 
This set included both balanced and imbalanced dummies, as well as symmetric and skewed continuous covariates.
Like Tipton (2015), these simulations focused on true correlation structures that included heteroscedasticity, clustering (i.e., a cluster specific random effect), and correlated errors.
The working models for these were then varied far from the true error stucture; for example, based on models with no clustering at all.
Finally, the number of clusters was varied from 10 to 100, each with between 1 and 10 observations. 
Type I error was then compared over a range of $\alpha$ levels for both the CR1 and CR2S F-tests.
The findings indicated that the CR2S F-test always had Type I error less than or equal to the stated $\alpha$ level, except in cases with extreme model mispecification. 
Even in these cases, however, the Type I error was in line with those observed for t-tests; for example, for $\alpha = 0.05$ the error was not above 0.06. 
In comparison, the Type I error of the CR1 test was often very high (between XX and XXX), with particularly large differences when the degrees of freedom of the two tests differed.
Again, like the t-test, the degrees of freedom of the CR2S F-test here were driven by covariate features, with particularly low degrees of freedom resulting when the covariate set studied included imbalances or skew.

\subsection{Simulation Design}

While the simulation study produced by Tipton and Pustejovsky (in press) included a variety of conditions, the set studied focused largely on the types of data found in meta-analysis and regression more broadly. 
These differ from the economic context in two ways.
First, in meta-analysis, it is common for there to be both heteroscedasticity assumed and for analysts to implement weights in the analysis (typically inverse-variance).
This use of weights is less common in economic applications.
Second, in meta-analysis, analysts are typically interested in a variety of covariate relationships.
In comparison, in economics, many applications focus on the effects of different policies (and thus dummies), particularly policies that may vary at the cluster and/or observation level.
For example, a policy may be implemented in a portion of states (the cluster level), or in all states, but at different time points (the observation level), or a combination of the two.
Given these differences, in this paper we provide an additional simulation study focused on designs more commonly found in economics.
Together with the results from Tipton and Pustejovsky (2015), the studies cover a wide range of conditions, offering a comprehensive examination of the CR2S F-test in application.

%Data generation

\subsection{Simulation Results}

\begin{Schunk}
\begin{Sinput}
> # setwd("paper_ClusterRobustTesting")
> library(knitr)
> library(xtable)
> library(foreign)
> library(plyr)
> library(tidyr)
> library(dplyr)
> library(stringr)
> library(lubridate)
> library(ggplot2)
> library(plm)
> #library(clubSandwich)
> devtools::load_all()
> # set global chunk options
> opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE, 
+                fig.path='CR_fig/', fig.align='center', fig.show='hold')
> load("R/Panel simulation results.Rdata")
> results <- within(results, {
+   constraints <- NULL
+   seed <- NULL
+   design <- names(design)
+ })
> head(results)
\end{Sinput}
\begin{Soutput}
             design iterations  m  n  icc trt_var k rho ar        test
1 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0 CR1 Naive-F
2 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0 CR2 Naive-F
3 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0     CR2 HTA
4 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0     CR2 HTB
5 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0     CR2 HTZ
6 RB-balanced-equal      10000 15 18 0.05       0 3 0.2  0 CR1 Naive-F
  hypothesis alpha1 alpha5 alpha10       df
1        t_B 0.0096 0.0501  0.1005 14.00000
2        t_B 0.0096 0.0501  0.1005 14.00000
3        t_B 0.0098 0.0509  0.1016 14.49257
4        t_B 0.0098 0.0509  0.1016 14.49257
5        t_B 0.0098 0.0509  0.1016 14.49257
6        t_C 0.0088 0.0490  0.0975 14.00000
\end{Soutput}
\begin{Sinput}
> gather(results, "alpha", "reject", alpha1:alpha10) %>%
+   mutate(alpha = as.numeric(substring(alpha, 6)) / 100) ->
+   results_long_all
> filter(results_long_all, test %in% c("CR1 Naive-F", "CR2 HTZ") 
+        & design %in% c("CR-balanced","CR-unbalanced","DD-balanced-unbalanced",
+                        "DD-unbalanced-unbalanced","RB-balanced-unequal","RB-unbalanced-unequal")) %>%
+   within({
+     test <- factor(ifelse(test=="CR1 Naive-F", "Naive", "AHZ"), levels = c("Naive","AHZ"))
+     UB <- alpha + qnorm(0.95) * sqrt(alpha * (1 - alpha) / iterations)
+     m_fac <- paste0("m = ",m)
+     q <- hypothesis
+     levels(q) <- c(1,1,2,3,3,6)
+     design <- str_extract(design, "[A-Z]+-[a-z]+")
+     q_alpha <- paste0("q = ", q, ", alpha = ", alpha)
+     alpha_q <- paste0("alpha = ", alpha, ", q = ", q)
+     alpha_m <- paste0("alpha = ", alpha, ", m = ", m)
+     m_test <- factor(paste0(test, " test, m = ", m))
+     m_test <- factor(m_test, levels = levels(m_test)[c(4:6,1:3)])
+   }) ->
+   results_long
> head(results_long)
\end{Sinput}
\begin{Soutput}
       design iterations  m  n  icc trt_var k rho ar  test hypothesis       df
1 RB-balanced      10000 15 18 0.05       0 3 0.2  0 Naive        t_B 14.00000
2 RB-balanced      10000 15 18 0.05       0 3 0.2  0   AHZ        t_B 14.49257
3 RB-balanced      10000 15 18 0.05       0 3 0.2  0 Naive        t_C 14.00000
4 RB-balanced      10000 15 18 0.05       0 3 0.2  0   AHZ        t_C 14.49257
5 RB-balanced      10000 15 18 0.05       0 3 0.2  0 Naive        F_1 14.00000
6 RB-balanced      10000 15 18 0.05       0 3 0.2  0   AHZ        F_1 13.49257
  alpha reject             m_test              alpha_m             alpha_q
1  0.01 0.0112 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
2  0.01 0.0117   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
3  0.01 0.0106 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
4  0.01 0.0107   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
5  0.01 0.0152 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 2
6  0.01 0.0110   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 2
              q_alpha q  m_fac         UB
1 q = 1, alpha = 0.01 1 m = 15 0.01163661
2 q = 1, alpha = 0.01 1 m = 15 0.01163661
3 q = 1, alpha = 0.01 1 m = 15 0.01163661
4 q = 1, alpha = 0.01 1 m = 15 0.01163661
5 q = 2, alpha = 0.01 2 m = 15 0.01163661
6 q = 2, alpha = 0.01 2 m = 15 0.01163661
\end{Soutput}
\begin{Sinput}
> select(results_long, -df) %>%
+   spread(test, reject) ->
+   results_compare
> head(results_compare)
\end{Sinput}
\begin{Soutput}
       design iterations  m  n  icc trt_var k rho ar hypothesis alpha
1 RB-balanced      10000 15 18 0.05       0 3 0.2  0        t_B  0.01
2 RB-balanced      10000 15 18 0.05       0 3 0.2  0        t_B  0.01
3 RB-balanced      10000 15 18 0.05       0 3 0.2  0        t_C  0.01
4 RB-balanced      10000 15 18 0.05       0 3 0.2  0        t_C  0.01
5 RB-balanced      10000 15 18 0.05       0 3 0.2  0        F_1  0.01
6 RB-balanced      10000 15 18 0.05       0 3 0.2  0        F_1  0.01
              m_test              alpha_m             alpha_q
1 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
2   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
3 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
4   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 1
5 Naive test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 2
6   AHZ test, m = 15 alpha = 0.01, m = 15 alpha = 0.01, q = 2
              q_alpha q  m_fac         UB  Naive    AHZ
1 q = 1, alpha = 0.01 1 m = 15 0.01163661 0.0112     NA
2 q = 1, alpha = 0.01 1 m = 15 0.01163661     NA 0.0117
3 q = 1, alpha = 0.01 1 m = 15 0.01163661 0.0106     NA
4 q = 1, alpha = 0.01 1 m = 15 0.01163661     NA 0.0107
5 q = 2, alpha = 0.01 2 m = 15 0.01163661 0.0152     NA
6 q = 2, alpha = 0.01 2 m = 15 0.01163661     NA 0.0110
\end{Soutput}
\begin{Sinput}
> filter(results_long, n==18 & icc==0.05 & 
+          trt_var==0 & rho==0.2 & design=="CR-balanced") %>%
+   mutate(reject=0) -> 
+   zeros_long
> filter(results_compare, m==15 & n==18 & icc==0.05 & 
+          trt_var==0 & rho==0.2 & design=="CR-balanced") %>%
+   mutate(Naive = 0, AHZ = 0) -> 
+   zeros_compare
\end{Sinput}
\end{Schunk}

